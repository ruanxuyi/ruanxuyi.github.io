title: "[Completed] Coursera: Machine Learning by Stanford - Part4"
date: 2016-07-18 10:15:48
tags:
- Coursera
- Machine Learning
---

![](https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png)

<!--more-->

# Week 10: Lesson 1

## Gradient Descent with Large Datasets

### <center> Learning With Large Datasets </center>
- alogrithm deal with large data set.  
- come up with computational efficient way to deal with large dataset.  

#### Learning with large datasets
- typically with $m=100,000,000$, computational heavy.  
- better to try random small subset of $m=1,000$ first.  
- plot the learning curve of {%raw%}$J_{cv}(\theta)$ and $J_{train}{\theta}$ {%endraw%} v.s. $m$ (training size) will help to better determine if increasing $m$ will likely imporve the learning accuracy. (high variance -> increasing $m$ will likely imporve accuracy).  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/high-variance.png)


### <center> Stochastic Gradient Descent </center>
- gradient escent - computational expensive for large data set.  
- to compute gradient descent for large data set $m$, we perfer using stochastic gradient descent.  
- batch gradient descent vs. stochastic gradient descent  
- instead of getting directly to global minimum, stochastic gradient descent will likely **wondering around** the global minimum.  

#### Gradient descent

Every iteration approaches directly to the global minimum step by step.  
However, as $m$ get large(`100,000,000` scale), every iteration, it needs to go through $m$ number of training sets and it is very heavy computation. It is considered heavy computation because it is memory cant hold up that amount of data at once, and swaping bewteen disk and memory is required.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/gradient-descent.png)

In case of large training example $m$, we need more efficient algorithm.  

#### Stochastic gradient descent

We defined:  

{%raw%}$$cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2$${%endraw%}

Similar to Gradient descent, We have:  

{%raw%}$$J_{train}(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))$${%endraw%}

The only difference was in the **iteration updating step.** Instead of iterating through the **entire** $m$ number of examples to get a **single** update, we update the parameter $\theta$ at **every** training set.  

#### Stochastic gradient algorithm

- 1.Randomly 'shuffle' the dataset  
- 2.For $i=1…m$  

{%raw%}$$\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j$${%endraw%}

$\Theta_j$ now updates for every $(x^{(i)}, y^{(i)})$ pair. With stochastic gradient, we can **make progress** in gradient descent **without** having to scan **all $m$ training set** first.   

Stochastic gradient descent **will not** converge to global minimum, instead, it will wondering **around** the global minimum and result in a solution that is close enough.  

Stochastic gradient descent usually take about $1-10$ passes through your data set to get near the global minimum.  

**Quiz:**
![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/quiz.png)


### <center> Mini-batch gradient descent </center>  
- mini-batch faster than stochastic gradient descent and batch gradient descent.  
- advantage and disadvantage of mini-batch gradient descent.  


#### Mini-batch gradient descent
- with good choice of parameter $b$, mini-batch sometimes faster than stochastic gradient descent.  
- batch gradient descent[$m$ example each iteration]/stochastic gradient descent[$1$ example each iteration], mini-batch gradient descent using $b$ example for each iteration.  
- $b\in[2,100]$  

For example, with $b=10$ and $m = 1000$:  
- Repeat:  
- for $i = 1,11,21,31,\dots,991:$  
- {%raw%}$\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$ (for each $j = 0, \dots, n$){%endraw%}  

#### Mini-batch vs Stochastic gradient descent

**Advantage**: Vectorization implementation over $b$ examples -> more parallel  

**Disadvantage**: extra parameter $b$ to fit.  

### <center> Stochastic Gradient descent convergence </center>
- choosing learning rate $\alpha$ for stochastic gradient descent  
- how to make sure it is converging okay?  
- compute and save average cost of hypothesis {%raw%}$cost(\theta, (x^{(i)},y^{(i)}))${%endraw%} over the last $1000$ examples processed by algorithm.  
- possible thing to try out for different situations.  

#### Checking for convergence

##### Better precision at covergence

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/smaller-learning-rate.png)

- red curve: learning algorithm with smaller learning rate $\alpha$. Smaller learning rate $\alpha$, smaller occilscation of stochastic gradient descent around the global minimum.    

##### Plot too noisy

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/larger-average-sample.png)

- red curve: increasing from $1000$ to $5000$, smoother plot

##### Flat curve(cost not decreasing)

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/flat-curve.png)

- red curve: increase to average of large sample data (from $1000$ to $5000$). Slowly decreasing  
- pink curve: if change to average of large sample data does not help, there might be something wrong with features you choose, etc.  

##### plot increasing

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/increaseing.png)

- try smaller value of learning rate $\alpha$

Learning rate $\alpha$ is typically held constant. Can slowly decrease $\alpha$ over time if we want $\theta$ to converge.  

{%raw%}$$\alpha_{new} = \frac{const1}{iterationNumber + const2}$${%endraw%}

As number of iteration (number of training example went through) increase, learning rate $\alpha$ gradually decrease. However, the two parameters $const1$ and $const2$ need extra time to compute. 


### <center> Online learning </center>
- learning continously from the data generated by the user.  
- learning example $(x, y)$ and discard that and move on with new coming data set. 


#### Online learning

With a continuous stream of users to a website, we can run an endless loop that gets $(x,y)$, where we collect some user actions for the features in $x$ to predict some behavior $y$.  

You can update $θ$ for each individual $(x,y)$ pair as you collect them. This way, you can adapt to new pools of users, since you are continuously updating theta.   

#### Other online learning example:  
- Chossing special offers to show user  
- customized selection of news articles  
- product reommendation  

# Week 10: Lesson 2

## Map-reduce

### <center> Map-reduce and data parallelism </center>
- run computation and data on different machines  
- large scale machine learning  

#### Map-reduce idea
- parallelizing computation accross different machine.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/map-reduce.png)

#### Map-reduce on batch gradient descent

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/map-reduce-on-batch-gradient-descent.png)

#### Map-reduce and summation over training set
> Can every learning algorithm be expressed as the summation of the training set?  

Many learning algorithms **can** be expressed as computing sums of functions over the training set.  

Your learning algorithm is MapReduceable if it can be expressed as computing **sums** of functions over the training set. Linear regression and logistic regression are easily parallelizable.

For neural networks, you can compute **forward propagation and back propagation** on subsets of your data on many machines. Those machines can report their derivatives back to a 'master' server that will combine them.


#### Single-computer, multi-core machines

![](http://7xihzu.com1.z0.glb.clouddn.com/20160814/MULTIPLE-CORE.png)

Advantage: not need to worry about **network latency**  

**Library with multi-core capability:** Some numerical linear algebra library that automatically implament the multi-core techique.  

# Week 11: Lesson 1

## Photo OCR

### <center> Problem Description and Pipeline </center>
- intro to Photo OCR problem  
- into to machine learning pipline  

#### Photo OCR problem
- recognize text in picture  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/photo-ocr.png)

#### Photo OCR pipline
- 1.text detection  
- 2.character segmentation  
- 3.character classification  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/ocr-example.png)

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/photo-ocr-pipline.png)

#### Machine learning pipline
- how to break a big problem into a sequence of different modules  
- different team can work seperate to different memeber/team  

### <center>Sliding Windows </center>###
- sliding windows classifier  
- sliding windows example on pedestrian detection  
- sliding windows example on character segmentation  

#### Supervised learning for pedestrain detection
![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/pedistrian-detection.png)

**Terms:**
- step size/stride = the number of pixels that the windows slide accross the image.  

**Run on a smaller scale windows**

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/pedistrian-detect-small.gif)

**Run on a larger scale windows**

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/pedistrian-detect-large.gif)

#### Text detection expansion

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/text-detect-expansion.png)

#### 1D sliding window for character segmentation
- supervised learning to find split of characters.  
- trained a classifier with neural network, etc.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/sliding-window-segmentation.gif)

### <center> Getting Lots of Data and Artificial Data </center>
- Artificial data: creating new data from scratch  
- Amplification: turn small training set into larger one  

#### Artificial data synthesis for photo OCR
- synthetic data: patch character from font library with a random background.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/synthetic-data-photo-ocr.png)

#### Artificial data: amplification
- take one current example and apply warp and distortion to create new training examples.  
- amplify one training example to 16 new training examples  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/artificial-warping.png)

#### Discussion on getting more data
- artificial data synthesis (from scratch)  
- amplify current examples to 10x of number of examples.  
- collect/label it yourself  
- "crowd source" (Amazon Mechanical Turk)  

### <center> Ceiling Analysis: What Part of the Pipeline to Work on Next </center>
- determine what type of pipeline to work and spend most time trying to improve.  

#### Ceiling analysis
- for all four stages, manually select correct answer to have 100% accuracy for each stages  
- see with the 100% accuracy, which stage's 100% accuracy provide the best improvement to overall accuracy.  
- we can later determine which stage to spend time on to improve.  
- estimate the upper bound (ceiling) of each stage.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/ceiling-analysis.png)

#### Ceiling analysis: Face recognition from image
- more complicated pipeline.  

![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/face-recg-pipline.png)

- from the ceiling analysis above, we can see that spending 18 months working on background removal will not likely to help improvement of overall accuracy.  
- however, working on face detection will make the most overall improvement in this case.  
- doing a ceiling analysis before-hand will significant save lots of effort.  
- dont waste your time on somthing that is not likely to give significant improvement  
- dont trust your gut feeling, use ceiling analysis to prove in a more reliable way.  

# Week 11: Lesson 3

## Summary
### <center> Problem Description and Pipeline </center>


![](http://7xihzu.com1.z0.glb.clouddn.com/20160815/certificate.png)

Thank you, Andrew Ng!


# Logs

- 08/14/2016: Week 10 completed.  
- 08/15/2016: Course completed.  
