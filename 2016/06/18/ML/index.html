<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Bree Serif:300,300italic,400,400italic,700,700italic|Arimo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/sushi.png?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera: Machine Learning by Stanford">
<meta property="og:url" content="http://www.xuyiruan.com/2016/06/18/ML/index.html">
<meta property="og:site_name" content="阮先生de小窝">
<meta property="og:description">
<meta property="og:image" content="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/news.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/genes.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/market.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cocktail.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/univariate-linear-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta3.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function-2para.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/sim-update.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec-alpha.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/fix-alpha.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/partial-derivative.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/best-hyphthesis-func.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/scalar-multiplication.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi-ex.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vs-for-loop.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-def.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-ex2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/identity-matrix.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/martix-idneitiy-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/singular.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/transpose.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/flip-matrix-transpose.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-before.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-after.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent-decrease.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Falpha-too-small.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fpolynomial-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fsqrt-root-feature.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fnormal-equ-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fx-inverse.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-desc-vs-normal-equ.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2FmyPlot.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcolormap.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcost-function-j.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fvect-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-classification.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-bad-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/sigmoid-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/logistic-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/decision-boundary.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/non-linear-logistic-regression.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera: Machine Learning by Stanford">
<meta name="twitter:description">




<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> Coursera: Machine Learning by Stanford | 阮先生de小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">阮先生de小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">多读书 多思考 少吃零食 多睡觉</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'TVdsuUVez9DD1Mx-gjc8','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Coursera: Machine Learning by Stanford
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-06-18T15:11:11-04:00" content="06-18-2016">
              06-18-2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/18/ML/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/18/ML/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/06/18/ML/" class="leancloud_visitors" data-flag-title="Coursera: Machine Learning by Stanford">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" alt=""></p>
<a id="more"></a>
<h1 id="Week_1:_Lesson_3">Week 1: Lesson 3</h1><h2 id="Supervise_Learning">Supervise Learning</h2><p><strong>Examples:</strong></p>
<ul>
<li>Given email labeled as spam/not spam, learn a spam filter. (classification)</li>
<li>Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabtetes or not. (classification)</li>
<li>Given data of different house types (room, square feet, year built, wood/concrete.. etc) with their corresponding price, evaluate/preidct the price of a given house. (Regression)</li>
</ul>
<h3 id="Regression:">Regression:</h3><ul>
<li>predict value according to the data set.  </li>
<li>predict <strong>continuous valued</strong> output(price)  </li>
</ul>
<h3 id="Classification:">Classification:</h3><ul>
<li><strong>discrete valued</strong> output(0 or 1).</li>
</ul>
<h3 id="SVM(Support_Vector_Machine):">SVM(Support Vector Machine):</h3><ul>
<li>process of infinte number of features.   </li>
</ul>
<h2 id="Unsupervised_Learning">Unsupervised Learning</h2><p><strong>Defination:</strong>   </p>
<ul>
<li>did not telling the algorithm how to categorize data sets into different types in advance.  </li>
<li>automatically find structure/categories on given data sets.  </li>
</ul>
<h3 id="Cluster_Learning:">Cluster Learning:</h3><p><strong>Examples:</strong>  </p>
<ul>
<li><p>Given a set of news articles found on the web, group them into set of articles about the same story. (Cluster)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/news.png" alt=""></p>
</li>
<li><p>automatically analyzes data sets and group them into different type of genes sequence for diffrent group of individuals’ data set. (Cluster)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/genes.png" alt=""></p>
</li>
</ul>
<p><strong>Applications:</strong>  </p>
<ul>
<li>Market Segementation:  algorithm automatically categorizes customers into different clusters according to customers’ data, which make marketing and advertisement more effecients and effecitve.   (Airline company tries to categorizes their frequent flyers to different types to better advertises products for different type of frequent flyiers [YMMV - different people get different targeted advertisement])</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/market.png" alt=""></p>
<h3 id="Cocktail_Party_Problem_Algorithm">Cocktail Party Problem Algorithm</h3><ul>
<li>seperate overlaped audio and let <code>cocktail party algorithm</code> to seperate them into different audios tracks.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cocktail.png" alt=""></p>
<h1 id="Week_1:_Lesson_5">Week 1: Lesson 5</h1><h2 id="Model_Representation">Model Representation</h2><ul>
<li>m = Number of training examples </li>
<li>x’s = “input” variable/ feature</li>
<li>y’s = “output” variable / target variable</li>
<li>(x,y) = one training example.</li>
<li>(x<sup>i</sup> , y<sup>i</sup>) = i<sup>th</sup> training example.  </li>
</ul>
<p>Traning set -&gt; Learning Algorithm -&gt; h(hypothesis) -&gt; </p>
<blockquote>
<p>h(hypothesis) - a function that maps from x’s -&gt; y’s. </p>
</blockquote>
<pre><code><span class="title">size</span> <span class="keyword">of</span> house -&gt; h -&gt; estimated price
</code></pre><h3 id="Univariate_Linear_Regression">Univariate Linear Regression</h3><ul>
<li>Linear Regression with <strong>one variable</strong>. (x)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/univariate-linear-regression.png" alt=""></li>
</ul>
<h2 id="Cost_Function">Cost Function</h2><ul>
<li>fit the best straigt line to our data  </li>
</ul>
<p>Given training set and Hypothesis:  h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x  (linear, single variable)  </p>
<p>How to choose θ<sub>i</sub>‘s (Paramenters)?  </p>
<ul>
<li><p>choose θ<sub>0</sub>, θ<sub>1</sub> so that h(x) is close to <code>y</code> for our training examples<code>(x,y)</code></p>
</li>
<li><p>minimize $ \frac 1 {2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $(m - # training examples)</p>
</li>
</ul>
<p><strong>Hypothesis Function:</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x$$
</p>
<p><strong>Cost Function(Squared error function)</strong><br>
$$J(\theta_0, \theta_1) = \frac 1 {2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$
<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function.png" alt=""></p>
<h2 id="Cost_Fucntion_-_Intro_I">Cost Fucntion - Intro I</h2><ul>
<li>difference and relation between Hypothesis Function and Cost Function   </li>
<li>pick a Hypothesis Function ($\theta_0=0$ pick $\theta_1$)  </li>
<li>Graphically understand the chice of $\theta_1$ affect Hypothesis Function and Cost Function.  </li>
</ul>
<p>Hypothesis function $h_0(x)$<br>is a funtion of house area $x$ and price of that house.<br>
$$h_{\theta}(x)=\theta_1 x$$ 
</p>
<p>Cost function $J(\theta_1)$ is a funciton of $\theta_1$, assuming $\theta_0=0$.</p>

$$J(\theta_1) = \frac 1 {2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$

<h3 id="Find_$\theta_1$_by_minimizing_$J(\theta_1)$_(local_MIN_of_$J(\theta_1)$)">Find $\theta_1$ by minimizing $J(\theta_1)$ (local MIN of $J(\theta_1)$)</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta1.png" alt=""></p>
<p>$$ \theta_1 = 1$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta2.png" alt=""></p>
<p>$$ \theta_1 = 0.5 $$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta3.png" alt=""></p>
<p>$$ \theta_1 = 0 $$ </p>
<p>In this case, the local min happens at $ \theta_1 = 1$.</p>
<h2 id="Cost_Fucntion_-_Intro_II">Cost Fucntion - Intro II</h2><ul>
<li>better understanding of the cost function  </li>
<li>understand contour plots  </li>
<li><strong>manually</strong> looking for $\theta_0$ and $\theta1$ pair that minimize the cost function. </li>
</ul>
<p>Pick a hypothesis funciton $\theta_0$ and $\theta1$. In this example, we assume $\theta_0\neq0$.</p>
<h3 id="Cost_Function_in_3D">Cost Function in 3D</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function-2para.png" alt=""></p>
<h3 id="Counter_Plot">Counter Plot</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot1.png" alt=""></p>
<p>$$ \theta_0 = 360,\theta_1 = 0 $$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot2.png" alt=""></p>
<p>$$ \theta_0 = 220,\theta_1 = 0.15 $$</p>
<h1 id="Week_1:_Lesson_6">Week 1: Lesson 6</h1><h2 id="Gradient_Descent">Gradient Descent</h2><ul>
<li>Gradient Descent - an algorithm to minimize the cost function J and other function.  </li>
<li>very general algorithm that used a lot in ML.  </li>
<li>Gradient - downhill as rapidly as possible. (take the steepest slope)   </li>
</ul>
<p><strong>Steps</strong>   </p>
<ol>
<li>start with some $ \theta_0 ,\theta_1 $ ($ \theta_0 = 0 ,\theta_1 = 0$)  </li>
<li>Keep chaning $ \theta_0 ,\theta_1 $ to reduce $J(\theta_0, \theta_1)$ until it reaches local minimum.  </li>
</ol>
<p><strong>Example</strong></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec1.png" alt=""></p>
<p>Start with a point that is little bit to the right result in a <strong>different path</strong> to another local-minima.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec2.png" alt=""></p>
<p><strong>Algorithm</strong><br>
$$ \theta_i :=\theta_i - \alpha \frac {\partial}{\partial\theta_i} J(\theta_0,\theta_1),(i = 0, i = 1)$$    
 </p>
<p>$:= $ - assign statement<br>$\alpha$ - learning rate (how far each step should go down hill)</p>
<p><strong>Implementation</strong><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/sim-update.png" alt="Simultaneous Update"></p>
<h2 id="Gradient_Descent_Intuition">Gradient Descent Intuition</h2><ul>
<li>better intuition of Gradient Descent Algorithm.  </li>
<li>understand the derivative term.  </li>
</ul>

$$ \theta_1 :=\theta_1 - \alpha \frac {\partial}{\partial\theta_1} J(\theta_1)$$    
 
<ul>
<li>if $\alpha$ is too small, gradient decent can be slow.   </li>
<li>if $\alpha$ too large, gradient decent can overshoot the minimum. It may fails to converge.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec-alpha.png" alt=""></p>
<p>As we approach a local minimum, the gradient decent will <strong>automatically</strong> take <strong>smaller steps</strong> even with fixed learning rate $\alpha$. </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/fix-alpha.png" alt=""></p>
<p>Even $\alpha$ is fixed, slope or $\frac {\partial}{\partial\theta_1} J(\theta_1)$ is decreasing. Therefore, the overall term $\alpha \frac {\partial}{\partial\theta_1} J(\theta_1) $ decreases.</p>
<h2 id="Gradient_Descent_For_Linear_Regression">Gradient Descent For Linear Regression</h2><ul>
<li>apply gradient descent algorithm to minimize suqare error cost function (J).  </li>
</ul>

$$ \theta_i :=\theta_i - \alpha \frac {\partial}{\partial\theta_i} J(\theta_0,\theta_1),(i = 0, i = 1)$$    
 
<p>After taking partial derivative:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/partial-derivative.png" alt=""></p>
<h3 id="Gradient_Descent_Algorithm">Gradient Descent Algorithm</h3>
$$ \theta_0 :=\theta_0 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)}) $$    

$$ \theta_1 :=\theta_1 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x^{(i)} $$    
 
<blockquote>
<p>IMPORTANT: $\theta_0$ and $\theta_1$ need to be updated <strong>simultaneously</strong>. </p>
</blockquote>
<p>Using Gradient Descnet Algorithm to find the best Hyphothesis Function.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/best-hyphthesis-func.png" alt=""></p>
<h3 id="“Batch”_Gradient_Descent_Algorithm">“Batch” Gradient Descent Algorithm</h3><ul>
<li>Batch: each step of gradient descent uses <strong>all the training examples/data</strong>.  </li>
<li>
$$\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$
</li>
<li>m = total # of training examples/data.  </li>
</ul>
<h1 id="Week_1:_Lesson_7">Week 1: Lesson 7</h1><h2 id="Matrices_and_Vectors">Matrices and Vectors</h2><h3 id="Matrix:">Matrix:</h3><ul>
<li>Rectangular array of numbers</li>
<li>CAP letter for Matrices(A,B,C,X.etc)  </li>
</ul>
<h3 id="Vector:">Vector:</h3><ul>
<li>An n x 1 matrix (only ONE column)  </li>
<li>Vector is a special type of matrices  </li>
<li>1-indexed(in course) vs 0-indexed(more in ML)  </li>
<li>lower case letter for value, vector (a, b, c, x, etc)  </li>
</ul>
<h2 id="Addition_and_Scalar_Multiplicaiton">Addition and Scalar Multiplicaiton</h2><h3 id="Addition">Addition</h3><ul>
<li>only add matrix of <strong>same dimension</strong>  </li>
</ul>
<h3 id="Scalar_Multication">Scalar Multication</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/scalar-multiplication.png" alt=""></p>
<h2 id="Matrix_Vector_Multiplication">Matrix Vector Multiplication</h2><h3 id="Definition:">Definition:</h3><ul>
<li>number of Column of A(n) has to MATCH # of row of x (n)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi.png" alt=""></p>
<h3 id="Example:">Example:</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi-ex.png" alt=""></p>
<h3 id="Advantage_of_using_Matrix_Multiplication">Advantage of using Matrix Multiplication</h3><ul>
<li><strong>simplify</strong> the actual code for implementation  </li>
<li>increase <strong>readiability</strong>  </li>
<li>more <strong>efficient</strong> to do matrix multiplication(left) than using for-loop(right) in case of large data-set  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vs-for-loop.png" alt=""></p>
<h3 id="Convert_predicting_housing_price_into_Matrix_Multiplication">Convert predicting housing price into Matrix Multiplication</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png" alt=""></p>
<h2 id="Matrix_Matrix_Multiplication">Matrix Matrix Multiplication</h2><ul>
<li>how to multiply two matrix together  </li>
</ul>
<h3 id="Example">Example</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi.png" alt="">  </p>
<blockquote>
<p><strong>matrix-matrix multiplication</strong> is simply a break down steps of <strong>matrix-vector multiplication</strong>   </p>
</blockquote>
<h3 id="Definition">Definition</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-def.png" alt=""></p>
<blockquote>
<p>A’s column# has to <strong>match</strong> with B’s row#. </p>
</blockquote>
<h3 id="One_more_example">One more example</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-ex2.png" alt=""></p>
<blockquote>
<p>TIPs: break down <strong>matrix-matrix multiplication</strong> into many <strong>matrix-vector multiplication</strong> will make your job more easier.  </p>
</blockquote>
<h3 id="Application_of_M-M-M">Application of M-M-M</h3><ul>
<li>predicting house price again, YES!  </li>
<li>as before we have four house sizes  </li>
<li>but now we have THREE <strong>different</strong> hypothesis functions  </li>
<li>we want to predict house prices base on three different models  </li>
<li>higly <strong>optimized</strong> linear algrba <strong>library</strong> are able to help us with complicated matrix-matrix-multiplicaiton  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png" alt=""></p>
<h2 id="Matrix_Multiplication_Properties">Matrix Multiplication Properties</h2><ul>
<li>benefit: pack lots of complication into just on matrix multiplication operation  </li>
<li>but be aware of the properties of matrix multiplication  </li>
</ul>
<h3 id="Commutative">Commutative</h3><ul>
<li>Matrix multiplication is NOT commutative<br>$$ A\times B\neq B\times A $$ </li>
</ul>
<h3 id="Associative">Associative</h3><ul>
<li>has to match both <strong>dimension</strong> and <strong>values</strong>  </li>
<li>Matrix multiplication IS associative  </li>
</ul>
<p>$$ A \times B \times C = A \times (B \times C)$$</p>
<h3 id="Identity_Matrix">Identity Matrix</h3><ul>
<li>denoted as $I$(or $I_{n\times n}$)  </li>
<li>Examples of identity matrices:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/identity-matrix.png" alt=""></li>
<li>Commutative property holds for identity matrix  </li>
</ul>
<p>For any matrix A, </p>

$$A*I_1 = I_2*A = A$$

<blockquote>
<p>If A has $m\times n$ dimension -&gt; $I_1$ has $n\times n$ dimension, while $I_2$ has $m\times m$ dimension  </p>
</blockquote>
<h2 id="Inverse_and_Transpose">Inverse and Transpose</h2><ul>
<li>1 = “identity”  </li>
<li>not all numbesr have an inverse(ex. 0)  </li>
<li>how to compute inverse of a matrix  </li>
</ul>
<h3 id="Matrix_inverse:">Matrix inverse:</h3><p>If A is an $m x m$ matrix, and if it has an inverse,  </p>

$$A*A^{-1} = A^{-1}*A = I$$  

<h3 id="Example_1">Example 1</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/martix-idneitiy-example.png" alt="">  </p>
<ul>
<li>How to compute that? We use higly optimized software to compute it.  </li>
<li>not all matrices have inverse. (a 2x2 matrices that has all zeros in it has NO inverse since there is no way to create diagonal 1’s(identity) on the result matrices)  </li>
<li>Matrices that don’t have an inverse are “<strong>singular</strong>“ or “<strong>degenerate</strong>“   </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/singular.png" alt="singular matrices"></p>
<h3 id="Matrix_Transpose">Matrix Transpose</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/transpose.png" alt=""></p>
<h3 id="Definition-1">Definition</h3><p>Let $A$ be an $m\times n$ matrix, and let $B=A^T$. Then $b$ is an $n\times m$ matrix, and $$B_{ij} = A_{ji}$$ .  </p>
<p>In example above:  </p>
 
$A_{12} = B_{21} = 2$, and  $A_{22} = B_{22} = 5$

<blockquote>
<p>Tips: simply filp the matrices <strong>along the 45 degree line</strong> will result in Matrix Transpose.  </p>
</blockquote>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/flip-matrix-transpose.png" alt=""></p>
<h1 id="Week_2:_Lesson2">Week 2: Lesson2</h1><h2 id="Multivariate_Linear_Regression">Multivariate Linear Regression</h2><h3 id="Mltivariate_Linear_Regression"><center> Mltivariate Linear Regression </center></h3><ul>
<li>multiple properities/features  </li>
<li>more useful model  </li>
</ul>
<h4 id="Multiple_features">Multiple features</h4><ul>
<li>$n$ = number of features  </li>
<li>$x^{(i)}$ = input (features) of $i^{th}$ traning example (a column vector n x 1) </li>
<li>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ traning example  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png" alt=""></p>
<p>Example above has <strong>four</strong> features, therefore, $n=4$. Since there are 47 samples/training data, $m=47$. $x^{2}$ indicates the column vector $ \begin{bmatrix}<br>1046 \\ 3 \\ 2 \\ 40 \\ \end{bmatrix} $ and the notation $x_2^{2}$ indicates <strong>second</strong> feature(# of bedrooms) from <strong>second</strong> training data.  </p>
<h4 id="New_Hypothesis_function">New Hypothesis function</h4><p><strong>Previously:(one feature)</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x$$
</p>
<p><strong>Now:(four features)</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+\theta_3 x_3+\theta_4 x_4$$
</p>
<p><strong>Example Hypthesis function:</strong><br>
$$h_{\theta}(x)=80+0.1 x_1+0.01 x_2+3 x_3-2 x_4$$
</p>
<ul>
<li>$80$ - base price  </li>
<li>$0.1x_1$ - size <strong>added up</strong> by factor of $0.1$ to base price  </li>
<li>$0.01x_2$ - number of bedrooms <strong>add up</strong> by a lower factor of $0.01$ to base price  </li>
<li>$3x_3$ - number of floor <strong>add up</strong> by a factor of 3 to base price  </li>
<li>$-2x_4$ - age of the house <strong>subtract by</strong> factor of 2 on base price  </li>
</ul>
<h4 id="Formal_Definition">Formal Definition</h4>
$$h_{\theta}(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n$$

<blockquote>
<p>For convenience of notation, we defined</p>
</blockquote>
<p> $$x_0 = 1$$.  </p>

$$h_{\theta}(x)=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n$$

<p>$x = \begin{bmatrix}<br>x_0 \\<br>x_1 \\<br>x_2 \\<br>… \\<br>x_n<br>\end{bmatrix}$, $\theta = \begin{bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>\theta_2 \\<br>… \\<br>\theta_n<br>\end{bmatrix}$, $\theta^T = \begin{bmatrix}<br>\theta_0,<br>\theta_1,<br>\theta_2,<br>…<br>\theta_n<br>\end{bmatrix}$</p>
<p>Therefore:  </p>

$$h_{\theta}(x)=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n = \theta^T x$$

<p>Result of $\theta^T x$ is $[1\times n] [n\times 1]$ = $1\times1$ dimensional (prediction <strong>price</strong> of the house)</p>
<h3 id="Gradient_Descent_for_Multiple_Variables"><center> Gradient Descent for Multiple Variables </center></h3><ul>
<li>automatically fit the paramenter in hypothesis function for <strong>multiple</strong> features.    </li>
</ul>

$$ \theta_j :=\theta_j - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_j^{(i)} $$    


$$ \theta_0 :=\theta_0 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_0^{(i)}, where (x_0^{(i)} = 1)$$
$$ \theta_1 :=\theta_1 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_1^{(i)} $$ 
$$ \theta_2 :=\theta_2 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_2^{(i)} $$  
$$ ... $$

 
<h3 id="Gradient_Descent_in_Practice_I_-_Feature_Scaling"><center> Gradient Descent in Practice I - Feature Scaling </center></h3><ul>
<li>go over some practical tricks to make gradient decent work well: feature scaling   </li>
<li>idea: make sure features are on a <strong>similiar scale</strong>.  </li>
<li>so that the gradient descent will converge faster  </li>
</ul>
<p><strong>Original Scale</strong><br>$x_1 = size(0-2000 feet^2)$<br>$x_2 = number of bedrooms(1-5)$<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-before.png" alt=""></p>
<p><strong>After Scaling</strong>  </p>
<p>$x_1 = \frac{size(feet^2)}{2000}$<br>$x_2 = \frac{number of bedroom}{5}$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-after.png" alt=""></p>
<p>Gradient Descent with scaling will result in a faster, <strong>direct path</strong> to find the local minimal and<strong> converge much faster, </strong>why??**</p>
<blockquote>
<p>Possible Reason: Feature Scaling + Mean Normalization allows updated $X$ values locate in a <strong>similiar</strong> &amp; <strong>smaller</strong> range ( $ -1 &lt; X &lt; 1$ ) instead of un-normalizaed range ($0&lt;x&lt;2000$). The <strong>smaller</strong>&amp;<strong>united</strong> range allow gradient descent to work <strong>better</strong> even with small value of $\alpha$.  </p>
</blockquote>
<h4 id="Feature_Scaling">Feature Scaling</h4><p>Get every featrue into <strong>approximately</strong> a $-1 \leq x_i \leq 1$ range.  </p>
<p><strong>Examples of Good Scaling:</strong>  </p>
<ul>
<li>$0\leq x_1\leq3$  </li>
<li>$-2\leq x_2\leq0.5$<br><strong>Examples of BAD Scaling:</strong>   </li>
<li>$-100\leq x_1\leq100$  </li>
<li>$-0.001\leq x_1\leq0.001$</li>
</ul>
<h4 id="Mean_Normalization">Mean Normalization</h4><ul>
<li>another techique to enhance feature scaling’s perforamnce      </li>
<li>Replace $x_i$ with $x_i - \mu_i$  </li>
<li>Help gradient descent converge more <strong>faster</strong> </li>
</ul>

$$x_i = \frac{x_i-\mu_i}{s_i}$$

<ul>
<li>$\mu_i$ denotes average value of $x_i$ in training set  </li>
<li>$s_i$ denotes the range $(max - min)$ or standard deviation of traning set  </li>
</ul>
<h3 id="Gradient_Descent_in_Practice_II_-_Learning_Rate"><center> Gradient Descent in Practice II - Learning Rate </center></h3>
$$ \theta_j :=\theta_j - \alpha \frac {\partial}{\partial\theta_j} J(\theta)$$    
 
<ul>
<li>anther technique to make gradient descent work well in practice  </li>
<li>how to make sure gradient descent is working <strong>correctly</strong>  </li>
<li>how to <strong>choose</strong> learning rate $\alpha$.  </li>
</ul>
<h4 id="Making_sure_gradient_descent_work_correctly">Making sure gradient descent work correctly</h4><ul>
<li>find the best $theta$ value that <strong>minimize</strong> the $J(\theta)$  </li>
<li>$J(\theta)$ should <strong>decrease</strong> after every iterations  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent-decrease.png" alt=""></p>
<p><strong>Manually convergence test:</strong>  Lookinto the graph to check if gradient descent converge.  </p>
<p><strong>Automatically convergence test:</strong> Declare convergence if $J(\theta)$ decreases by less than $10^{-3}$ in one interation. </p>
<blockquote>
<p>determine a good threadshold ($10^{-3}$) is sometimes confusing  </p>
</blockquote>
<h4 id="Possible_bugs">Possible bugs</h4><ul>
<li>$\alpha$ too large  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Falpha-too-small.png" alt="">  </p>
<h4 id="To_choose_α,_try(three-fold)">To choose α, try(three-fold)</h4><p>$$…,0.001,0.003,0.01,0.03,0.1,0.3,1…$$ </p>
<h3 id="Features_and_Polynomial_Regression"><center> Features and Polynomial Regression </center></h3><ul>
<li><strong>choose</strong> appropriate features  </li>
<li><strong>polynomical regression</strong> for complicated, non-linear functions  </li>
</ul>
<h4 id="Definiing_new_feature">Definiing new feature</h4><ul>
<li>instead of using the feature provide, we might able to come up with new feature to simplify the gradient descent algorithm  </li>
</ul>
<p><strong>Given features:</strong> <code>frontage &amp; depth</code> of a house<br><strong>New feature:</strong> <code>Area</code> ($Area = frontage\times depth $)</p>
<h4 id="Polynomical_Regression">Polynomical Regression</h4><ul>
<li>fit complicate data set with more accurate polynomial function  </li>
<li>derive polynomical regression with <strong>multi-varient linear regression</strong><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fpolynomial-regression.png" alt=""> </li>
</ul>

$$h_0(x)=\theta_0+\theta_1\times x_1+\theta_2\times x_2+\theta_3 \times x_3 $$ 
$$ h_0(x)= \theta_0+\theta_1\times (size)+\theta_2\times (size)^2+\theta_3 \times (size)^3 $$ 
  
<p>where<br>feature 1: $x_1 = (size)$<br>feature 2: $x_2 = (size)^2$<br>feature 3: $x_3 = (size)^3$  </p>
<p><strong>Scale Range of the features</strong></p>
<p>feature 1: $(size): 1-1000$<br>feature 2: $(size)^2: 1-1000,000$<br>feature 3: $(size)^3: 1-10^9$ </p>
<h4 id="Choose_a_feature">Choose a feature</h4><ul>
<li>if you dont want to go into cubic model function ($x^3$)  </li>
<li>we can use other feature  ($\sqrt{(x)}$)</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fsqrt-root-feature.png" alt=""></p>
<h1 id="Week_2:_Lesson_3">Week 2: Lesson 3</h1><h2 id="Computing_Parameters_Analytically">Computing Parameters Analytically</h2><h3 id="Normal_Equation"><center>Normal Equation</center></h3><ul>
<li>so far, we use gradient descent to find optimal $\theta$  </li>
<li>a <strong>better way</strong> to solve optimal parameter $\theta$  </li>
<li>solve for $\theta$ analytically  </li>
<li><strong>without</strong> iteratively computation (gradient descent)</li>
<li><strong>one step</strong> computation for optimal parameter $\theta<br>$  </li>
<li>Feature Scaling is <strong>not necessary</strong> in Normal Equation</li>
</ul>
<h4 id="Example-1">Example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fnormal-equ-example.png" alt=""></p>
<h4 id="Equation">Equation</h4><p>$$\theta = (X^TX)^{-1}X^Ty$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fx-inverse.png" alt=""></p>
<blockquote>
<p>$x_0^i = 1$</p>
</blockquote>
<p>Octave: <code>pinv(x&#39;*x)*x&#39;*y</code>  </p>
<h4 id="Gradient_Descent_vs_Dis-advantage">Gradient Descent vs Dis-advantage</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-desc-vs-normal-equ.png" alt=""></p>
<blockquote>
<p>if $n$ number of features is very large ($n \geq 10^6$), it will be better to use gradient descent to aviod the $O(n^3)$ time complexity of normal equation.</p>
</blockquote>
<h3 id="Normal_Equation_Noninvertibility"><center>Normal Equation Noninvertibility</center></h3><ul>
<li>advanced concpet  </li>
<li>not all the matrices is invertable (ex. singular matrices)  </li>
<li>what if $X^TX$ is non-invertible? (singular/degenerate) </li>
</ul>
<h4 id="pinv_vs_inv_in_Octave"><code>pinv</code> vs <code>inv</code> in Octave</h4><ul>
<li><code>pinv</code> sudo inverse  </li>
<li><code>inv</code> inverse  </li>
</ul>
<blockquote>
<p><code>pinv</code> will result in a correct value even matrics is not invertible  </p>
</blockquote>
<h4 id="Case_where_$X^TX$_is_non-invertible">Case where $X^TX$ is non-invertible</h4><ul>
<li>Redudant features (linearly dependent)  </li>
</ul>
<p>$x_1$ = size in $feet^2$<br>$x_2$ = size in $m^2$  </p>
<blockquote>
<p>if two features ($feet^2$, $m^2$) has linear relation, $X^TX$ cannot be inverted  </p>
</blockquote>
<ul>
<li>Too many features (e.g. $m \leq n $) </li>
</ul>
<p><strong>Example</strong>:<br>traning set ($m = 10$), number of features ($n = 100$)  </p>
<blockquote>
<p>Number of traning set too small(<strong>not enough</strong> data), it is very hard for us to predict the 100 optimal $\theta$ for each of the features.  </p>
</blockquote>
<p><strong>Solution</strong>:  </p>
<ul>
<li><strong>delete</strong> some <strong>features</strong>  </li>
<li>use <strong>regularization</strong>(cover later)  </li>
</ul>
<h1 id="Week_2:_Lesson_6">Week 2: Lesson 6</h1><h2 id="Octave/Matlab_Tutorial">Octave/Matlab Tutorial</h2><h3 id="Basic_Operations"><center>Basic Operations</center></h3><p><code>eye(3)</code>  % 3x3 identity matrix</p>
<pre><code>1     0     0
0     1     0
0     0     1
</code></pre><p><code>sum(v, 1)</code> -&gt; sum column<br><code>sum(v, 2)</code> -&gt; sum row<br><code>size(v, 1)</code> -&gt; number of rows<br><code>size(v, 2)</code> -&gt; number of cols  </p>
<h3 id="Moving_Data_Around"><center>Moving Data Around</center></h3><h4 id="A(row_index,_col_index)">A(row_index, col_index)</h4><p><strong><code>A(2,:)</code>  % get the 2nd row.</strong>   </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>   <span class="number">100</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>   <span class="number">102</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">2</span>,:)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br></pre></td></tr></table></figure>
<p><strong><code>A(:,2)</code>  % get the 2nd col</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A(:,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<p><strong><code>A = [A, [100; 101; 102]];</code> % append column vector</strong>  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = <span class="matrix">[A, [<span class="number">100</span>; <span class="number">101</span>; <span class="number">102</span>]</span>]</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>   <span class="number">100</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>   <span class="number">102</span></span><br></pre></td></tr></table></figure>
<p><strong><code>A(:)</code> % Select all elements as a column vector.</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A(:)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br><span class="line">   <span class="number">100</span></span><br><span class="line">   <span class="number">101</span></span><br><span class="line">   <span class="number">102</span></span><br></pre></td></tr></table></figure></p>
<p><strong>% Putting data together</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="matrix">[<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span></span><br><span class="line">B = <span class="matrix">[<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>]</span> </span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p><strong>concatenating A and B matrices side by side</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; C=<span class="matrix">[A B]</span>=<span class="matrix">[A,B]</span></span><br><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Concatenating A and B top and bottom</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; C=<span class="matrix">[A;B]</span></span><br><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Computing_On_Data"><center>Computing On Data</center></h3><p><strong>element-wise operation</strong>   </p>
<ul>
<li>notations - <code>.</code></li>
</ul>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .* <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">2</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span>     <span class="number">8</span></span><br><span class="line">    <span class="number">10</span>    <span class="number">12</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .^ <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">9</span>    <span class="number">16</span></span><br><span class="line">    <span class="number">25</span>    <span class="number">36</span></span><br></pre></td></tr></table></figure>
<p><strong>element-wise log/exp/abs</strong>  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">log</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">         <span class="number">0</span></span><br><span class="line">    <span class="number">0.6931</span></span><br><span class="line">    <span class="number">1.0986</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">exp</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">2.7183</span></span><br><span class="line">    <span class="number">7.3891</span></span><br><span class="line">   <span class="number">20.0855</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">abs</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><strong>Important</strong><br>For square matrices calculation  $A .* A \neq A^2$</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="matrix">[<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>]</span></span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .* A</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">9</span>    <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A^<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">7</span>    <span class="number">10</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">22</span></span><br></pre></td></tr></table></figure>
<p><strong>Find the transpose</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A<span class="operator">'</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">3</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span>     <span class="number">4</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p><strong>Flip matrix up-side-down</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; flipup(C)</span><br><span class="line">Undefined <span class="function"><span class="keyword">function</span> <span class="title">or</span> <span class="title">variable</span> '<span class="title">flipup</span>'.</span></span><br><span class="line"> </span><br><span class="line">Did you mean:</span><br><span class="line">&gt;&gt; <span class="built_in">flipud</span>(C)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h3 id="Plotting_Data"><center>Plotting Data</center></h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% plotting</span></span><br><span class="line">t = <span class="matrix">[<span class="number">0</span>:<span class="number">0.01</span>:<span class="number">0.98</span>]</span>;</span><br><span class="line">y1 = <span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t); </span><br><span class="line">plot(t,y1);</span><br><span class="line">y2 = <span class="built_in">cos</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">hold on;  <span class="comment">% "hold off" to turn off</span></span><br><span class="line">plot(t,y2,<span class="string">'r'</span>);</span><br><span class="line">xlabel(<span class="string">'time'</span>);</span><br><span class="line">ylabel(<span class="string">'value'</span>);</span><br><span class="line">legend(<span class="string">'sin'</span>,<span class="string">'cos'</span>);</span><br><span class="line">title(<span class="string">'my plot'</span>);</span><br><span class="line">print -dpng <span class="string">'myPlot.png'</span></span><br><span class="line">close;           <span class="comment">% or,  "close all" to close all figs</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2FmyPlot.png" alt=""></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">    <span class="number">17</span>    <span class="number">24</span>     <span class="number">1</span>     <span class="number">8</span>    <span class="number">15</span></span><br><span class="line">    <span class="number">23</span>     <span class="number">5</span>     <span class="number">7</span>    <span class="number">14</span>    <span class="number">16</span></span><br><span class="line">     <span class="number">4</span>     <span class="number">6</span>    <span class="number">13</span>    <span class="number">20</span>    <span class="number">22</span></span><br><span class="line">    <span class="number">10</span>    <span class="number">12</span>    <span class="number">19</span>    <span class="number">21</span>     <span class="number">3</span></span><br><span class="line">    <span class="number">11</span>    <span class="number">18</span>    <span class="number">25</span>     <span class="number">2</span>     <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; imagesc(A)</span><br><span class="line">&gt;&gt; imagesc(A), colorbar, colormap gray;</span><br><span class="line">&gt;&gt; print -dpng <span class="string">'colormap.png'</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcolormap.png" alt=""></p>
<h3 id="Control_Statements:_for,_while,_if_statement"><center>Control Statements: for, while, if statement</center></h3><h4 id="for-loop">for-loop</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; v=<span class="built_in">zeros</span>(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     </span><br><span class="line">&gt;&gt; <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">10</span>,</span><br><span class="line">v(<span class="built_in">i</span>) = <span class="number">2</span>^<span class="built_in">i</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">&gt;&gt; v</span><br><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">           <span class="number">2</span></span><br><span class="line">           <span class="number">4</span></span><br><span class="line">           <span class="number">8</span></span><br><span class="line">          <span class="number">16</span></span><br><span class="line">          <span class="number">32</span></span><br><span class="line">          <span class="number">64</span></span><br><span class="line">         <span class="number">128</span></span><br><span class="line">         <span class="number">256</span></span><br><span class="line">         <span class="number">512</span></span><br><span class="line">        <span class="number">1024</span></span><br></pre></td></tr></table></figure>
<h4 id="while-loop">while-loop</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> true, </span><br><span class="line">  v(<span class="built_in">i</span>) = <span class="number">999</span>; </span><br><span class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">6</span>,</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h4 id="script(function)">script(function)</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">% script file calcuate.m</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% function calculate has </span></span><br><span class="line"><span class="comment">% one input - x </span></span><br><span class="line"><span class="comment">% two ouputs - y1 &amp; y2</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y1, y2]</span> = <span class="title">calculate</span><span class="params">(x)</span></span></span><br><span class="line"></span><br><span class="line">y1 = x^<span class="number">2</span>;</span><br><span class="line">y2 = x^<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<p>Terminal</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="matrix">[a, b]</span> = calculate(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">    <span class="number">25</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">b =</span><br><span class="line"></span><br><span class="line">   <span class="number">125</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Cost_function_J_example">Cost function J example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcost-function-j.png" alt=""></p>
<h3 id="Vectorization"><center>Vectorization</center></h3><ul>
<li>take advantage of linear algrba library  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fvect-example.png" alt=""></p>
<h4 id="unvectorized_implementation">unvectorized implementation</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">prediction = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n+<span class="number">1</span>,</span><br><span class="line">	prediction = prediction + theta(<span class="built_in">j</span>) * x(<span class="built_in">j</span>)</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<h4 id="Vectorized_implementation">Vectorized implementation</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">prediction = theta<span class="operator">'</span> * x;</span><br></pre></td></tr></table></figure>
<h4 id="Gradient_descent">Gradient descent</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png" alt=""></p>
<p><strong>Vectorized Implementation</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by </span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line">    tempTheta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta, <span class="number">1</span>) , <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: <span class="built_in">size</span>(tempTheta, <span class="number">1</span>),</span><br><span class="line">        DiffVector = X * theta - y; <span class="comment">% 97 * 1 vector // VECTORIZATION</span></span><br><span class="line">        sum = X(:,<span class="built_in">i</span>)<span class="operator">'</span> * DiffVector; <span class="comment">% 1 x 97 times 97 x 1 = 1 x 1 (sum) // VECTORIZATION</span></span><br><span class="line">        tempTheta(<span class="built_in">i</span>) = theta(<span class="built_in">i</span>) - alpha * (<span class="number">1</span>/m) * sum;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    theta = tempTheta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration    </span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h1 id="Week_3:_Lesson_1">Week 3: Lesson 1</h1><h2 id="Classification_and_Representation"><center>Classification and Representation</center></h2><h3 id="Classification">Classification</h3><ul>
<li>introduce logistic regression  </li>
<li>classification problem  </li>
<li>start with 2 classes classification (binary classification)  </li>
</ul>
<h4 id="Classification_Example:">Classification Example:</h4><ul>
<li>email: spam/not spam  </li>
<li>online transaction: fraudulent(yes/no)  </li>
</ul>
<p>$$ y \in 0, 1 $$  </p>
<h4 id="Linear_Regression_to_do_Classification_problem">Linear Regression to do Classification problem</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-classification.png" alt="">  </p>
<ul>
<li>in this particular example, linear regression method is actually doing something reasonable  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-bad-example.png" alt=""></p>
<ul>
<li>as we add one more data point to the right, the hypothesis function become smaller slope, two data points previous classify as malignant(y = 1) now become non-malignant.    </li>
<li>in most of case, linear regression is not the best approach for classification problem  </li>
</ul>
<h4 id="Logistic_Regression:">Logistic Regression:</h4><ul>
<li>an classification algorithm  </li>
</ul>
<p>$$0\leq h_\theta(x) \leq 1$$</p>
<h3 id="Hypothesis_Representation">Hypothesis Representation</h3><ul>
<li>output value between 0 and 1 $0\leq h_\theta(x) \leq 1$  </li>
</ul>
<h4 id="Logistic_Regression_Model">Logistic Regression Model</h4><ul>
<li>want $0\leq h_\theta(x) \leq 1$   </li>
</ul>
<h4 id="Sigmoid_function/Logistic_Function">Sigmoid function/Logistic Function</h4><p>$g(z)$ is called Sigmoid function/Logistic function<br>$$h_\theta(x) = g(\theta^Tx) , where$$<br>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/sigmoid-function.png" alt=""></p>
<p>$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$</p>
<h4 id="Interpretation_of_Hypothesis_Output">Interpretation of Hypothesis Output</h4><ul>
<li>$h_\theta(x) $ = estimated probability that $y=1$ on input x  </li>
</ul>
<p>$H_\theta(x) = P(y=1|x;\theta)$ reads the probability of $y=1$ given patient with feature $x$, parameterized by $\theta$. </p>
<h3 id="Decision_Boundary">Decision Boundary</h3><ul>
<li>virtualization </li>
<li>better sense of Logistic regression  </li>
</ul>
<h4 id="logistic_regression">logistic regression</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/logistic-regression.png" alt=""></p>
<p>$g(z) \geq 0.5$ when $z \geq 0$, since $h<em>\theta(x) = g(\theta^Tx)$, therefore, $h</em>\theta(x) \geq 0.5$ where $\theta^Tx \geq 0 $</p>
<p>Similarly, $h_\theta(x) &lt; 0.5$ where $\theta^Tx &lt; 0 $</p>
<h4 id="Decision_Boundary_Vitualizaiton">Decision Boundary Vitualizaiton</h4><ul>
<li>once $\theta$ value fixed, the line for decision boundary was also fixed  </li>
<li>data points from one side of the decision boundary will result in a different <code>y values</code> from data points from the other side of the decision boundary  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/decision-boundary.png" alt=""></p>
<h4 id="Non-linear_decision_boundaries">Non-linear decision boundaries</h4><ul>
<li>decision boundary is <strong>non-linear</strong>  </li>
<li>we can add <strong>higher order polynomial terms</strong> into the logistic regression  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/non-linear-logistic-regression.png" alt=""></p>
<ul>
<li>decision boundary is determined by the <strong>parameter of the hypothesis</strong> function, not the <strong>traning set</strong>. </li>
<li>Traning set is used to determine a better set of $\theta$ values, once $\theta$ values are set, decision boundary is fixed.  </li>
</ul>
<h1 id="Week_3:_Lesson_2">Week 3: Lesson 2</h1><h1 id="Week_3:_Lesson_3">Week 3: Lesson 3</h1><h1 id="Logs">Logs</h1><ul>
<li>06/19/2016: Complete week 1 notes and quizs  </li>
<li>06/21/2016: week 1 linear algrba review completed</li>
<li>06/27/2016: week 2 notes updated</li>
<li>06/27/2016: updated week 2: added notes for feature scaling &amp; mean normalization.  </li>
<li>06/28/2016: week 3 started: logistic regression, decision boundary  </li>
</ul>
<h1 id="Reference">Reference</h1><p><a href="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" target="_blank" rel="external">Front cover from Coursera</a>  </p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag">#Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/06/09/learning-how-to-learn/" rel="next" title="Coursera: Learning How to Learn by UCSD">
                <i class="fa fa-chevron-left"></i> Coursera: Learning How to Learn by UCSD
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/28/Search-for-a-range/" rel="prev" title="Lintcode 61 - Search For a Range">
                Lintcode 61 - Search For a Range <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/06/18/ML/"
     data-title="Coursera: Machine Learning by Stanford"
     data-content=""
     data-url="http://www.xuyiruan.com/2016/06/18/ML/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/06/18/ML/"
           data-title="Coursera: Machine Learning by Stanford" data-url="http://www.xuyiruan.com/2016/06/18/ML/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/author.jpg"
               alt="🍣之神" />
          <p class="site-author-name" itemprop="name">🍣之神</p>
          <p class="site-description motion-element" itemprop="description">阮先生’s blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">22</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ruanxuyi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/xuyiruan/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/xuyi-ruan-a728a889" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://mszhuchinese.com" title="MsZhuChinese" target="_blank">MsZhuChinese</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.winniebabe.com" title="WinnieBabe" target="_blank">WinnieBabe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://suncuss.me" title="BossSun" target="_blank">BossSun</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_3"><span class="nav-number">1.</span> <span class="nav-text">Week 1: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervise_Learning"><span class="nav-number">1.1.</span> <span class="nav-text">Supervise Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression:"><span class="nav-number">1.1.1.</span> <span class="nav-text">Regression:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification:"><span class="nav-number">1.1.2.</span> <span class="nav-text">Classification:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM(Support_Vector_Machine):"><span class="nav-number">1.1.3.</span> <span class="nav-text">SVM(Support Vector Machine):</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised_Learning"><span class="nav-number">1.2.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster_Learning:"><span class="nav-number">1.2.1.</span> <span class="nav-text">Cluster Learning:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cocktail_Party_Problem_Algorithm"><span class="nav-number">1.2.2.</span> <span class="nav-text">Cocktail Party Problem Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_5"><span class="nav-number">2.</span> <span class="nav-text">Week 1: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model_Representation"><span class="nav-number">2.1.</span> <span class="nav-text">Model Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Univariate_Linear_Regression"><span class="nav-number">2.1.1.</span> <span class="nav-text">Univariate Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Function"><span class="nav-number">2.2.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Fucntion_-_Intro_I"><span class="nav-number">2.3.</span> <span class="nav-text">Cost Fucntion - Intro I</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Find_$\theta_1$_by_minimizing_$J(\theta_1)$_(local_MIN_of_$J(\theta_1)$)"><span class="nav-number">2.3.1.</span> <span class="nav-text">Find $\theta_1$ by minimizing $J(\theta_1)$ (local MIN of $J(\theta_1)$)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Fucntion_-_Intro_II"><span class="nav-number">2.4.</span> <span class="nav-text">Cost Fucntion - Intro II</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost_Function_in_3D"><span class="nav-number">2.4.1.</span> <span class="nav-text">Cost Function in 3D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Counter_Plot"><span class="nav-number">2.4.2.</span> <span class="nav-text">Counter Plot</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_6"><span class="nav-number">3.</span> <span class="nav-text">Week 1: Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent_Intuition"><span class="nav-number">3.2.</span> <span class="nav-text">Gradient Descent Intuition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent_For_Linear_Regression"><span class="nav-number">3.3.</span> <span class="nav-text">Gradient Descent For Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_Algorithm"><span class="nav-number">3.3.1.</span> <span class="nav-text">Gradient Descent Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#“Batch”_Gradient_Descent_Algorithm"><span class="nav-number">3.3.2.</span> <span class="nav-text">“Batch” Gradient Descent Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_7"><span class="nav-number">4.</span> <span class="nav-text">Week 1: Lesson 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrices_and_Vectors"><span class="nav-number">4.1.</span> <span class="nav-text">Matrices and Vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix:"><span class="nav-number">4.1.1.</span> <span class="nav-text">Matrix:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vector:"><span class="nav-number">4.1.2.</span> <span class="nav-text">Vector:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Addition_and_Scalar_Multiplicaiton"><span class="nav-number">4.2.</span> <span class="nav-text">Addition and Scalar Multiplicaiton</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Addition"><span class="nav-number">4.2.1.</span> <span class="nav-text">Addition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scalar_Multication"><span class="nav-number">4.2.2.</span> <span class="nav-text">Scalar Multication</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Vector_Multiplication"><span class="nav-number">4.3.</span> <span class="nav-text">Matrix Vector Multiplication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition:"><span class="nav-number">4.3.1.</span> <span class="nav-text">Definition:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example:"><span class="nav-number">4.3.2.</span> <span class="nav-text">Example:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advantage_of_using_Matrix_Multiplication"><span class="nav-number">4.3.3.</span> <span class="nav-text">Advantage of using Matrix Multiplication</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convert_predicting_housing_price_into_Matrix_Multiplication"><span class="nav-number">4.3.4.</span> <span class="nav-text">Convert predicting housing price into Matrix Multiplication</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Matrix_Multiplication"><span class="nav-number">4.4.</span> <span class="nav-text">Matrix Matrix Multiplication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Example"><span class="nav-number">4.4.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition"><span class="nav-number">4.4.2.</span> <span class="nav-text">Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#One_more_example"><span class="nav-number">4.4.3.</span> <span class="nav-text">One more example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Application_of_M-M-M"><span class="nav-number">4.4.4.</span> <span class="nav-text">Application of M-M-M</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Multiplication_Properties"><span class="nav-number">4.5.</span> <span class="nav-text">Matrix Multiplication Properties</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Commutative"><span class="nav-number">4.5.1.</span> <span class="nav-text">Commutative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Associative"><span class="nav-number">4.5.2.</span> <span class="nav-text">Associative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Identity_Matrix"><span class="nav-number">4.5.3.</span> <span class="nav-text">Identity Matrix</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverse_and_Transpose"><span class="nav-number">4.6.</span> <span class="nav-text">Inverse and Transpose</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix_inverse:"><span class="nav-number">4.6.1.</span> <span class="nav-text">Matrix inverse:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example_1"><span class="nav-number">4.6.2.</span> <span class="nav-text">Example 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix_Transpose"><span class="nav-number">4.6.3.</span> <span class="nav-text">Matrix Transpose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition-1"><span class="nav-number">4.6.4.</span> <span class="nav-text">Definition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson2"><span class="nav-number">5.</span> <span class="nav-text">Week 2: Lesson2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multivariate_Linear_Regression"><span class="nav-number">5.1.</span> <span class="nav-text">Multivariate Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mltivariate_Linear_Regression"><span class="nav-number">5.1.1.</span> <span class="nav-text"> Mltivariate Linear Regression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple_features"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">Multiple features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#New_Hypothesis_function"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">New Hypothesis function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Formal_Definition"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">Formal Definition</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_for_Multiple_Variables"><span class="nav-number">5.1.2.</span> <span class="nav-text"> Gradient Descent for Multiple Variables </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_in_Practice_I_-_Feature_Scaling"><span class="nav-number">5.1.3.</span> <span class="nav-text"> Gradient Descent in Practice I - Feature Scaling </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature_Scaling"><span class="nav-number">5.1.3.1.</span> <span class="nav-text">Feature Scaling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mean_Normalization"><span class="nav-number">5.1.3.2.</span> <span class="nav-text">Mean Normalization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_in_Practice_II_-_Learning_Rate"><span class="nav-number">5.1.4.</span> <span class="nav-text"> Gradient Descent in Practice II - Learning Rate </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Making_sure_gradient_descent_work_correctly"><span class="nav-number">5.1.4.1.</span> <span class="nav-text">Making sure gradient descent work correctly</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Possible_bugs"><span class="nav-number">5.1.4.2.</span> <span class="nav-text">Possible bugs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#To_choose_α,_try(three-fold)"><span class="nav-number">5.1.4.3.</span> <span class="nav-text">To choose α, try(three-fold)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Features_and_Polynomial_Regression"><span class="nav-number">5.1.5.</span> <span class="nav-text"> Features and Polynomial Regression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Definiing_new_feature"><span class="nav-number">5.1.5.1.</span> <span class="nav-text">Definiing new feature</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Polynomical_Regression"><span class="nav-number">5.1.5.2.</span> <span class="nav-text">Polynomical Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choose_a_feature"><span class="nav-number">5.1.5.3.</span> <span class="nav-text">Choose a feature</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson_3"><span class="nav-number">6.</span> <span class="nav-text">Week 2: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Computing_Parameters_Analytically"><span class="nav-number">6.1.</span> <span class="nav-text">Computing Parameters Analytically</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal_Equation"><span class="nav-number">6.1.1.</span> <span class="nav-text">Normal Equation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-1"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Equation"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Descent_vs_Dis-advantage"><span class="nav-number">6.1.1.3.</span> <span class="nav-text">Gradient Descent vs Dis-advantage</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal_Equation_Noninvertibility"><span class="nav-number">6.1.2.</span> <span class="nav-text">Normal Equation Noninvertibility</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pinv_vs_inv_in_Octave"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">pinv vs inv in Octave</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Case_where_$X^TX$_is_non-invertible"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">Case where $X^TX$ is non-invertible</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson_6"><span class="nav-number">7.</span> <span class="nav-text">Week 2: Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Octave/Matlab_Tutorial"><span class="nav-number">7.1.</span> <span class="nav-text">Octave/Matlab Tutorial</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic_Operations"><span class="nav-number">7.1.1.</span> <span class="nav-text">Basic Operations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Moving_Data_Around"><span class="nav-number">7.1.2.</span> <span class="nav-text">Moving Data Around</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A(row_index,_col_index)"><span class="nav-number">7.1.2.1.</span> <span class="nav-text">A(row_index, col_index)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Computing_On_Data"><span class="nav-number">7.1.3.</span> <span class="nav-text">Computing On Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Plotting_Data"><span class="nav-number">7.1.4.</span> <span class="nav-text">Plotting Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Control_Statements:_for,_while,_if_statement"><span class="nav-number">7.1.5.</span> <span class="nav-text">Control Statements: for, while, if statement</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#for-loop"><span class="nav-number">7.1.5.1.</span> <span class="nav-text">for-loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#while-loop"><span class="nav-number">7.1.5.2.</span> <span class="nav-text">while-loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#script(function)"><span class="nav-number">7.1.5.3.</span> <span class="nav-text">script(function)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost_function_J_example"><span class="nav-number">7.1.5.4.</span> <span class="nav-text">Cost function J example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorization"><span class="nav-number">7.1.6.</span> <span class="nav-text">Vectorization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#unvectorized_implementation"><span class="nav-number">7.1.6.1.</span> <span class="nav-text">unvectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vectorized_implementation"><span class="nav-number">7.1.6.2.</span> <span class="nav-text">Vectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_descent"><span class="nav-number">7.1.6.3.</span> <span class="nav-text">Gradient descent</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_1"><span class="nav-number">8.</span> <span class="nav-text">Week 3: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification_and_Representation"><span class="nav-number">8.1.</span> <span class="nav-text">Classification and Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification"><span class="nav-number">8.1.1.</span> <span class="nav-text">Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Classification_Example:"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">Classification Example:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear_Regression_to_do_Classification_problem"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Linear Regression to do Classification problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_Regression:"><span class="nav-number">8.1.1.3.</span> <span class="nav-text">Logistic Regression:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hypothesis_Representation"><span class="nav-number">8.1.2.</span> <span class="nav-text">Hypothesis Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_Regression_Model"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">Logistic Regression Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid_function/Logistic_Function"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">Sigmoid function/Logistic Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interpretation_of_Hypothesis_Output"><span class="nav-number">8.1.2.3.</span> <span class="nav-text">Interpretation of Hypothesis Output</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision_Boundary"><span class="nav-number">8.1.3.</span> <span class="nav-text">Decision Boundary</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#logistic_regression"><span class="nav-number">8.1.3.1.</span> <span class="nav-text">logistic regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision_Boundary_Vitualizaiton"><span class="nav-number">8.1.3.2.</span> <span class="nav-text">Decision Boundary Vitualizaiton</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-linear_decision_boundaries"><span class="nav-number">8.1.3.3.</span> <span class="nav-text">Non-linear decision boundaries</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_2"><span class="nav-number">9.</span> <span class="nav-text">Week 3: Lesson 2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_3"><span class="nav-number">10.</span> <span class="nav-text">Week 3: Lesson 3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logs"><span class="nav-number">11.</span> <span class="nav-text">Logs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">12.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">🍣之神</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"xruan"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("oKoPG1XTXkC7T1oasGQwau2g-gzGzoHsz", "JFJCQtIsUKhrX6S7Gvgqdqgk");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
