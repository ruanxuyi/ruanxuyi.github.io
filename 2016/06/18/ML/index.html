<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Bree Serif:300,300italic,400,400italic,700,700italic|Arimo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/sushi.png?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera: Machine Learning by Stanford">
<meta property="og:url" content="http://www.xuyiruan.com/2016/06/18/ML/index.html">
<meta property="og:site_name" content="阮先生de小窝">
<meta property="og:description">
<meta property="og:image" content="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/news.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/genes.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/market.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cocktail.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/univariate-linear-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta3.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function-2para.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/sim-update.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec-alpha.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/fix-alpha.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/partial-derivative.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160620/best-hyphthesis-func.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/scalar-multiplication.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi-ex.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vs-for-loop.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-def.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-ex2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/identity-matrix.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/martix-idneitiy-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/singular.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/transpose.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160621/flip-matrix-transpose.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-before.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-after.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent-decrease.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Falpha-too-small.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fpolynomial-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fsqrt-root-feature.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fnormal-equ-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fx-inverse.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-desc-vs-normal-equ.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2FmyPlot.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcolormap.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcost-function-j.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fvect-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-classification.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-bad-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/sigmoid-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/logistic-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/decision-boundary.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160628/non-linear-logistic-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/convex.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/non-convex.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/logistic-regression-cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/fminunc.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/logistic-regression-with-advanced-optimzation-algorithm.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/multi-class-classification.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/one-vs-all-classifcation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/over-fitting.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/large-lambda.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/gradient-descent-with-regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/normal-equation-regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/non-invertable-regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/regularized-logistic-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/logistic-regression-gradient-descent.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160703/advanced-optimzation-regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/car.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neuron.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/simple-neuron-model.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/math-layer-represnetiation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/learn-its-own-feature.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/other-structure.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-and.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/or-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/negation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-xor.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/handwritten-digit-recognization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/multiclass-classification.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise2.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera: Machine Learning by Stanford">
<meta name="twitter:description">




<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> Coursera: Machine Learning by Stanford | 阮先生de小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">阮先生de小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">多读书 多思考 少吃零食 多睡觉</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'TVdsuUVez9DD1Mx-gjc8','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Coursera: Machine Learning by Stanford
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-06-18T15:11:11-07:00" content="06-18-2016">
              06-18-2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/06/18/ML/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/06/18/ML/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/06/18/ML/" class="leancloud_visitors" data-flag-title="Coursera: Machine Learning by Stanford">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" alt=""></p>
<a id="more"></a>
<h1 id="Week_1:_Lesson_3">Week 1: Lesson 3</h1><h2 id="Supervise_Learning">Supervise Learning</h2><p><strong>Examples:</strong></p>
<ul>
<li>Given email labeled as spam/not spam, learn a spam filter. (classification)</li>
<li>Given a dataset of patients diagnosed as either having diabetes or not, learn to classify new patients as having diabtetes or not. (classification)</li>
<li>Given data of different house types (room, square feet, year built, wood/concrete.. etc) with their corresponding price, evaluate/preidct the price of a given house. (Regression)</li>
</ul>
<h3 id="Regression:">Regression:</h3><ul>
<li>predict value according to the data set.  </li>
<li>predict <strong>continuous valued</strong> output(price)  </li>
</ul>
<h3 id="Classification:">Classification:</h3><ul>
<li><strong>discrete valued</strong> output(0 or 1).</li>
</ul>
<h3 id="SVM(Support_Vector_Machine):">SVM(Support Vector Machine):</h3><ul>
<li>process of infinte number of features.   </li>
</ul>
<h2 id="Unsupervised_Learning">Unsupervised Learning</h2><p><strong>Defination:</strong>   </p>
<ul>
<li>did not telling the algorithm how to categorize data sets into different types in advance.  </li>
<li>automatically find structure/categories on given data sets.  </li>
</ul>
<h3 id="Cluster_Learning:">Cluster Learning:</h3><p><strong>Examples:</strong>  </p>
<ul>
<li><p>Given a set of news articles found on the web, group them into set of articles about the same story. (Cluster)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/news.png" alt=""></p>
</li>
<li><p>automatically analyzes data sets and group them into different type of genes sequence for diffrent group of individuals’ data set. (Cluster)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/genes.png" alt=""></p>
</li>
</ul>
<p><strong>Applications:</strong>  </p>
<ul>
<li>Market Segementation:  algorithm automatically categorizes customers into different clusters according to customers’ data, which make marketing and advertisement more effecients and effecitve.   (Airline company tries to categorizes their frequent flyers to different types to better advertises products for different type of frequent flyiers [YMMV - different people get different targeted advertisement])</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/market.png" alt=""></p>
<h3 id="Cocktail_Party_Problem_Algorithm">Cocktail Party Problem Algorithm</h3><ul>
<li>seperate overlaped audio and let <code>cocktail party algorithm</code> to seperate them into different audios tracks.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cocktail.png" alt=""></p>
<h1 id="Week_1:_Lesson_5">Week 1: Lesson 5</h1><h2 id="Model_Representation">Model Representation</h2><ul>
<li>m = Number of training examples </li>
<li>x’s = “input” variable/ feature</li>
<li>y’s = “output” variable / target variable</li>
<li>(x,y) = one training example.</li>
<li>(x<sup>i</sup> , y<sup>i</sup>) = i<sup>th</sup> training example.  </li>
</ul>
<p>Traning set -&gt; Learning Algorithm -&gt; h(hypothesis) -&gt; </p>
<blockquote>
<p>h(hypothesis) - a function that maps from x’s -&gt; y’s. </p>
</blockquote>
<pre><code><span class="title">size</span> <span class="keyword">of</span> house -&gt; h -&gt; estimated price
</code></pre><h3 id="Univariate_Linear_Regression">Univariate Linear Regression</h3><ul>
<li>Linear Regression with <strong>one variable</strong>. (x)<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/univariate-linear-regression.png" alt=""></li>
</ul>
<h2 id="Cost_Function">Cost Function</h2><ul>
<li>fit the best straigt line to our data  </li>
</ul>
<p>Given training set and Hypothesis:  h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x  (linear, single variable)  </p>
<p>How to choose θ<sub>i</sub>‘s (Paramenters)?  </p>
<ul>
<li><p>choose θ<sub>0</sub>, θ<sub>1</sub> so that h(x) is close to <code>y</code> for our training examples<code>(x,y)</code></p>
</li>
<li><p>minimize $ \frac 1 {2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $(m - # training examples)</p>
</li>
</ul>
<p><strong>Hypothesis Function:</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x$$
</p>
<p><strong>Cost Function(Squared error function)</strong><br>
$$J(\theta_0, \theta_1) = \frac 1 {2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$
<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function.png" alt=""></p>
<h2 id="Cost_Fucntion_-_Intro_I">Cost Fucntion - Intro I</h2><ul>
<li>difference and relation between Hypothesis Function and Cost Function   </li>
<li>pick a Hypothesis Function ($\theta_0=0$ pick $\theta_1$)  </li>
<li>Graphically understand the chice of $\theta_1$ affect Hypothesis Function and Cost Function.  </li>
</ul>
<p>Hypothesis function $h_0(x)$<br>is a funtion of house area $x$ and price of that house.<br>
$$h_{\theta}(x)=\theta_1 x$$ 
</p>
<p>Cost function $J(\theta_1)$ is a funciton of $\theta_1$, assuming $\theta_0=0$.</p>

$$J(\theta_1) = \frac 1 {2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$

<h3 id="Find_$\theta_1$_by_minimizing_$J(\theta_1)$_(local_MIN_of_$J(\theta_1)$)">Find $\theta_1$ by minimizing $J(\theta_1)$ (local MIN of $J(\theta_1)$)</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta1.png" alt=""></p>
<p>$$ \theta_1 = 1$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta2.png" alt=""></p>
<p>$$ \theta_1 = 0.5 $$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/theta3.png" alt=""></p>
<p>$$ \theta_1 = 0 $$ </p>
<p>In this case, the local min happens at $ \theta_1 = 1$.</p>
<h2 id="Cost_Fucntion_-_Intro_II">Cost Fucntion - Intro II</h2><ul>
<li>better understanding of the cost function  </li>
<li>understand contour plots  </li>
<li><strong>manually</strong> looking for $\theta_0$ and $\theta1$ pair that minimize the cost function. </li>
</ul>
<p>Pick a hypothesis funciton $\theta_0$ and $\theta1$. In this example, we assume $\theta_0\neq0$.</p>
<h3 id="Cost_Function_in_3D">Cost Function in 3D</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/cost-function-2para.png" alt=""></p>
<h3 id="Counter_Plot">Counter Plot</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot1.png" alt=""></p>
<p>$$ \theta_0 = 360,\theta_1 = 0 $$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/counter-plot2.png" alt=""></p>
<p>$$ \theta_0 = 220,\theta_1 = 0.15 $$</p>
<h1 id="Week_1:_Lesson_6">Week 1: Lesson 6</h1><h2 id="Gradient_Descent">Gradient Descent</h2><ul>
<li>Gradient Descent - an algorithm to minimize the cost function J and other function.  </li>
<li>very general algorithm that used a lot in ML.  </li>
<li>Gradient - downhill as rapidly as possible. (take the steepest slope)   </li>
</ul>
<p><strong>Steps</strong>   </p>
<ol>
<li>start with some $ \theta_0 ,\theta_1 $ ($ \theta_0 = 0 ,\theta_1 = 0$)  </li>
<li>Keep chaning $ \theta_0 ,\theta_1 $ to reduce $J(\theta_0, \theta_1)$ until it reaches local minimum.  </li>
</ol>
<p><strong>Example</strong></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec1.png" alt=""></p>
<p>Start with a point that is little bit to the right result in a <strong>different path</strong> to another local-minima.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec2.png" alt=""></p>
<p><strong>Algorithm</strong><br>
$$ \theta_i :=\theta_i - \alpha \frac {\partial}{\partial\theta_i} J(\theta_0,\theta_1),(i = 0, i = 1)$$    
 </p>
<p>$:= $ - assign statement<br>$\alpha$ - learning rate (how far each step should go down hill)</p>
<p><strong>Implementation</strong><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/sim-update.png" alt="Simultaneous Update"></p>
<h2 id="Gradient_Descent_Intuition">Gradient Descent Intuition</h2><ul>
<li>better intuition of Gradient Descent Algorithm.  </li>
<li>understand the derivative term.  </li>
</ul>

$$ \theta_1 :=\theta_1 - \alpha \frac {\partial}{\partial\theta_1} J(\theta_1)$$    
 
<ul>
<li>if $\alpha$ is too small, gradient decent can be slow.   </li>
<li>if $\alpha$ too large, gradient decent can overshoot the minimum. It may fails to converge.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/gradient-dec-alpha.png" alt=""></p>
<p>As we approach a local minimum, the gradient decent will <strong>automatically</strong> take <strong>smaller steps</strong> even with fixed learning rate $\alpha$. </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/fix-alpha.png" alt=""></p>
<p>Even $\alpha$ is fixed, slope or $\frac {\partial}{\partial\theta_1} J(\theta_1)$ is decreasing. Therefore, the overall term $\alpha \frac {\partial}{\partial\theta_1} J(\theta_1) $ decreases.</p>
<h2 id="Gradient_Descent_For_Linear_Regression">Gradient Descent For Linear Regression</h2><ul>
<li>apply gradient descent algorithm to minimize suqare error cost function (J).  </li>
</ul>

$$ \theta_i :=\theta_i - \alpha \frac {\partial}{\partial\theta_i} J(\theta_0,\theta_1),(i = 0, i = 1)$$    
 
<p>After taking partial derivative:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/partial-derivative.png" alt=""></p>
<h3 id="Gradient_Descent_Algorithm">Gradient Descent Algorithm</h3>
$$ \theta_0 :=\theta_0 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)}) $$    

$$ \theta_1 :=\theta_1 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x^{(i)} $$    
 
<blockquote>
<p>IMPORTANT: $\theta_0$ and $\theta_1$ need to be updated <strong>simultaneously</strong>. </p>
</blockquote>
<p>Using Gradient Descnet Algorithm to find the best Hyphothesis Function.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160620/best-hyphthesis-func.png" alt=""></p>
<h3 id="“Batch”_Gradient_Descent_Algorithm">“Batch” Gradient Descent Algorithm</h3><ul>
<li>Batch: each step of gradient descent uses <strong>all the training examples/data</strong>.  </li>
<li>
$$\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$
</li>
<li>m = total # of training examples/data.  </li>
</ul>
<h1 id="Week_1:_Lesson_7">Week 1: Lesson 7</h1><h2 id="Matrices_and_Vectors">Matrices and Vectors</h2><h3 id="Matrix:">Matrix:</h3><ul>
<li>Rectangular array of numbers</li>
<li>CAP letter for Matrices(A,B,C,X.etc)  </li>
</ul>
<h3 id="Vector:">Vector:</h3><ul>
<li>An n x 1 matrix (only ONE column)  </li>
<li>Vector is a special type of matrices  </li>
<li>1-indexed(in course) vs 0-indexed(more in ML)  </li>
<li>lower case letter for value, vector (a, b, c, x, etc)  </li>
</ul>
<h2 id="Addition_and_Scalar_Multiplicaiton">Addition and Scalar Multiplicaiton</h2><h3 id="Addition">Addition</h3><ul>
<li>only add matrix of <strong>same dimension</strong>  </li>
</ul>
<h3 id="Scalar_Multication">Scalar Multication</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/scalar-multiplication.png" alt=""></p>
<h2 id="Matrix_Vector_Multiplication">Matrix Vector Multiplication</h2><h3 id="Definition:">Definition:</h3><ul>
<li>number of Column of A(n) has to MATCH # of row of x (n)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi.png" alt=""></p>
<h3 id="Example:">Example:</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vector-multi-ex.png" alt=""></p>
<h3 id="Advantage_of_using_Matrix_Multiplication">Advantage of using Matrix Multiplication</h3><ul>
<li><strong>simplify</strong> the actual code for implementation  </li>
<li>increase <strong>readiability</strong>  </li>
<li>more <strong>efficient</strong> to do matrix multiplication(left) than using for-loop(right) in case of large data-set  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-vs-for-loop.png" alt=""></p>
<h3 id="Convert_predicting_housing_price_into_Matrix_Multiplication">Convert predicting housing price into Matrix Multiplication</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png" alt=""></p>
<h2 id="Matrix_Matrix_Multiplication">Matrix Matrix Multiplication</h2><ul>
<li>how to multiply two matrix together  </li>
</ul>
<h3 id="Example">Example</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi.png" alt="">  </p>
<blockquote>
<p><strong>matrix-matrix multiplication</strong> is simply a break down steps of <strong>matrix-vector multiplication</strong>   </p>
</blockquote>
<h3 id="Definition">Definition</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-def.png" alt=""></p>
<blockquote>
<p>A’s column# has to <strong>match</strong> with B’s row#. </p>
</blockquote>
<h3 id="One_more_example">One more example</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-ex2.png" alt=""></p>
<blockquote>
<p>TIPs: break down <strong>matrix-matrix multiplication</strong> into many <strong>matrix-vector multiplication</strong> will make your job more easier.  </p>
</blockquote>
<h3 id="Application_of_M-M-M">Application of M-M-M</h3><ul>
<li>predicting house price again, YES!  </li>
<li>as before we have four house sizes  </li>
<li>but now we have THREE <strong>different</strong> hypothesis functions  </li>
<li>we want to predict house prices base on three different models  </li>
<li>higly <strong>optimized</strong> linear algrba <strong>library</strong> are able to help us with complicated matrix-matrix-multiplicaiton  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/matrix-matrix-multi-application.png" alt=""></p>
<h2 id="Matrix_Multiplication_Properties">Matrix Multiplication Properties</h2><ul>
<li>benefit: pack lots of complication into just on matrix multiplication operation  </li>
<li>but be aware of the properties of matrix multiplication  </li>
</ul>
<h3 id="Commutative">Commutative</h3><ul>
<li>Matrix multiplication is NOT commutative<br>$$ A\times B\neq B\times A $$ </li>
</ul>
<h3 id="Associative">Associative</h3><ul>
<li>has to match both <strong>dimension</strong> and <strong>values</strong>  </li>
<li>Matrix multiplication IS associative  </li>
</ul>
<p>$$ A \times B \times C = A \times (B \times C)$$</p>
<h3 id="Identity_Matrix">Identity Matrix</h3><ul>
<li>denoted as $I$(or $I_{n\times n}$)  </li>
<li>Examples of identity matrices:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/identity-matrix.png" alt=""></li>
<li>Commutative property holds for identity matrix  </li>
</ul>
<p>For any matrix A, </p>

$$A*I_1 = I_2*A = A$$

<blockquote>
<p>If A has $m\times n$ dimension -&gt; $I_1$ has $n\times n$ dimension, while $I_2$ has $m\times m$ dimension  </p>
</blockquote>
<h2 id="Inverse_and_Transpose">Inverse and Transpose</h2><ul>
<li>1 = “identity”  </li>
<li>not all numbesr have an inverse(ex. 0)  </li>
<li>how to compute inverse of a matrix  </li>
</ul>
<h3 id="Matrix_inverse:">Matrix inverse:</h3><p>If A is an $m x m$ matrix, and if it has an inverse,  </p>

$$A*A^{-1} = A^{-1}*A = I$$  

<h3 id="Example_1">Example 1</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/martix-idneitiy-example.png" alt="">  </p>
<ul>
<li>How to compute that? We use higly optimized software to compute it.  </li>
<li>not all matrices have inverse. (a 2x2 matrices that has all zeros in it has NO inverse since there is no way to create diagonal 1’s(identity) on the result matrices)  </li>
<li>Matrices that don’t have an inverse are “<strong>singular</strong>“ or “<strong>degenerate</strong>“   </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/singular.png" alt="singular matrices"></p>
<h3 id="Matrix_Transpose">Matrix Transpose</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/transpose.png" alt=""></p>
<h3 id="Definition-1">Definition</h3><p>Let $A$ be an $m\times n$ matrix, and let $B=A^T$. Then $b$ is an $n\times m$ matrix, and $$B_{ij} = A_{ji}$$ .  </p>
<p>In example above:  </p>
 
$A_{12} = B_{21} = 2$, and  $A_{22} = B_{22} = 5$

<blockquote>
<p>Tips: simply filp the matrices <strong>along the 45 degree line</strong> will result in Matrix Transpose.  </p>
</blockquote>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160621/flip-matrix-transpose.png" alt=""></p>
<h1 id="Week_2:_Lesson2">Week 2: Lesson2</h1><h2 id="Multivariate_Linear_Regression">Multivariate Linear Regression</h2><h3 id="Mltivariate_Linear_Regression"><center> Mltivariate Linear Regression </center></h3><ul>
<li>multiple properities/features  </li>
<li>more useful model  </li>
</ul>
<h4 id="Multiple_features">Multiple features</h4><ul>
<li>$n$ = number of features  </li>
<li>$x^{(i)}$ = input (features) of $i^{th}$ traning example (a column vector n x 1) </li>
<li>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ traning example  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png" alt=""></p>
<p>Example above has <strong>four</strong> features, therefore, $n=4$. Since there are 47 samples/training data, $m=47$. $x^{2}$ indicates the column vector $ \begin{bmatrix}<br>1046 \\ 3 \\ 2 \\ 40 \\ \end{bmatrix} $ and the notation $x_2^{2}$ indicates <strong>second</strong> feature(# of bedrooms) from <strong>second</strong> training data.  </p>
<h4 id="New_Hypothesis_function">New Hypothesis function</h4><p><strong>Previously:(one feature)</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x$$
</p>
<p><strong>Now:(four features)</strong><br>
$$h_{\theta}(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+\theta_3 x_3+\theta_4 x_4$$
</p>
<p><strong>Example Hypthesis function:</strong><br>
$$h_{\theta}(x)=80+0.1 x_1+0.01 x_2+3 x_3-2 x_4$$
</p>
<ul>
<li>$80$ - base price  </li>
<li>$0.1x_1$ - size <strong>added up</strong> by factor of $0.1$ to base price  </li>
<li>$0.01x_2$ - number of bedrooms <strong>add up</strong> by a lower factor of $0.01$ to base price  </li>
<li>$3x_3$ - number of floor <strong>add up</strong> by a factor of 3 to base price  </li>
<li>$-2x_4$ - age of the house <strong>subtract by</strong> factor of 2 on base price  </li>
</ul>
<h4 id="Formal_Definition">Formal Definition</h4>
$$h_{\theta}(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n$$

<blockquote>
<p>For convenience of notation, we defined</p>
</blockquote>
<p> $$x_0 = 1$$.  </p>

$$h_{\theta}(x)=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n$$

<p>$x = \begin{bmatrix}<br>x_0 \\<br>x_1 \\<br>x_2 \\<br>… \\<br>x_n<br>\end{bmatrix}$, $\theta = \begin{bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>\theta_2 \\<br>… \\<br>\theta_n<br>\end{bmatrix}$, $\theta^T = \begin{bmatrix}<br>\theta_0,<br>\theta_1,<br>\theta_2,<br>…<br>\theta_n<br>\end{bmatrix}$</p>
<p>Therefore:  </p>

$$h_{\theta}(x)=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n = \theta^T x$$

<p>Result of $\theta^T x$ is $[1\times n] [n\times 1]$ = $1\times1$ dimensional (prediction <strong>price</strong> of the house)</p>
<h3 id="Gradient_Descent_for_Multiple_Variables"><center> Gradient Descent for Multiple Variables </center></h3><ul>
<li>automatically fit the paramenter in hypothesis function for <strong>multiple</strong> features.    </li>
</ul>

$$ \theta_j :=\theta_j - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_j^{(i)} $$    


$$ \theta_0 :=\theta_0 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_0^{(i)}, where (x_0^{(i)} = 1)$$
$$ \theta_1 :=\theta_1 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_1^{(i)} $$ 
$$ \theta_2 :=\theta_2 - \alpha \frac {1}{m}\sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_2^{(i)} $$  
$$ ... $$

 
<h3 id="Gradient_Descent_in_Practice_I_-_Feature_Scaling"><center> Gradient Descent in Practice I - Feature Scaling </center></h3><ul>
<li>go over some practical tricks to make gradient decent work well: feature scaling   </li>
<li>idea: make sure features are on a <strong>similiar scale</strong>.  </li>
<li>so that the gradient descent will converge faster  </li>
</ul>
<p><strong>Original Scale</strong><br>$x_1 = size(0-2000 feet^2)$<br>$x_2 = number of bedrooms(1-5)$<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-before.png" alt=""></p>
<p><strong>After Scaling</strong>  </p>
<p>$x_1 = \frac{size(feet^2)}{2000}$<br>$x_2 = \frac{number of bedroom}{5}$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fscale-after.png" alt=""></p>
<p>Gradient Descent with scaling will result in a faster, <strong>direct path</strong> to find the local minimal and<strong> converge much faster, </strong>why??**</p>
<blockquote>
<p>Possible Reason: Feature Scaling + Mean Normalization allows updated $X$ values locate in a <strong>similiar</strong> &amp; <strong>smaller</strong> range ( $ -1 &lt; X &lt; 1$ ) instead of un-normalizaed range ($0&lt;x&lt;2000$). The <strong>smaller</strong>&amp;<strong>united</strong> range allow gradient descent to work <strong>better</strong> even with small value of $\alpha$.  </p>
</blockquote>
<h4 id="Feature_Scaling">Feature Scaling</h4><p>Get every featrue into <strong>approximately</strong> a $-1 \leq x_i \leq 1$ range.  </p>
<p><strong>Examples of Good Scaling:</strong>  </p>
<ul>
<li>$0\leq x_1\leq3$  </li>
<li>$-2\leq x_2\leq0.5$<br><strong>Examples of BAD Scaling:</strong>   </li>
<li>$-100\leq x_1\leq100$  </li>
<li>$-0.001\leq x_1\leq0.001$</li>
</ul>
<h4 id="Mean_Normalization">Mean Normalization</h4><ul>
<li>another techique to enhance feature scaling’s perforamnce      </li>
<li>Replace $x_i$ with $x_i - \mu_i$  </li>
<li>Help gradient descent converge more <strong>faster</strong> </li>
</ul>

$$x_i = \frac{x_i-\mu_i}{s_i}$$

<ul>
<li>$\mu_i$ denotes average value of $x_i$ in training set  </li>
<li>$s_i$ denotes the range $(max - min)$ or standard deviation of traning set  </li>
</ul>
<h3 id="Gradient_Descent_in_Practice_II_-_Learning_Rate"><center> Gradient Descent in Practice II - Learning Rate </center></h3>
$$ \theta_j :=\theta_j - \alpha \frac {\partial}{\partial\theta_j} J(\theta)$$    
 
<ul>
<li>anther technique to make gradient descent work well in practice  </li>
<li>how to make sure gradient descent is working <strong>correctly</strong>  </li>
<li>how to <strong>choose</strong> learning rate $\alpha$.  </li>
</ul>
<h4 id="Making_sure_gradient_descent_work_correctly">Making sure gradient descent work correctly</h4><ul>
<li>find the best $theta$ value that <strong>minimize</strong> the $J(\theta)$  </li>
<li>$J(\theta)$ should <strong>decrease</strong> after every iterations  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent-decrease.png" alt=""></p>
<p><strong>Manually convergence test:</strong>  Lookinto the graph to check if gradient descent converge.  </p>
<p><strong>Automatically convergence test:</strong> Declare convergence if $J(\theta)$ decreases by less than $10^{-3}$ in one interation. </p>
<blockquote>
<p>determine a good threadshold ($10^{-3}$) is sometimes confusing  </p>
</blockquote>
<h4 id="Possible_bugs">Possible bugs</h4><ul>
<li>$\alpha$ too large  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Falpha-too-small.png" alt="">  </p>
<h4 id="To_choose_α,_try(three-fold)">To choose α, try(three-fold)</h4><p>$$…,0.001,0.003,0.01,0.03,0.1,0.3,1…$$ </p>
<h3 id="Features_and_Polynomial_Regression"><center> Features and Polynomial Regression </center></h3><ul>
<li><strong>choose</strong> appropriate features  </li>
<li><strong>polynomical regression</strong> for complicated, non-linear functions  </li>
</ul>
<h4 id="Definiing_new_feature">Definiing new feature</h4><ul>
<li>instead of using the feature provide, we might able to come up with new feature to simplify the gradient descent algorithm  </li>
</ul>
<p><strong>Given features:</strong> <code>frontage &amp; depth</code> of a house<br><strong>New feature:</strong> <code>Area</code> ($Area = frontage\times depth $)</p>
<h4 id="Polynomical_Regression">Polynomical Regression</h4><ul>
<li>fit complicate data set with more accurate polynomial function  </li>
<li>derive polynomical regression with <strong>multi-varient linear regression</strong><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fpolynomial-regression.png" alt=""> </li>
</ul>

$$h_0(x)=\theta_0+\theta_1\times x_1+\theta_2\times x_2+\theta_3 \times x_3 $$ 
$$ h_0(x)= \theta_0+\theta_1\times (size)+\theta_2\times (size)^2+\theta_3 \times (size)^3 $$ 
  
<p>where<br>feature 1: $x_1 = (size)$<br>feature 2: $x_2 = (size)^2$<br>feature 3: $x_3 = (size)^3$  </p>
<p><strong>Scale Range of the features</strong></p>
<p>feature 1: $(size): 1-1000$<br>feature 2: $(size)^2: 1-1000,000$<br>feature 3: $(size)^3: 1-10^9$ </p>
<h4 id="Choose_a_feature">Choose a feature</h4><ul>
<li>if you dont want to go into cubic model function ($x^3$)  </li>
<li>we can use other feature  ($\sqrt{(x)}$)</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fsqrt-root-feature.png" alt=""></p>
<h1 id="Week_2:_Lesson_3">Week 2: Lesson 3</h1><h2 id="Computing_Parameters_Analytically">Computing Parameters Analytically</h2><h3 id="Normal_Equation"><center>Normal Equation</center></h3><ul>
<li>so far, we use gradient descent to find optimal $\theta$  </li>
<li>a <strong>better way</strong> to solve optimal parameter $\theta$  </li>
<li>solve for $\theta$ analytically  </li>
<li><strong>without</strong> iteratively computation (gradient descent)</li>
<li><strong>one step</strong> computation for optimal parameter $\theta<br>$  </li>
<li>Feature Scaling is <strong>not necessary</strong> in Normal Equation</li>
</ul>
<h4 id="Example-1">Example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fnormal-equ-example.png" alt=""></p>
<h4 id="Equation">Equation</h4><p>$$\theta = (X^TX)^{-1}X^Ty$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fx-inverse.png" alt=""></p>
<blockquote>
<p>$x_0^i = 1$</p>
</blockquote>
<p>Octave: <code>pinv(x&#39;*x)*x&#39;*y</code>  </p>
<h4 id="Gradient_Descent_vs_Dis-advantage">Gradient Descent vs Dis-advantage</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-desc-vs-normal-equ.png" alt=""></p>
<blockquote>
<p>if $n$ number of features is very large ($n \geq 10^6$), it will be better to use gradient descent to aviod the $O(n^3)$ time complexity of normal equation.</p>
</blockquote>
<h3 id="Normal_Equation_Noninvertibility"><center>Normal Equation Noninvertibility</center></h3><ul>
<li>advanced concpet  </li>
<li>not all the matrices is invertable (ex. singular matrices)  </li>
<li>what if $X^TX$ is non-invertible? (singular/degenerate) </li>
</ul>
<h4 id="pinv_vs_inv_in_Octave"><code>pinv</code> vs <code>inv</code> in Octave</h4><ul>
<li><code>pinv</code> sudo inverse  </li>
<li><code>inv</code> inverse  </li>
</ul>
<blockquote>
<p><code>pinv</code> will result in a correct value even matrics is not invertible  </p>
</blockquote>
<h4 id="Case_where_$X^TX$_is_non-invertible">Case where $X^TX$ is non-invertible</h4><ul>
<li>Redudant features (linearly dependent)  </li>
</ul>
<p>$x_1$ = size in $feet^2$<br>$x_2$ = size in $m^2$  </p>
<blockquote>
<p>if two features ($feet^2$, $m^2$) has linear relation, $X^TX$ cannot be inverted  </p>
</blockquote>
<ul>
<li>Too many features (e.g. $m \leq n $) </li>
</ul>
<p><strong>Example</strong>:<br>traning set ($m = 10$), number of features ($n = 100$)  </p>
<blockquote>
<p>Number of traning set too small(<strong>not enough</strong> data), it is very hard for us to predict the 100 optimal $\theta$ for each of the features.  </p>
</blockquote>
<p><strong>Solution</strong>:  </p>
<ul>
<li><strong>delete</strong> some <strong>features</strong>  </li>
<li>use <strong>regularization</strong>(cover later)  </li>
</ul>
<h1 id="Week_2:_Lesson_6">Week 2: Lesson 6</h1><h2 id="Octave/Matlab_Tutorial">Octave/Matlab Tutorial</h2><h3 id="Basic_Operations"><center>Basic Operations</center></h3><p><code>eye(3)</code>  % 3x3 identity matrix</p>
<pre><code>1     0     0
0     1     0
0     0     1
</code></pre><p><code>sum(v, 1)</code> -&gt; sum column<br><code>sum(v, 2)</code> -&gt; sum row<br><code>size(v, 1)</code> -&gt; number of rows<br><code>size(v, 2)</code> -&gt; number of cols  </p>
<h3 id="Moving_Data_Around"><center>Moving Data Around</center></h3><h4 id="A(row_index,_col_index)">A(row_index, col_index)</h4><p><strong><code>A(2,:)</code>  % get the 2nd row.</strong>   </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>   <span class="number">100</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>   <span class="number">102</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">2</span>,:)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br></pre></td></tr></table></figure>
<p><strong><code>A(:,2)</code>  % get the 2nd col</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A(:,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<p><strong><code>A = [A, [100; 101; 102]];</code> % append column vector</strong>  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = <span class="matrix">[A, [<span class="number">100</span>; <span class="number">101</span>; <span class="number">102</span>]</span>]</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>   <span class="number">100</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>   <span class="number">101</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>   <span class="number">102</span></span><br></pre></td></tr></table></figure>
<p><strong><code>A(:)</code> % Select all elements as a column vector.</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">A(:)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br><span class="line">   <span class="number">100</span></span><br><span class="line">   <span class="number">101</span></span><br><span class="line">   <span class="number">102</span></span><br></pre></td></tr></table></figure></p>
<p><strong>% Putting data together</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="matrix">[<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span></span><br><span class="line">B = <span class="matrix">[<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>]</span> </span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p><strong>concatenating A and B matrices side by side</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; C=<span class="matrix">[A B]</span>=<span class="matrix">[A,B]</span></span><br><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Concatenating A and B top and bottom</strong><br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; C=<span class="matrix">[A;B]</span></span><br><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">    <span class="number">11</span>    <span class="number">12</span></span><br><span class="line">    <span class="number">13</span>    <span class="number">14</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Computing_On_Data"><center>Computing On Data</center></h3><p><strong>element-wise operation</strong>   </p>
<ul>
<li>notations - <code>.</code></li>
</ul>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .* <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">2</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span>     <span class="number">8</span></span><br><span class="line">    <span class="number">10</span>    <span class="number">12</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .^ <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">9</span>    <span class="number">16</span></span><br><span class="line">    <span class="number">25</span>    <span class="number">36</span></span><br></pre></td></tr></table></figure>
<p><strong>element-wise log/exp/abs</strong>  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">log</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">         <span class="number">0</span></span><br><span class="line">    <span class="number">0.6931</span></span><br><span class="line">    <span class="number">1.0986</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">exp</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">    <span class="number">2.7183</span></span><br><span class="line">    <span class="number">7.3891</span></span><br><span class="line">   <span class="number">20.0855</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">abs</span>(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><strong>Important</strong><br>For square matrices calculation  $A .* A \neq A^2$</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="matrix">[<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>]</span></span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A .* A</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">9</span>    <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A^<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">7</span>    <span class="number">10</span></span><br><span class="line">    <span class="number">15</span>    <span class="number">22</span></span><br></pre></td></tr></table></figure>
<p><strong>Find the transpose</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A<span class="operator">'</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">3</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span>     <span class="number">4</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p><strong>Flip matrix up-side-down</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; flipup(C)</span><br><span class="line">Undefined <span class="function"><span class="keyword">function</span> <span class="title">or</span> <span class="title">variable</span> '<span class="title">flipup</span>'.</span></span><br><span class="line"> </span><br><span class="line">Did you mean:</span><br><span class="line">&gt;&gt; <span class="built_in">flipud</span>(C)</span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h3 id="Plotting_Data"><center>Plotting Data</center></h3><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% plotting</span></span><br><span class="line">t = <span class="matrix">[<span class="number">0</span>:<span class="number">0.01</span>:<span class="number">0.98</span>]</span>;</span><br><span class="line">y1 = <span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t); </span><br><span class="line">plot(t,y1);</span><br><span class="line">y2 = <span class="built_in">cos</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*t);</span><br><span class="line">hold on;  <span class="comment">% "hold off" to turn off</span></span><br><span class="line">plot(t,y2,<span class="string">'r'</span>);</span><br><span class="line">xlabel(<span class="string">'time'</span>);</span><br><span class="line">ylabel(<span class="string">'value'</span>);</span><br><span class="line">legend(<span class="string">'sin'</span>,<span class="string">'cos'</span>);</span><br><span class="line">title(<span class="string">'my plot'</span>);</span><br><span class="line">print -dpng <span class="string">'myPlot.png'</span></span><br><span class="line">close;           <span class="comment">% or,  "close all" to close all figs</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2FmyPlot.png" alt=""></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = <span class="built_in">magic</span>(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">    <span class="number">17</span>    <span class="number">24</span>     <span class="number">1</span>     <span class="number">8</span>    <span class="number">15</span></span><br><span class="line">    <span class="number">23</span>     <span class="number">5</span>     <span class="number">7</span>    <span class="number">14</span>    <span class="number">16</span></span><br><span class="line">     <span class="number">4</span>     <span class="number">6</span>    <span class="number">13</span>    <span class="number">20</span>    <span class="number">22</span></span><br><span class="line">    <span class="number">10</span>    <span class="number">12</span>    <span class="number">19</span>    <span class="number">21</span>     <span class="number">3</span></span><br><span class="line">    <span class="number">11</span>    <span class="number">18</span>    <span class="number">25</span>     <span class="number">2</span>     <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; imagesc(A)</span><br><span class="line">&gt;&gt; imagesc(A), colorbar, colormap gray;</span><br><span class="line">&gt;&gt; print -dpng <span class="string">'colormap.png'</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcolormap.png" alt=""></p>
<h3 id="Control_Statements:_for,_while,_if_statement"><center>Control Statements: for, while, if statement</center></h3><h4 id="for-loop">for-loop</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; v=<span class="built_in">zeros</span>(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     <span class="number">0</span></span><br><span class="line">     </span><br><span class="line">&gt;&gt; <span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="number">10</span>,</span><br><span class="line">v(<span class="built_in">i</span>) = <span class="number">2</span>^<span class="built_in">i</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">&gt;&gt; v</span><br><span class="line"></span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">           <span class="number">2</span></span><br><span class="line">           <span class="number">4</span></span><br><span class="line">           <span class="number">8</span></span><br><span class="line">          <span class="number">16</span></span><br><span class="line">          <span class="number">32</span></span><br><span class="line">          <span class="number">64</span></span><br><span class="line">         <span class="number">128</span></span><br><span class="line">         <span class="number">256</span></span><br><span class="line">         <span class="number">512</span></span><br><span class="line">        <span class="number">1024</span></span><br></pre></td></tr></table></figure>
<h4 id="while-loop">while-loop</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> true, </span><br><span class="line">  v(<span class="built_in">i</span>) = <span class="number">999</span>; </span><br><span class="line">  <span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">i</span> == <span class="number">6</span>,</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h4 id="script(function)">script(function)</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">% script file calcuate.m</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% function calculate has </span></span><br><span class="line"><span class="comment">% one input - x </span></span><br><span class="line"><span class="comment">% two ouputs - y1 &amp; y2</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y1, y2]</span> = <span class="title">calculate</span><span class="params">(x)</span></span></span><br><span class="line"></span><br><span class="line">y1 = x^<span class="number">2</span>;</span><br><span class="line">y2 = x^<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<p>Terminal</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="matrix">[a, b]</span> = calculate(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">    <span class="number">25</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">b =</span><br><span class="line"></span><br><span class="line">   <span class="number">125</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="Cost_function_J_example">Cost function J example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fcost-function-j.png" alt=""></p>
<h3 id="Vectorization"><center>Vectorization</center></h3><ul>
<li>take advantage of linear algrba library  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fvect-example.png" alt=""></p>
<h4 id="unvectorized_implementation">unvectorized implementation</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">prediction = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n+<span class="number">1</span>,</span><br><span class="line">	prediction = prediction + theta(<span class="built_in">j</span>) * x(<span class="built_in">j</span>)</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<h4 id="Vectorized_implementation">Vectorized implementation</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">prediction = theta<span class="operator">'</span> * x;</span><br></pre></td></tr></table></figure>
<h4 id="Gradient_descent">Gradient descent</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160627%2Fgradient-descent.png" alt=""></p>
<p><strong>Vectorized Implementation</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescent</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line"><span class="comment">%GRADIENTDESCENT Performs gradient descent to learn theta</span></span><br><span class="line"><span class="comment">%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by </span></span><br><span class="line"><span class="comment">%   taking num_iters gradient steps with learning rate alpha</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% Initialize some useful values</span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line">    tempTheta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(theta, <span class="number">1</span>) , <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: <span class="built_in">size</span>(tempTheta, <span class="number">1</span>),</span><br><span class="line">        DiffVector = X * theta - y; <span class="comment">% 97 * 1 vector // VECTORIZATION</span></span><br><span class="line">        sum = X(:,<span class="built_in">i</span>)<span class="operator">'</span> * DiffVector; <span class="comment">% 1 x 97 times 97 x 1 = 1 x 1 (sum) // VECTORIZATION</span></span><br><span class="line">        tempTheta(<span class="built_in">i</span>) = theta(<span class="built_in">i</span>) - alpha * (<span class="number">1</span>/m) * sum;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    theta = tempTheta;</span><br><span class="line"></span><br><span class="line">    <span class="comment">% Save the cost J in every iteration    </span></span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h1 id="Week_3:_Lesson_1">Week 3: Lesson 1</h1><h2 id="Classification_and_Representation"><center>Classification and Representation</center></h2><h3 id="Classification">Classification</h3><ul>
<li>introduce logistic regression  </li>
<li>classification problem  </li>
<li>start with 2 classes classification (binary classification)  </li>
</ul>
<h4 id="Classification_Example:">Classification Example:</h4><ul>
<li>email: spam/not spam  </li>
<li>online transaction: fraudulent(yes/no)  </li>
</ul>
<p>$$ y \in 0, 1 $$  </p>
<h4 id="Linear_Regression_to_do_Classification_problem">Linear Regression to do Classification problem</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-classification.png" alt="">  </p>
<ul>
<li>in this particular example, linear regression method is actually doing something reasonable  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/linear-regression-bad-example.png" alt=""></p>
<ul>
<li>as we add one more data point to the right, the hypothesis function become smaller slope, two data points previous classify as malignant(y = 1) now become non-malignant.    </li>
<li>in most of case, linear regression is not the best approach for classification problem  </li>
<li>in example above, we might <strong>manually</strong> change the hypothesis function to $h_\theta(x) = 0.3x$. However, if we added one more data point, we will need to <strong>manually</strong> assigned a <strong>different</strong> <strong>slope</strong> for the hypothesis function. We need to find <strong>something else</strong> smarter (evenly seperate using the $h_\theta(x) = 0.5 $ threshold for classification problem). With new approch, even more data points invovole, the alorithm can automatically update the $\theta$ values to make the hypothesis function sucessfully classify the traning sets)  </li>
</ul>
<h4 id="Logistic_Regression:">Logistic Regression:</h4><ul>
<li>an classification algorithm  </li>
</ul>
<p>$$0\leq h_\theta(x) \leq 1$$</p>
<h3 id="Hypothesis_Representation">Hypothesis Representation</h3><ul>
<li>output value between 0 and 1 $$0\leq h_\theta(x) \leq 1$$  </li>
</ul>
<h4 id="Logistic_Regression_Model">Logistic Regression Model</h4><ul>
<li>want $0\leq h_\theta(x) \leq 1$   </li>
</ul>
<h4 id="Sigmoid_function/Logistic_Function">Sigmoid function/Logistic Function</h4><p>$g(z)$ is called Sigmoid function/Logistic function<br>$$h_\theta(x) = g(\theta^Tx) , where$$<br>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/sigmoid-function.png" alt=""></p>
<p>$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$</p>
<h4 id="Interpretation_of_Hypothesis_Output">Interpretation of Hypothesis Output</h4><ul>
<li>$h_\theta(x) $ = estimated probability that $y=1$ on input x  </li>
</ul>
<p>$H_\theta(x) = P(y=1|x;\theta)$ reads the probability of $y=1$ given patient with feature $x$, parameterized by $\theta$. </p>
<h3 id="Decision_Boundary">Decision Boundary</h3><ul>
<li>virtualization </li>
<li>better sense of Logistic regression  </li>
</ul>
<h4 id="logistic_regression">logistic regression</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/logistic-regression.png" alt=""><br>
$g(z) \geq 0.5$ when $z \geq 0$, since $h_\theta(x) = g(\theta^Tx)$, therefore, $h_\theta(x) \geq 0.5$ where $\theta^Tx \geq 0 $
</p>
<p>Similarly, $h_\theta(x) &lt; 0.5$ where $\theta^Tx &lt; 0 $</p>
<h4 id="Decision_Boundary_Vitualizaiton">Decision Boundary Vitualizaiton</h4><ul>
<li>once $\theta$ value fixed, the line for decision boundary was also fixed  </li>
<li>data points from one side of the decision boundary will result in a different <code>y values</code> from data points from the other side of the decision boundary  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/decision-boundary.png" alt=""></p>
<h4 id="Non-linear_decision_boundaries">Non-linear decision boundaries</h4><ul>
<li>decision boundary is <strong>non-linear</strong>  </li>
<li>we can add <strong>higher order polynomial terms</strong> into the logistic regression  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160628/non-linear-logistic-regression.png" alt=""></p>
<ul>
<li>decision boundary is determined by the <strong>parameter of the hypothesis</strong> function, not the <strong>traning set</strong>. </li>
<li>Traning set is used to determine a better set of $\theta$ values, once $\theta$ values are set, decision boundary is fixed.  </li>
</ul>
<h1 id="Week_3:_Lesson_2">Week 3: Lesson 2</h1><h2 id="Logistic_Regression_Model-1">Logistic Regression Model</h2><h3 id="Cost_Function-1">Cost Function</h3><ul>
<li>how do we choose $\theta$  </li>
</ul>
<p>For <code>Linear Regression</code>: </p>

$$J(\theta) = \frac 1 {m} \sum_{i=1}^m \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$$

<p>However, for cost function for <code>Logistic Regression</code>, we might not be able to utilize the same function since the cost function (with $h_{\theta}(x)$ being a sigmoid function) will result in a non-covex fucntion.  </p>
<blockquote>
<p>Non-convex function: function that is not able to converge to a local minium.  </p>
</blockquote>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/convex.png" alt=""><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/non-convex.png" alt="">  </p>
<p>Therefore, we need to come up with <strong>new</strong> cost function, that does come up into a <strong>convex</strong> function. (so that the fucntion can converge to a local/global minimum).  </p>
<h4 id="Logistic_regression_cost_function">Logistic regression cost function</h4>
$$Cost(h_\theta(x),y) = 
\left\{\begin{matrix}
 -log(h_\theta(x)) \; if\;y=1 \\ 
 -log(1-h_\theta(x))\; if \;y = 0 
\end{matrix}\right.$$

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png" alt=""></p>
<h3 id="Simplified_Cost_Function_and_Gradient_Descent">Simplified Cost Function and Gradient Descent</h3><ul>
<li>simpler way to write out cost fucntion  </li>
<li>apply gradient descent to get the best $\theta$ values.  </li>
</ul>
<h4 id="Logistic_regression_cost_function-1">Logistic regression cost function</h4>
$$J(\theta) = \frac 1 {m} \sum_{i=1}^m Cost(h_\theta(x^{(i)}), y^{(i)})$$


$$Cost(h_\theta(x),y) = 
\left\{\begin{matrix}
 -log(h_\theta(x)) \; if\;y=1 \\ 
 -log(1-h_\theta(x))\; if \;y = 0 
\end{matrix}\right.$$

Compressed version for cost function  


$$Cost(h_\theta(x),y) = 
 -y\;log(h_\theta(x)) 
 -(1-y	)log(1-h_\theta(x))$$


<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/logistic-regression-cost-function.png" alt=""><br>Above is the compact version of our cost fucntion for logistic regression.  </p>
<blockquote>
<p>Why would we choose this partitular <code>log</code> fucntion as cost fucntion is because this function was proven that <strong>provide efficiency</strong> to find $\theta$ values and the most <strong>convex feature</strong> for logistic regression.  </p>
</blockquote>
<h4 id="Find_the_theta_values_that_minimize_the_cost_function">Find the theta values that minimize the cost function</h4><p>$$\min_{\theta} J(\theta)$$</p>
<p><strong>Hypothesis Function</strong>  </p>
<p>$$h_\theta(x) = \frac{1}{1+e^{-\theta^{T}x}}$$</p>
<p>the output of the hypothesis function $h_\theta$ indicates<br>$$P(y=1\; | \; x;\theta)$$<br>the <strong>probability</strong> of result being <code>y=1</code> with given <code>x</code> values (from user) and $\theta$ values (pre-set by gradient descent).  </p>
<h4 id="Gradient_Descent-1">Gradient Descent</h4><p><strong>Update Rules</strong>:<br>
$$ \theta_j :=\theta_j - \alpha \frac {\partial}{\partial\theta_j} J(\theta)$$    
 </p>

$$ \theta_j :=\theta_j - \alpha \sum_{i=1}^m(h_0(x^{(i)}) - y^{(i)})*x_j^{(i)} $$    

<p>Gradient Descent for logistic regression looks exactly <strong>identical</strong> to linear reagression. However, $h_\theta$ function is different.  </p>
<p>$$h_\theta(x) = \frac{1}{1+e^{-\theta^{T}x }}$$</p>
<p>Performing the gradient descent for each of the features $\theta$ and then performing the simultaaneously update all $\theta$ values.  </p>
<blockquote>
<p>$\theta_j$ could be $\theta_0$, $\theta_1$, $\theta_2$… etc  </p>
</blockquote>
<p>Instead of using a <strong>for-loop</strong> to update all the $\theta$ values for all features, using <strong>vectorization multiplication</strong> will be more efficient.  </p>
<h3 id="Advanced_Optimization">Advanced Optimization</h3><ul>
<li>advacned optimzation concept  </li>
</ul>
<h4 id="List_of_optimzation_algorithms:_(to_minimize_the_cost_fucntion)">List of optimzation algorithms: (to minimize the cost fucntion)</h4><ul>
<li>gradient descent  </li>
<li>conjugate gradient  </li>
<li>BFGS  </li>
<li>L-BFGS  </li>
</ul>
<p><strong>Advantages</strong> of the other three alogrithms:  </p>
<ul>
<li><strong>no</strong> need to <strong>maually</strong> pick $\alpha$  </li>
<li>often <strong>faster</strong> than gradient descent    </li>
</ul>
<p><strong>Disadvantages</strong>:  </p>
<ul>
<li>more complex  </li>
</ul>
<p><strong>Example</strong>:   </p>
<p>Cost function</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function0.png" alt=""><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function.png" alt="">  </p>
<p>Instead of manually choose learning rate $\alpha$, we will used advanced optimzation library to automatically use the best learning rate $\alpha$ to find optimal $\theta$ values.  </p>
<h4 id="fminunc_function">fminunc function</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/fminunc.png" alt="">  </p>
<h4 id="Application_of_advanced_optimzation_library_to_logistic_regression">Application of advanced optimzation library to logistic regression</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/logistic-regression-with-advanced-optimzation-algorithm.png" alt=""></p>
<h4 id="Summary:">Summary:</h4><p><strong>benefit</strong> of using advanced optimzation algorith library</p>
<ul>
<li>harder to debug  </li>
<li>but faster  </li>
<li><strong>large</strong> machine learning <strong>problem</strong> will benefit  </li>
</ul>
<h1 id="Week_3:_Lesson_3">Week 3: Lesson 3</h1><h2 id="Multiclass_Classification">Multiclass Classification</h2><h3 id="Multiclass_Classification:_One-vs-all">Multiclass Classification: One-vs-all</h3><ul>
<li>how to get logistic regression to work for multiclass classification problems.  </li>
<li>one-versus-all classification  </li>
</ul>
<h4 id="Multiclass_classification_examples">Multiclass classification examples</h4><ul>
<li>email classification: work, friends, family, hobby  </li>
<li>weather: sunny, cloudy, rain, snow  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/multi-class-classification.png" alt=""></p>
<h4 id="One-vs-all_Example_(One-vs-rest)">One-vs-all Example (One-vs-rest)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/one-vs-all-classifcation.png" alt=""></p>
<ul>
<li>in example above, we come up with <strong>three classifiers</strong>  </li>
<li>each of which was trained to recognize one of three classes  </li>
</ul>
<h4 id="One-vs-all_Definition">One-vs-all Definition</h4><p>Train a logistic regression classifier $h_\theta^{i}(x)$ for each class $i$ to predict the <strong>probability</strong> that $y=i$.  </p>
<p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes $$\max<em>{i} h</em>\theta^{(i)}(x)$$  </p>
<blockquote>
<p>maxmizes since $h_\theta^{(i)}(x)$ is the probability of class is $i$ given $x$ value.  </p>
</blockquote>
<h2 id="Solving_the_Problem_of_Overfitting">Solving the Problem of Overfitting</h2><h3 id="The_Problem_of_Overfitting">The Problem of Overfitting</h3><ul>
<li>definition of overfitting  </li>
<li>regularization: technique to reduce overfitting problem  </li>
</ul>
<h4 id="What_is_overfitting?">What is overfitting?</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/over-fitting.png" alt=""></p>
<blockquote>
<p><strong>Overfitting:</strong><br>if we have too many features, the learned hypothesis may <strong>fit</strong> the <strong>trainng set very well</strong>. However, it fails to <strong>generalize</strong> well to make accurate predictions on <strong>new, previously unseen examples</strong>. </p>
</blockquote>
<p>We dont have enough data to constrain it to give us a good hypothesis </p>
<blockquote>
<p><strong>High variance:</strong><br>high order polynomial, hypothesis can fit almost any of the function.  </p>
</blockquote>
<h4 id="Addressing_overfitting">Addressing overfitting</h4><ul>
<li>ploting is a good start  </li>
<li>but once the number of features are getting larger  </li>
<li>it is getting harder to plot and visualize the features.  </li>
</ul>
<blockquote>
<p>If we have <strong>large number of features</strong> and the <strong>training data set</strong> is <strong>limited</strong>, then <strong>overfitting</strong> will become a problem. How can we avoid overfitting?  </p>
</blockquote>
<h4 id="Options_to_avoid_overfitting">Options to avoid overfitting</h4><p>1.<strong>Reduce</strong> number of features.  </p>
<ul>
<li>manually select which features to keep.  </li>
<li>model selection algorithm (automatically select features)  </li>
</ul>
<p>2.<strong>Regularization</strong>  </p>
<ul>
<li>Keep all features, but <strong>reduce</strong> <strong>magnitude</strong>/<strong>values</strong> of parameters $\theta_j$.  </li>
<li>works well when we have lots of features, each of which contributes a bit to predicting y. </li>
</ul>
<h3 id="Cost_Function-2">Cost Function</h3><ul>
<li>main intution of how regularization work  </li>
<li>cost function for regularization  </li>
</ul>
<h4 id="Regularization">Regularization</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/regularization.png" alt=""></p>
<p>(penalize some parameters/features with high cost -&gt; features($\theta$ values) will result in smaller values)</p>
<p>Small values for parameters $\theta_0, \theta_1,…,\theta_n$  </p>
<ul>
<li>“Simpler” hypothesis (polynomial -&gt; quadratic function)  </li>
<li>less prone to overfiting  </li>
</ul>
<h4 id="which_one/feature_to_pick_to_reduce_values?">which one/feature to pick to reduce values?</h4><p>We basically regularize all features.  </p>
$$J(\theta) =  \frac 1 {2m}[\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j=1}^{n}\theta_j^2] $$
 $$ \lambda \sum_{j=1}^{n}\theta_j^2 $$
<p>is called the regularzation term.  $ \lambda$ is the regularization parameter.  </p>
<blockquote>
<p>Regularization parameter $\lambda$:<br>it control to both<br>1) fitting the training data set well,<br>and 2) keeping the paramenters small (smaller parameters ~ 0 -&gt; <strong>get rid</strong> of the feature paramenters -&gt; keep hypothesis simple to prevent overfitting)</p>
</blockquote>
<h4 id="What_happen_with_large_$\lambda$_value?">What happen with large $\lambda$ value?</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/large-lambda.png" alt=""></p>
<h4 id="NextTime">NextTime</h4><ul>
<li>for regularzation to work well, need to choose good $\lambda$ value  </li>
<li>multi-selection -&gt; automatically choose regulazation parameter $\lambda$.  </li>
</ul>
<h3 id="Regularized_Linear_Regression">Regularized Linear Regression</h3><h4 id="Gradient_Descent-2">Gradient Descent</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/gradient-descent-with-regularization.png" alt=""></p>
<h4 id="Normal_Equation-1">Normal Equation</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/normal-equation-regularization.png" alt="">  </p>
<h4 id="Non-invertibility_(optional/advanced)">Non-invertibility (optional/advanced)</h4><ul>
<li>using regularization takes care of any non-invertable issue. (if $\lambda$ &gt; 0).  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/non-invertable-regularization.png" alt=""></p>
<h4 id="Summary">Summary</h4><ul>
<li>avoid overfitting with <strong>regularized</strong> linear regression  </li>
<li>next video will talk about <strong>regularized</strong> logistic regression  </li>
</ul>
<h3 id="Regularized_Logistic_Regression">Regularized Logistic Regression</h3><ul>
<li>previously - gradient descent &amp; advanced technique algorithm(automatically choosing learning rate)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/regularized-logistic-regression.png" alt=""></p>
<ul>
<li><strong>blue boundary</strong> is the logistic regression <strong>without</strong> regularization  </li>
<li><strong>pink boundary</strong> is the logistic regression <strong>with</strong> regularization  </li>
</ul>
<p>As we added the regularization term  $\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_j^2 $. We wanted to make our hypothesis function simpler to <strong>prevent</strong> overfitting. Therefore, we need to cancel out some high order-polynomial terms so that the function will become simpler. To make that happened, we tried to <strong>minmize</strong> the $\theta$ values by adding this regularzation terms within the <strong>cost function</strong>. And after we doing the <strong>derivative</strong> <strong>of the cost function</strong> will likey not choosing the larger $\theta$ values, it will likely choose the <strong>smallest possble</strong> $\theta$ values.  </p>
<h4 id="Gradient_Descent-3">Gradient Descent</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/logistic-regression-gradient-descent.png" alt=""></p>
<ul>
<li>same update rule for logistic regression compare to linear regression  </li>
<li>however, <strong>different</strong> <strong>hypothesis</strong> <strong>function</strong> was applied.  </li>
</ul>
<h4 id="Advanced_optimization_with_regularization">Advanced optimization with regularization</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160703/advanced-optimzation-regularization.png" alt="">  </p>
<p><strong>gradient Descent vs Advanced optimization:</strong><br>it allow us to use gradient descent algorithm BUT <strong>no</strong> need to <strong>maually</strong> pick $\alpha$ values. The computer automatically pick the best $\alpha$ value for us.  </p>
<p><strong>gradient Descent vs normal equation:</strong><br>it gives us different algorithm to compute $\theta$ values.  </p>
<h1 id="Week_4:_Lesson_1">Week 4: Lesson 1</h1><h2 id="Motivations"><center>Motivations</center></h2><h3 id="Non-linear_Hypotheses">Non-linear Hypotheses</h3><ul>
<li>new learning algorithm: <strong>neural network</strong>  </li>
</ul>
<h4 id="Why_neural_network:_new_learning_algorithm?">Why <strong>neural network</strong>: new learning algorithm?</h4><ul>
<li>non-linear classification(logistic regress ex) works well with only <strong>two features</strong>  </li>
<li>more interesting problems will invovle more than two features  </li>
<li>Too many features (quadratic order -&gt; $O(N^2)$, cubic order -&gt; $O()$) will result in <strong>overfitting</strong>  </li>
<li>Use <strong>subset</strong> of features will not provide best model to fet all the training set  </li>
</ul>
<h4 id="Car_identify_example">Car identify example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/car.png" alt=""></p>
<ul>
<li>$50 * 50$ picture = $2500$ pixels (in grayscale).  </li>
<li>RGB -&gt; $2500 * 3 = 7500$ pixels  </li>
<li>the $x$ vector represent pixel intensity(0-255)  </li>
<li>if we use non-linear hypothesis with quadratic features $(x_i \times x_j)$, 3 millions features (computational expensive!)  </li>
<li>simple <strong>logicstic regression</strong> with quadratic/cubic features <strong>not</strong> a good way to learn complex non-linear hypothesis when number of n(feature) is large.   </li>
</ul>
<h3 id="Neurons_and_the_Brain">Neurons and the Brain</h3><ul>
<li>neural networks are old algorithm  </li>
<li>background in neural networks  </li>
<li>how to apply them into problem  </li>
</ul>
<h4 id="Neural_Networks">Neural Networks</h4><ul>
<li>Origins: Algorithms that try to mimic the brain.  </li>
<li>Was widely used in 80s, and diminished in late 90s.(lack of computational power)  </li>
<li>Recent resurgence: state-of-art technique for many applicaitons  </li>
</ul>
<blockquote>
<p><strong>Auditory Cortex</strong>: part of your brain that allows you to lean to hear.  </p>
<p><strong>somatosensory Cortex</strong>: part of your brain that allows you to process sense of touch. </p>
</blockquote>
<h1 id="Week_4:_Lesson_2">Week 4: Lesson 2</h1><h2 id="Neural_Networks-1"><center>Neural Networks</center></h2><h3 id="Model_Representation_I">Model Representation I</h3><ul>
<li>how to represent hypothesis and model with neural network  </li>
</ul>
<h4 id="Simulation_of_neuron_in_the_brain">Simulation of neuron in the brain</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neuron.png" alt=""></p>
<ul>
<li><strong>dendrite</strong> - “input wires”  </li>
<li><strong>axon</strong> - “output wires” send signal to other neurons  </li>
<li><strong>neuron cell body</strong> - processing inputs signal and output signal to other neurons  </li>
</ul>
<h4 id="Single_neuron_model:_logistic_unit">Single neuron model: logistic unit</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/simple-neuron-model.png" alt=""></p>
<h4 id="Neural_Network">Neural Network</h4><ul>
<li>group of single neuron models  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png" alt="">  </p>
<ul>
<li>Input layer: layer 1  </li>
<li>Output layer: layer 3  </li>
<li>Hidden layer: anything that is NOT an input/output layer  </li>
</ul>
<h4 id="computation_steps_of_neural_netwrok">computation steps of neural netwrok</h4><p><strong>Notations</strong>:<br>$a_i^{(j)} = $ “activation” of unit $i$ in layer $j$.<br>$\theta^{(j)} = $ matrix of weights controlling function mapping from layer $j$ to layer $j+1$.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-example.png" alt=""></p>
<p><strong>Mathmatical representation:</strong></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/math-layer-represnetiation.png" alt=""></p>
<p>If network has $s<em>j$ units in layer $j$, $s</em>{j+1}$ units in layer $j+1$, then $\theta^{(j)}$ will be of dimension $$s_{j+1} \times (s_j +1)$$. </p>
<p><strong>Example</strong>:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise.png" alt=""></p>
<h3 id="Model_Representation_II">Model Representation II</h3><ul>
<li>how to do the computation in vectorize  </li>
<li>how neural network can that help us to learn complex non-linear hyphothesis  </li>
</ul>
<h4 id="Forward_propagation:_Vectorized_implementation">Forward propagation: Vectorized implementation</h4><ul>
<li>efficient way to compute $h(x)$  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png" alt=""></p>
<h4 id="Neural_Network_learning_its_own_features???">Neural Network learning its own features???</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/learn-its-own-feature.png" alt=""></p>
<h4 id="Other_network_architecures">Other network architecures</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/other-structure.png" alt=""></p>
<h4 id="exercise">exercise</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p1.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p2.png" alt=""></p>
<h3 id="Examples_and_Intuitions_I">Examples and Intuitions I</h3><ul>
<li>example shows how neural network to compute non-linear function  </li>
</ul>
<h4 id="Simple_example:_AND">Simple example: AND</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-and.png" alt=""></p>
<h4 id="Simple_example2:_OR">Simple example2: OR</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/or-function.png" alt=""></p>
<h3 id="Examples_and_Intuitions_II">Examples and Intuitions II</h3><h4 id="Negation">Negation</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/negation.png" alt=""></p>
<h4 id="XNOR">XNOR</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-xor.png" alt=""></p>
<p>$XNOR = (x_1 AND x_2) OR ((NOT x_1) AND (NOT x_2))$ </p>
<h4 id="More_application:Handwritten_digit_classification">More application:Handwritten digit classification</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/handwritten-digit-recognization.png" alt=""></p>
<h3 id="Multiclass_Classification-1">Multiclass Classification</h3><ul>
<li><strong>more than one categories</strong> we tried to distinguish  </li>
<li>ex. handwritten digits recognization problem  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/multiclass-classification.png" alt=""></p>
<h4 id="Exercise">Exercise</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise2.png" alt=""></p>
<p>$$s_{j+1} \times (s_j +1)$$. </p>
<p>Solution: sAccodring to equation above: $S_{j+1} = 10 $, and $S_j = 5$, therefore, $\theta^{(2)} = 10 \times 6 = 60 $.  </p>
<h1 id="Logs">Logs</h1><ul>
<li>06/19/2016: Complete week 1 notes and quizs  </li>
<li>06/21/2016: week 1 linear algrba review completed</li>
<li>06/27/2016: week 2 notes updated</li>
<li>06/27/2016: updated week 2: added notes for feature scaling &amp; mean normalization.  </li>
<li>06/28/2016: week 3 started: logistic regression, decision boundary  </li>
<li>07/01/2016: week 3 lesson 2: Logistic Regression Model &amp; advanced optimazation models  </li>
<li>07/03/2016: week 3 lesson 3: multi-classification, overfitting, regularization  </li>
<li>07/07/2016: week 4 lesson 1-2 completed.  </li>
<li></li>
</ul>
<h1 id="Reference">Reference</h1><p><a href="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" target="_blank" rel="external">Front cover from Coursera</a>  </p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag">#Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/06/09/learning-how-to-learn/" rel="next" title="Coursera: Learning How to Learn by UCSD">
                <i class="fa fa-chevron-left"></i> Coursera: Learning How to Learn by UCSD
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/06/28/Search-for-a-range/" rel="prev" title="Lintcode 61 - Search For a Range">
                Lintcode 61 - Search For a Range <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/06/18/ML/"
     data-title="Coursera: Machine Learning by Stanford"
     data-content=""
     data-url="http://www.xuyiruan.com/2016/06/18/ML/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/06/18/ML/"
           data-title="Coursera: Machine Learning by Stanford" data-url="http://www.xuyiruan.com/2016/06/18/ML/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/author.jpg"
               alt="🍣之神" />
          <p class="site-author-name" itemprop="name">🍣之神</p>
          <p class="site-description motion-element" itemprop="description">阮先生’s blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">20</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">22</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ruanxuyi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/xuyiruan/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/xuyi-ruan-a728a889" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://mszhuchinese.com" title="MsZhuChinese" target="_blank">MsZhuChinese</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.winniebabe.com" title="WinnieBabe" target="_blank">WinnieBabe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://suncuss.me" title="BossSun" target="_blank">BossSun</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_3"><span class="nav-number">1.</span> <span class="nav-text">Week 1: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervise_Learning"><span class="nav-number">1.1.</span> <span class="nav-text">Supervise Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression:"><span class="nav-number">1.1.1.</span> <span class="nav-text">Regression:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification:"><span class="nav-number">1.1.2.</span> <span class="nav-text">Classification:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM(Support_Vector_Machine):"><span class="nav-number">1.1.3.</span> <span class="nav-text">SVM(Support Vector Machine):</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised_Learning"><span class="nav-number">1.2.</span> <span class="nav-text">Unsupervised Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster_Learning:"><span class="nav-number">1.2.1.</span> <span class="nav-text">Cluster Learning:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cocktail_Party_Problem_Algorithm"><span class="nav-number">1.2.2.</span> <span class="nav-text">Cocktail Party Problem Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_5"><span class="nav-number">2.</span> <span class="nav-text">Week 1: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model_Representation"><span class="nav-number">2.1.</span> <span class="nav-text">Model Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Univariate_Linear_Regression"><span class="nav-number">2.1.1.</span> <span class="nav-text">Univariate Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Function"><span class="nav-number">2.2.</span> <span class="nav-text">Cost Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Fucntion_-_Intro_I"><span class="nav-number">2.3.</span> <span class="nav-text">Cost Fucntion - Intro I</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Find_$\theta_1$_by_minimizing_$J(\theta_1)$_(local_MIN_of_$J(\theta_1)$)"><span class="nav-number">2.3.1.</span> <span class="nav-text">Find $\theta_1$ by minimizing $J(\theta_1)$ (local MIN of $J(\theta_1)$)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Fucntion_-_Intro_II"><span class="nav-number">2.4.</span> <span class="nav-text">Cost Fucntion - Intro II</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost_Function_in_3D"><span class="nav-number">2.4.1.</span> <span class="nav-text">Cost Function in 3D</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Counter_Plot"><span class="nav-number">2.4.2.</span> <span class="nav-text">Counter Plot</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_6"><span class="nav-number">3.</span> <span class="nav-text">Week 1: Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent_Intuition"><span class="nav-number">3.2.</span> <span class="nav-text">Gradient Descent Intuition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient_Descent_For_Linear_Regression"><span class="nav-number">3.3.</span> <span class="nav-text">Gradient Descent For Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_Algorithm"><span class="nav-number">3.3.1.</span> <span class="nav-text">Gradient Descent Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#“Batch”_Gradient_Descent_Algorithm"><span class="nav-number">3.3.2.</span> <span class="nav-text">“Batch” Gradient Descent Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_1:_Lesson_7"><span class="nav-number">4.</span> <span class="nav-text">Week 1: Lesson 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrices_and_Vectors"><span class="nav-number">4.1.</span> <span class="nav-text">Matrices and Vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix:"><span class="nav-number">4.1.1.</span> <span class="nav-text">Matrix:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vector:"><span class="nav-number">4.1.2.</span> <span class="nav-text">Vector:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Addition_and_Scalar_Multiplicaiton"><span class="nav-number">4.2.</span> <span class="nav-text">Addition and Scalar Multiplicaiton</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Addition"><span class="nav-number">4.2.1.</span> <span class="nav-text">Addition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scalar_Multication"><span class="nav-number">4.2.2.</span> <span class="nav-text">Scalar Multication</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Vector_Multiplication"><span class="nav-number">4.3.</span> <span class="nav-text">Matrix Vector Multiplication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition:"><span class="nav-number">4.3.1.</span> <span class="nav-text">Definition:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example:"><span class="nav-number">4.3.2.</span> <span class="nav-text">Example:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advantage_of_using_Matrix_Multiplication"><span class="nav-number">4.3.3.</span> <span class="nav-text">Advantage of using Matrix Multiplication</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convert_predicting_housing_price_into_Matrix_Multiplication"><span class="nav-number">4.3.4.</span> <span class="nav-text">Convert predicting housing price into Matrix Multiplication</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Matrix_Multiplication"><span class="nav-number">4.4.</span> <span class="nav-text">Matrix Matrix Multiplication</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Example"><span class="nav-number">4.4.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition"><span class="nav-number">4.4.2.</span> <span class="nav-text">Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#One_more_example"><span class="nav-number">4.4.3.</span> <span class="nav-text">One more example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Application_of_M-M-M"><span class="nav-number">4.4.4.</span> <span class="nav-text">Application of M-M-M</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Matrix_Multiplication_Properties"><span class="nav-number">4.5.</span> <span class="nav-text">Matrix Multiplication Properties</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Commutative"><span class="nav-number">4.5.1.</span> <span class="nav-text">Commutative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Associative"><span class="nav-number">4.5.2.</span> <span class="nav-text">Associative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Identity_Matrix"><span class="nav-number">4.5.3.</span> <span class="nav-text">Identity Matrix</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inverse_and_Transpose"><span class="nav-number">4.6.</span> <span class="nav-text">Inverse and Transpose</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix_inverse:"><span class="nav-number">4.6.1.</span> <span class="nav-text">Matrix inverse:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example_1"><span class="nav-number">4.6.2.</span> <span class="nav-text">Example 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix_Transpose"><span class="nav-number">4.6.3.</span> <span class="nav-text">Matrix Transpose</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition-1"><span class="nav-number">4.6.4.</span> <span class="nav-text">Definition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson2"><span class="nav-number">5.</span> <span class="nav-text">Week 2: Lesson2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multivariate_Linear_Regression"><span class="nav-number">5.1.</span> <span class="nav-text">Multivariate Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mltivariate_Linear_Regression"><span class="nav-number">5.1.1.</span> <span class="nav-text"> Mltivariate Linear Regression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple_features"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">Multiple features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#New_Hypothesis_function"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">New Hypothesis function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Formal_Definition"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">Formal Definition</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_for_Multiple_Variables"><span class="nav-number">5.1.2.</span> <span class="nav-text"> Gradient Descent for Multiple Variables </span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_in_Practice_I_-_Feature_Scaling"><span class="nav-number">5.1.3.</span> <span class="nav-text"> Gradient Descent in Practice I - Feature Scaling </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature_Scaling"><span class="nav-number">5.1.3.1.</span> <span class="nav-text">Feature Scaling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mean_Normalization"><span class="nav-number">5.1.3.2.</span> <span class="nav-text">Mean Normalization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Descent_in_Practice_II_-_Learning_Rate"><span class="nav-number">5.1.4.</span> <span class="nav-text"> Gradient Descent in Practice II - Learning Rate </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Making_sure_gradient_descent_work_correctly"><span class="nav-number">5.1.4.1.</span> <span class="nav-text">Making sure gradient descent work correctly</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Possible_bugs"><span class="nav-number">5.1.4.2.</span> <span class="nav-text">Possible bugs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#To_choose_α,_try(three-fold)"><span class="nav-number">5.1.4.3.</span> <span class="nav-text">To choose α, try(three-fold)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Features_and_Polynomial_Regression"><span class="nav-number">5.1.5.</span> <span class="nav-text"> Features and Polynomial Regression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Definiing_new_feature"><span class="nav-number">5.1.5.1.</span> <span class="nav-text">Definiing new feature</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Polynomical_Regression"><span class="nav-number">5.1.5.2.</span> <span class="nav-text">Polynomical Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choose_a_feature"><span class="nav-number">5.1.5.3.</span> <span class="nav-text">Choose a feature</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson_3"><span class="nav-number">6.</span> <span class="nav-text">Week 2: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Computing_Parameters_Analytically"><span class="nav-number">6.1.</span> <span class="nav-text">Computing Parameters Analytically</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal_Equation"><span class="nav-number">6.1.1.</span> <span class="nav-text">Normal Equation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-1"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Equation"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Descent_vs_Dis-advantage"><span class="nav-number">6.1.1.3.</span> <span class="nav-text">Gradient Descent vs Dis-advantage</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal_Equation_Noninvertibility"><span class="nav-number">6.1.2.</span> <span class="nav-text">Normal Equation Noninvertibility</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pinv_vs_inv_in_Octave"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">pinv vs inv in Octave</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Case_where_$X^TX$_is_non-invertible"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">Case where $X^TX$ is non-invertible</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_2:_Lesson_6"><span class="nav-number">7.</span> <span class="nav-text">Week 2: Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Octave/Matlab_Tutorial"><span class="nav-number">7.1.</span> <span class="nav-text">Octave/Matlab Tutorial</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic_Operations"><span class="nav-number">7.1.1.</span> <span class="nav-text">Basic Operations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Moving_Data_Around"><span class="nav-number">7.1.2.</span> <span class="nav-text">Moving Data Around</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A(row_index,_col_index)"><span class="nav-number">7.1.2.1.</span> <span class="nav-text">A(row_index, col_index)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Computing_On_Data"><span class="nav-number">7.1.3.</span> <span class="nav-text">Computing On Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Plotting_Data"><span class="nav-number">7.1.4.</span> <span class="nav-text">Plotting Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Control_Statements:_for,_while,_if_statement"><span class="nav-number">7.1.5.</span> <span class="nav-text">Control Statements: for, while, if statement</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#for-loop"><span class="nav-number">7.1.5.1.</span> <span class="nav-text">for-loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#while-loop"><span class="nav-number">7.1.5.2.</span> <span class="nav-text">while-loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#script(function)"><span class="nav-number">7.1.5.3.</span> <span class="nav-text">script(function)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost_function_J_example"><span class="nav-number">7.1.5.4.</span> <span class="nav-text">Cost function J example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorization"><span class="nav-number">7.1.6.</span> <span class="nav-text">Vectorization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#unvectorized_implementation"><span class="nav-number">7.1.6.1.</span> <span class="nav-text">unvectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Vectorized_implementation"><span class="nav-number">7.1.6.2.</span> <span class="nav-text">Vectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_descent"><span class="nav-number">7.1.6.3.</span> <span class="nav-text">Gradient descent</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_1"><span class="nav-number">8.</span> <span class="nav-text">Week 3: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification_and_Representation"><span class="nav-number">8.1.</span> <span class="nav-text">Classification and Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification"><span class="nav-number">8.1.1.</span> <span class="nav-text">Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Classification_Example:"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">Classification Example:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear_Regression_to_do_Classification_problem"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Linear Regression to do Classification problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_Regression:"><span class="nav-number">8.1.1.3.</span> <span class="nav-text">Logistic Regression:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hypothesis_Representation"><span class="nav-number">8.1.2.</span> <span class="nav-text">Hypothesis Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_Regression_Model"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">Logistic Regression Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid_function/Logistic_Function"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">Sigmoid function/Logistic Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Interpretation_of_Hypothesis_Output"><span class="nav-number">8.1.2.3.</span> <span class="nav-text">Interpretation of Hypothesis Output</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision_Boundary"><span class="nav-number">8.1.3.</span> <span class="nav-text">Decision Boundary</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#logistic_regression"><span class="nav-number">8.1.3.1.</span> <span class="nav-text">logistic regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision_Boundary_Vitualizaiton"><span class="nav-number">8.1.3.2.</span> <span class="nav-text">Decision Boundary Vitualizaiton</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-linear_decision_boundaries"><span class="nav-number">8.1.3.3.</span> <span class="nav-text">Non-linear decision boundaries</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_2"><span class="nav-number">9.</span> <span class="nav-text">Week 3: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic_Regression_Model-1"><span class="nav-number">9.1.</span> <span class="nav-text">Logistic Regression Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost_Function-1"><span class="nav-number">9.1.1.</span> <span class="nav-text">Cost Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_regression_cost_function"><span class="nav-number">9.1.1.1.</span> <span class="nav-text">Logistic regression cost function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Simplified_Cost_Function_and_Gradient_Descent"><span class="nav-number">9.1.2.</span> <span class="nav-text">Simplified Cost Function and Gradient Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_regression_cost_function-1"><span class="nav-number">9.1.2.1.</span> <span class="nav-text">Logistic regression cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Find_the_theta_values_that_minimize_the_cost_function"><span class="nav-number">9.1.2.2.</span> <span class="nav-text">Find the theta values that minimize the cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Descent-1"><span class="nav-number">9.1.2.3.</span> <span class="nav-text">Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advanced_Optimization"><span class="nav-number">9.1.3.</span> <span class="nav-text">Advanced Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#List_of_optimzation_algorithms:_(to_minimize_the_cost_fucntion)"><span class="nav-number">9.1.3.1.</span> <span class="nav-text">List of optimzation algorithms: (to minimize the cost fucntion)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#fminunc_function"><span class="nav-number">9.1.3.2.</span> <span class="nav-text">fminunc function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application_of_advanced_optimzation_library_to_logistic_regression"><span class="nav-number">9.1.3.3.</span> <span class="nav-text">Application of advanced optimzation library to logistic regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summary:"><span class="nav-number">9.1.3.4.</span> <span class="nav-text">Summary:</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_3:_Lesson_3"><span class="nav-number">10.</span> <span class="nav-text">Week 3: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass_Classification"><span class="nav-number">10.1.</span> <span class="nav-text">Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiclass_Classification:_One-vs-all"><span class="nav-number">10.1.1.</span> <span class="nav-text">Multiclass Classification: One-vs-all</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiclass_classification_examples"><span class="nav-number">10.1.1.1.</span> <span class="nav-text">Multiclass classification examples</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-vs-all_Example_(One-vs-rest)"><span class="nav-number">10.1.1.2.</span> <span class="nav-text">One-vs-all Example (One-vs-rest)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-vs-all_Definition"><span class="nav-number">10.1.1.3.</span> <span class="nav-text">One-vs-all Definition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Solving_the_Problem_of_Overfitting"><span class="nav-number">10.2.</span> <span class="nav-text">Solving the Problem of Overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The_Problem_of_Overfitting"><span class="nav-number">10.2.1.</span> <span class="nav-text">The Problem of Overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What_is_overfitting?"><span class="nav-number">10.2.1.1.</span> <span class="nav-text">What is overfitting?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Addressing_overfitting"><span class="nav-number">10.2.1.2.</span> <span class="nav-text">Addressing overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Options_to_avoid_overfitting"><span class="nav-number">10.2.1.3.</span> <span class="nav-text">Options to avoid overfitting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost_Function-2"><span class="nav-number">10.2.2.</span> <span class="nav-text">Cost Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularization"><span class="nav-number">10.2.2.1.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#which_one/feature_to_pick_to_reduce_values?"><span class="nav-number">10.2.2.2.</span> <span class="nav-text">which one/feature to pick to reduce values?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#What_happen_with_large_$\lambda$_value?"><span class="nav-number">10.2.2.3.</span> <span class="nav-text">What happen with large $\lambda$ value?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NextTime"><span class="nav-number">10.2.2.4.</span> <span class="nav-text">NextTime</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularized_Linear_Regression"><span class="nav-number">10.2.3.</span> <span class="nav-text">Regularized Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Descent-2"><span class="nav-number">10.2.3.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normal_Equation-1"><span class="nav-number">10.2.3.2.</span> <span class="nav-text">Normal Equation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-invertibility_(optional/advanced)"><span class="nav-number">10.2.3.3.</span> <span class="nav-text">Non-invertibility (optional/advanced)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summary"><span class="nav-number">10.2.3.4.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularized_Logistic_Regression"><span class="nav-number">10.2.4.</span> <span class="nav-text">Regularized Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Descent-3"><span class="nav-number">10.2.4.1.</span> <span class="nav-text">Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Advanced_optimization_with_regularization"><span class="nav-number">10.2.4.2.</span> <span class="nav-text">Advanced optimization with regularization</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_4:_Lesson_1"><span class="nav-number">11.</span> <span class="nav-text">Week 4: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivations"><span class="nav-number">11.1.</span> <span class="nav-text">Motivations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linear_Hypotheses"><span class="nav-number">11.1.1.</span> <span class="nav-text">Non-linear Hypotheses</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why_neural_network:_new_learning_algorithm?"><span class="nav-number">11.1.1.1.</span> <span class="nav-text">Why neural network: new learning algorithm?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Car_identify_example"><span class="nav-number">11.1.1.2.</span> <span class="nav-text">Car identify example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neurons_and_the_Brain"><span class="nav-number">11.1.2.</span> <span class="nav-text">Neurons and the Brain</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Networks"><span class="nav-number">11.1.2.1.</span> <span class="nav-text">Neural Networks</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_4:_Lesson_2"><span class="nav-number">12.</span> <span class="nav-text">Week 4: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural_Networks-1"><span class="nav-number">12.1.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Representation_I"><span class="nav-number">12.1.1.</span> <span class="nav-text">Model Representation I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simulation_of_neuron_in_the_brain"><span class="nav-number">12.1.1.1.</span> <span class="nav-text">Simulation of neuron in the brain</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Single_neuron_model:_logistic_unit"><span class="nav-number">12.1.1.2.</span> <span class="nav-text">Single neuron model: logistic unit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network"><span class="nav-number">12.1.1.3.</span> <span class="nav-text">Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#computation_steps_of_neural_netwrok"><span class="nav-number">12.1.1.4.</span> <span class="nav-text">computation steps of neural netwrok</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Representation_II"><span class="nav-number">12.1.2.</span> <span class="nav-text">Model Representation II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward_propagation:_Vectorized_implementation"><span class="nav-number">12.1.2.1.</span> <span class="nav-text">Forward propagation: Vectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network_learning_its_own_features???"><span class="nav-number">12.1.2.2.</span> <span class="nav-text">Neural Network learning its own features???</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other_network_architecures"><span class="nav-number">12.1.2.3.</span> <span class="nav-text">Other network architecures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#exercise"><span class="nav-number">12.1.2.4.</span> <span class="nav-text">exercise</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples_and_Intuitions_I"><span class="nav-number">12.1.3.</span> <span class="nav-text">Examples and Intuitions I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple_example:_AND"><span class="nav-number">12.1.3.1.</span> <span class="nav-text">Simple example: AND</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple_example2:_OR"><span class="nav-number">12.1.3.2.</span> <span class="nav-text">Simple example2: OR</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples_and_Intuitions_II"><span class="nav-number">12.1.4.</span> <span class="nav-text">Examples and Intuitions II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Negation"><span class="nav-number">12.1.4.1.</span> <span class="nav-text">Negation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XNOR"><span class="nav-number">12.1.4.2.</span> <span class="nav-text">XNOR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#More_application:Handwritten_digit_classification"><span class="nav-number">12.1.4.3.</span> <span class="nav-text">More application:Handwritten digit classification</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiclass_Classification-1"><span class="nav-number">12.1.5.</span> <span class="nav-text">Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise"><span class="nav-number">12.1.5.1.</span> <span class="nav-text">Exercise</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logs"><span class="nav-number">13.</span> <span class="nav-text">Logs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">14.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">🍣之神</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"xruan"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("oKoPG1XTXkC7T1oasGQwau2g-gzGzoHsz", "JFJCQtIsUKhrX6S7Gvgqdqgk");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
