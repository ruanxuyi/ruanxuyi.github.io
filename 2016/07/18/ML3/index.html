<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Bree Serif:300,300italic,400,400italic,700,700italic|Arimo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/sushi.png?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="article">
<meta property="og:title" content="[Completed] Coursera: Machine Learning by Stanford - Part3">
<meta property="og:url" content="http://www.xuyiruan.com/2016/07/18/ML3/index.html">
<meta property="og:site_name" content="阮先生de小窝">
<meta property="og:description">
<meta property="og:image" content="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/hypothesis.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-costfunction.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/large-margin-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/overfitting-underfitting-svm.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-bad-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-good-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/similarity-function-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/sigma-value.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-mean-non-separate.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-dataset.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-global-cluster.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-elbow-method.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/elbow-method-bad.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/data-compression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/pca.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/pca-vs-linear-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-q.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-a.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/anormal-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/p-model.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-distribution.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/parameter-estimation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-estimation-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160808/anomal-detection-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160810/Anomaly%20Detection-aircraft-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160810/gaussian.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160810/non-gaussian-look.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160810/non-gaussain-to-gaussian.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160810/anomaly-error-analysis.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommand-system-movie.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/movie-rating.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/gradient-descent-update.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommander-update-simultanously.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/collaborate-filtering.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommander-system-unrate-response.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[Completed] Coursera: Machine Learning by Stanford - Part3">
<meta name="twitter:description">




<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> [Completed] Coursera: Machine Learning by Stanford - Part3 | 阮先生de小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">阮先生de小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">多读书 多思考 少吃零食 多睡觉</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'TVdsuUVez9DD1Mx-gjc8','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                [Completed] Coursera: Machine Learning by Stanford - Part3
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-07-18T10:15:48+08:00" content="07-18-2016">
              07-18-2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/18/ML3/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/18/ML3/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/07/18/ML3/" class="leancloud_visitors" data-flag-title="[Completed] Coursera: Machine Learning by Stanford - Part3">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" alt=""></p>
<a id="more"></a>
<h1 id="Week_7:_Lesson_1">Week 7: Lesson 1</h1><h2 id="Large_Margin_Classification">Large Margin Classification</h2><h3 id="Optimization_Objective"><center> Optimization Objective </center></h3><ul>
<li>introduction of SVM  </li>
<li>more powerful way to learn complex non-linear function  </li>
<li>supervised learning algorithm: linear regression, neural network, SVM.  </li>
</ul>
<h4 id="Alternative_logistic_regression_to_SVM">Alternative logistic regression to SVM</h4><p><strong>Recall for logistic regression:</strong>  </p>

Hypothesis function: $h_{\theta}(x) = \frac{1}{1+e^{-\theta T x}}$

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/hypothesis.png" alt=""></p>
<p><strong>Cost Function:</strong> </p>

$Cost(h_\theta(x),y) = 
 -y\;log(h_\theta(x)) 
 -(1-y	)log(1-h_\theta(x))$

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png" alt=""></p>
<p><strong>1. Change Cost function for SVM:</strong>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-costfunction.png" alt="">  </p>
<p>Pink line denotes new cost function for SVM. $Cost_1(z)$ is cost fucntion when $y=1$. $Cost_0(z)$ is cost fucntion when $y=0$. </p>
<p><strong>2. Change constant parameter for SVM:</strong>  </p>
<p>$1/m$ and $\frac{\lambda}{2m}$ terms goes away by multipling equation by $m$, which will not affect the optimzation process. Later we further multiply equation with $\frac{1}{\lambda}$ which result in following.</p>
<h4 id="SVM_cost_function">SVM cost function</h4>
$$J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$$

<p>As we increase $C$, we regularize <strong>less</strong> (less weight on regularzation term - reduce overfitting). If we decrease $C$, we regularize <strong>more</strong> (more weight on regularzation term - reduce underfitting).</p>
<h4 id="SVM_hypothesis_function">SVM hypothesis function</h4><p>The hypothesis of the SVN is not interpreted as the probability of $y$ being 1 or 0 (as it is for the hypothesis of logistic regression).  </p>

$$
h_\theta(x) = 
\begin{cases}
    1 & \text{if} \ \Theta^Tx \geq 0 \\
    0 & \text{otherwise}
\end{cases}
$$

<h3 id="Large_Margin_Intuition"><center>Large Margin Intuition</center></h3><ul>
<li>introduction for large margin classifier  </li>
<li>value $C$’s effect on position of the classifier  </li>
<li>$C = \frac{1}{\lambda}$ relationship   </li>
</ul>
<h4 id="Support_Vector_Machine">Support Vector Machine</h4><p>The cost function for SVM looks like following:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y0.png" alt=""><br>If $y=0$, we want $\theta^Tx \leq -1$ (not just $\leq 0$).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y1.png" alt=""><br>If $y=1$, we want $\theta^Tx \geq 1$ (not just $\geq 0$).  </p>
<p>SVM create a better <strong>precision</strong> by setting $\theta^Tx \leq -1$ and $\theta^Tx \geq 1$  </p>
<h4 id="Large_Margin_Classifier">Large Margin Classifier</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/large-margin-classifier.png" alt=""></p>
<p>Instead of learning the green/pink classifer, SVM learns a more robust classifer (with more margin to either of the <strong>blue</strong> line), which marked as the <strong>black</strong> line.  </p>
<h4 id="SVM_constant_value_C">SVM constant value C</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/overfitting-underfitting-svm.png" alt="">  </p>
<p>In logistic regression, large $\lambda$ result in underfitting, and small $\lambda$ result in overfitting. </p>
<p>Similarly, in SVM, large $C$ (small $\lambda$) value results in overfitting, and small $C$ (large $\lambda$) value results in underfitting.  </p>
<p>$$C = \frac{1}{\lambda}$$  </p>
<h3 id="Mathematics_Behind_Large_Margin_Classification"><center> Mathematics Behind Large Margin Classification </center></h3><ul>
<li>why/how svm large margin classifier ???</li>
</ul>
<h4 id="Vector_Inner_Product">Vector Inner Product</h4><p>why $\theta$ line is vertical to classifier line.  </p>
<p>SVM decision boundary has to satisfy with following two conditions:  </p>

$\begin{align*}
&min_\Theta \dfrac{1}{2}\sum_{j=1}^n \Theta_j^2
\end{align*}$

<p>for minimum of $Θ$ values</p>
<p>and  </p>
<p>So we now have a new optimization objective by substituting $p^{(i)} \cdot ||\Theta ||$ in for $\Theta^Tx^{(i)}$:<br>If y=1, we want $p^{(i)} \cdot ||\Theta || \geq 1$<br>If y=0, we want $p^{(i)} \cdot ||\Theta || \leq -1$  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-bad-classifier.png" alt=""></p>

We want $p^{(i)} \cdot ||\Theta || \geq 1$, if $p^{(i)}$ is small(distance of `x` or `o` to blue line), $||\Theta ||$ will be large (which is NOT good, we want to minimize $\theta$ values).  

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-good-classifier.png" alt=""></p>

We want $p^{(i)} \cdot ||\Theta || \geq 1$, if $p^{(i)}$ is large, $||\Theta ||$ could be small (which is good, we want to minimize $\theta$ values).  

<h1 id="Week_7:_Lesson_2">Week 7: Lesson 2</h1><h2 id="Kernels">Kernels</h2><h3 id="Kernels_I"><center> Kernels I</center></h3><ul>
<li>Introduction to new form for hypothesis function for SVM  </li>
<li>Introduction to Gaussian Kernels (similarity function)  </li>
<li>properity of Gaussian Kernels  </li>
<li>Effect of sigma value in Gaussian Kernels  </li>
</ul>
<h4 id="Hypothesis_function_for_SVM">Hypothesis function for SVM</h4><p>Instead of using the hypothesis function in form below (for <strong>high level polynomial</strong> logistic regression):  </p>

$$\begin{align*}
h_\Theta(x) = \Theta_1x_1 + \Theta_2x_2 + \Theta_3x_1x_2 + \Theta_4x_1^2 + \dots
\end{align*}$$

<p>We use the following hypothesis function form for SVM:  </p>

$$\begin{align*}
h_\Theta(x) = \Theta_1f_1 + \Theta_2f_2 + \Theta_3f_3 + \dots
\end{align*}$$

<p>Where:  </p>

$$\begin{align*}
f_1 = x_1 \newline
f_2 = x_2 \newline
f_3 = x_1x_2 \newline
f_4 = x_1^2 \newline
\dots \newline
\end{align*}$$


<h4 id="Gaussian_Kernels:_The_similarity_function">Gaussian Kernels: The similarity function</h4><p>The similarity of data point $x$ and some landmark $l$ define as:  </p>

$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2})$$

<h4 id="Properties_of_the_similarity_function">Properties of the similarity function</h4><p>If $x \approx l^{(i)}$, then $f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1$  </p>
<p>If $x$ is far from $l^{(i)}$, then $f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0$  </p>
<p>In other word, if data point $x$ is close to the landmark $i$, then the value of $f_i$ will close to $1$. Other wise, if data point $x$ is far from the landmark $i$, then the value of $f_i$ will close to $0$.</p>
<p><strong>Example:</strong>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/similarity-function-example.png" alt=""></p>
<h4 id="Effect_of_sigma_value">Effect of sigma value</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/sigma-value.png" alt=""></p>
<p>If $\sigma$ value increase, the shape will become wider. If $\sigma$ value decrease, the shape will become narrower.</p>
<h3 id="Kernels_II"><center> Kernels II</center></h3><ul>
<li>where to get landmark $l^{(1)}, l^{(2)}, l^{(3)},..?$</li>
<li>how SVM with kernels can learn complex non-linear function.  </li>
<li>how to get parameter $\Theta$  </li>
<li>SVM parameters $C$ and $\sigma^2$  </li>
</ul>
<h4 id="Choosing_the_landmarks">Choosing the landmarks</h4><p>We can choose landmarks $l_i$ from exact same location as the training data $x_i$.  </p>
<p>Given training data $x$: </p>

$$f_1 = similarity(x,l^{(1)}), f_2 = similarity(x,l^{(2)}), f_3 = similarity(x,l^3) \dots$$

<p>Each training data $x^{(i)}$ has $m$ features (total of $m$ training datas), which denotes as a “feature vector” $f^{(i)}$. We may also have $f_0=1$ to correspond with $\Theta_0$. </p>

$$x^{(i)} \rightarrow \begin{bmatrix}
f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \newline
f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \newline
\dots \newline
f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \newline
\end{bmatrix}$$


<h4 id="Choosing_the_parameters_$\Theta$">Choosing the parameters $\Theta$</h4><p>we can use the SVM minimization algorithm but with $f(i)$ substituted in for $x(i)$:</p>

$\large
min_\Theta C \sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tf^{(i)}) + (1 - y^{(i)})\text{cost}_0(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$

<blockquote>
<p><strong>Note</strong>: Kernals could also use with logistic regression but it works much faster with SVM because of computational optimizations on SVMs. </p>
</blockquote>
<h4 id="SVM_parameters">SVM parameters</h4><p><strong>Choosing $C$ ($C=\frac{1}{\lambda}$)</strong>:  </p>
<p>If $C$ is large: higher variance/lower bias/overfitting<br>If $C$ is small: lower variance/higher bias/underfitting  </p>
<p><strong>Choosing $\sigma^2$</strong></p>
<p>With a large $\sigma^2$: the features $f_i$ vary <strong>more</strong> smoothly, higher bias/lower variance/underfitting<br>With a small $\sigma^2$: the features $f_i$ vary <strong>less</strong> smoothly, lower bias/higher variance/overfitting  </p>
<h1 id="Week_7:_Lesson_3">Week 7: Lesson 3</h1><h2 id="SVMs_in_Practice">SVMs in Practice</h2><h3 id="Using_An_SVM"><center>Using An SVM</center></h3><ul>
<li>not recommand writing own implementation for solving $\Theta$ value, use the SVM software package (<code>liblinear</code>, <code>libsvm</code>) instead.  </li>
</ul>
<h4 id="Kernel_(similarity)_functions:">Kernel (similarity) functions:</h4><p>$$f = exp(-\frac{\Vert x1 - x2 \Vert ^2}{2\lambda})$$</p>
<h4 id="Implementation_choice_for_SVM">Implementation choice for SVM</h4><ul>
<li>Choice of parameter $C$.  </li>
<li>Choice of kernel $\sigma^2$(similarity function).  </li>
<li>Linear kernel (No Kernel) - (<code>n</code> large, <code>m</code> small)</li>
<li>Complex non-linear kernel (Gaussian Kernel) (<code>n</code> small, <code>m</code> large)  </li>
</ul>
<p>Why # of feature n = # of training example m??? </p>
<h4 id="Multi-class_classification">Multi-class classification</h4><ol>
<li><p>SVM package already build-in multi-class classification</p>
</li>
<li><p>use one-vs-all method</p>
</li>
</ol>
<h4 id="Logistic_regression_vs-_SVMs">Logistic regression vs. SVMs</h4><p><code>n</code> = number of features.<br><code>m</code> = number of training examples.  </p>
<p>quiz question???<br>TODO: notes for last lecture</p>
<h1 id="Week_8:_Lesson_1">Week 8: Lesson 1</h1><h2 id="Clustering">Clustering</h2><h3 id="Unsupervised_Learning:_Introduction"><center>Unsupervised Learning: Introduction</center></h3><ul>
<li>learned from un-labeled data  </li>
<li>in unsupervised learning, training set is of the form ${x^{(1)},x^{(2)},x^{(3)}, \dots x^{(m)}, }$ without labels $y^{(i)}$  </li>
</ul>
<h4 id="First_unsupervised_learning_algorithm:_clustering">First unsupervised learning algorithm: clustering</h4><p>give un-labelled training sets to learn coherent structure/cluster/subset.  </p>
<h4 id="Application_of_Clustering">Application of Clustering</h4><ul>
<li>market segementation  </li>
<li>social netowrk analysis: clustering algorithm to find conherent groups of friends   </li>
<li>organize computing clusters: how to design data center efficiently  </li>
<li>astronomical data analysis  </li>
</ul>
<h3 id="K-means_algorithm"><center>K-means algorithm </center></h3><ul>
<li>K-means : most popluar clustering algorithm   </li>
<li>k-means algorithm sudo code    </li>
<li>K-means algorithm for non-well-separated data   </li>
</ul>
<h4 id="K-means_introudction:">K-means introudction:</h4><p><strong>Step 1:</strong> initialize $K$ cluster centroids (ex. K=2 clusters -&gt; 2 points [red/blue])  </p>
<p><strong>Step 2:</strong> cluster assignments: loop through all the training sets data (1 to $m$) and mark data sets to index of the closer cluster centroids. In $K = 2$, mark as red (1) if it is closer to red cluster centroid, mark as blue (2), if closer to blue cluster centroid.<br><strong>Step 3:</strong> move the <strong>cluster centroids</strong> by doing mean of locations of all red/blue <strong>data points</strong> seperately<br><strong>Step 4:</strong> repeat step 2-3 to update cluster assignments based on current updated cluster centroids’ location until it saturtated.  </p>
<h4 id="K-means_algorithm_input:">K-means algorithm input:</h4><p>Input:  </p>
<ul>
<li>$K$ (number of clusters)  </li>
<li>Training set ${x^{(1)},x^{(2)},x^{(3)}, \dots x^{(m)}}$  </li>
</ul>
<p>$x^{(i)} \in R^n$ where $n$ represents number of features (drop $x_0 = 1$ convention)</p>
<h4 id="K-means_algorithm:">K-means algorithm:</h4>

Randomly initialize $K$ cluster centroids $\mu_1,\mu_1,\mu_1, \dots \mu_K \in R^n [vector \ \mu]$  
Repeat {  
	for $i = 1$ to $m$: [data points cluster/color  assignemnt]  
		$c^{(i)}$ [vector] = index (from 1 to $K$) of cluster centroid closes to $x^{(i)}$  
	for $k=1$ to $K$: [move centroid]  
		$\mu_k$ = average (mean) of points, update $K$ centroids' positions   
}  


<h4 id="K-means_for_non-well-separated_clusters">K-means for non-well-separated clusters</h4><p>The following example shows how K-means algorithm separate non-well-separated clusters:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-mean-non-separate.png" alt="">  </p>
<p>After running K-mean on the data set above, the algorithm will automatically cluster the data points into three clusters.  </p>
<h3 id="Optimization_Objective-1"><center> Optimization Objective </center></h3><ul>
<li>cost function : optimization objective  </li>
<li>notation for  $c^{(i)},$ $x^{(i)},$ $\mu_k $, $\mu_{c^{(i)}}$ </li>
</ul>
<h4 id="K-means_optimization_objective">K-means optimization objective</h4><p>$c^{(i)} = $ index of cluster ($1,2, \dots K$) to which data point $x^{(i)}$ is currently assigned.<br> $\mu_k = $ location of cluster centroid $k \ (\mu_k \in R^n)$  <br>$\mu_{c^{(i)}} = $ location of <strong>cluster centroid</strong> of cluster to which example data $x^{(i)}$ has been assigned.  </p>
<p>Optimization objective:<br> $$J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots \mu_K) = \frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} - \mu_{c^{(i)}} \Vert^2$$ </p>
<p>$$min \ J(cost \ function)  $$ -&gt; distortion k-mean cost function </p>
<h3 id="Random_Initialization"><center> Random Initialization </center></h3><ul>
<li>how to random initialization cluster centroids  </li>
<li>how to avoid local minimum  </li>
<li>multiple random initialization algorithm  </li>
</ul>
<h4 id="Random_initialization">Random initialization</h4><ul>
<li>should have $K &lt; m$  </li>
<li><strong>randomly</strong> pick $K$ training examples  </li>
<li><strong>set</strong> $\mu_1, \dots, \mu_K$ equal to these $K$ examples  </li>
</ul>
<blockquote>
<p><strong>Notes</strong>: Depends on location of random initialization, K-means algorithm can actually end up in different solution.  </p>
</blockquote>
<h4 id="Avoid_local_optima">Avoid local optima</h4><p>Example training set:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-dataset.png" alt=""></p>
<p>Global minimum (Good):<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-global-cluster.png" alt=""></p>
<p>Local optima (Not good):<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal.png" alt=""></p>
<p>Solution: Try <strong>multiple random initialization</strong> ($50-1000$ times) to get global optima.  </p>
<h4 id="Multiple_Random_initialization_algorithm">Multiple Random initialization algorithm</h4><p>For $i=1 \ to \ 100 $ {  </p>
<ol>
<li>Randomly initialize K-means  </li>
<li>Run K-means. Get $c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k$  </li>
<li>Compute cost function (distortion) $J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k)$  </li>
</ol>
<p>}  </p>
<p>Pick clustering htat gave <strong>lowest</strong> cost $J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k)$  </p>
<blockquote>
<p><strong>Note</strong>: multiple random initialization is especially helpful for small number of cluster ($K = 2-&gt;10$). With a large $K$, multiple random initialization is not necessary.  </p>
</blockquote>
<h3 id="Choosing_the_Number_of_Clusters"><center> Choosing the Number of Clusters </center></h3><ul>
<li>how to choose the number of clusters $K$  </li>
</ul>
<h4 id="Elbow_method">Elbow method</h4><p>Graph Value of the cost function verus the no. of the cluster $K$. The location where “elbow” appear is the location of the best $K$ value.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-elbow-method.png" alt="">  </p>
<p>However, elbow method does not always works well:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/elbow-method-bad.png" alt=""></p>
<p>Choose by hand <strong>manually</strong> (tshirt size: xs, s, m, l, xl OR s, m, l) is still the best approach for choosing $K$.  </p>
<h1 id="Week_8:_Lesson_3">Week 8: Lesson 3</h1><h2 id="Motivation">Motivation</h2><h3 id="Motivation_I:_Data_Compression"><center> Motivation I: Data Compression </center></h3><ul>
<li>compressed and reduce dimension of redundant data -&gt; less computer memory, disk space  </li>
<li>speed up the learning algorithm  </li>
</ul>
<h4 id="Data_Compression_From_2D_to_1D">Data Compression From 2D to 1D</h4><ul>
<li>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will help to reduce the memory and disk usage.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/data-compression.png" alt="">  </p>
<blockquote>
<p><strong>Note</strong>: in dimensionality reduction, variable $m$ will stay the same size; $n$ the number of features each example carries, will be reduced.  </p>
</blockquote>
<h3 id="Motivation_II:_Data_Visualization"><center> Motivation II: Data Visualization </center></h3><ul>
<li>better understand our data with data visualization.  </li>
<li>with large number of features, $n=50$ for example, we could select two or three features, which can effectivly summarize all other features, and plot it in 2D or 3D.  </li>
</ul>
<h1 id="Week_8:_Lesson_4">Week 8: Lesson 4</h1><h2 id="Principal_Component_Analysis">Principal Component Analysis</h2><h3 id="Principal_Component_Analysis_Problem_Formulation"><center>Principal Component Analysis Problem Formulation</center></h3><ul>
<li>PCA introduction: an compression algorithm  </li>
<li>PCA definition  </li>
<li>PCA terms: direction vector (+, - direction all work)  </li>
<li>PCA vs. Linear Regression  </li>
</ul>
<h4 id="Principal_Component_Analysis_(PCA)_problem_formulation">Principal Component Analysis (PCA) problem formulation</h4><p><strong>Reduce from 2D to 1D:</strong> Find a direction vector, which can be used to project data onto it so as to <strong>minimize</strong> the projection error.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/pca.png" alt=""></p>
<p>PCA try to find a <strong>lower dimensional surface</strong> (3D to 2D) to project the data which minimize the square distance between each points and lower the projection error.  </p>
<blockquote>
<p><strong>Note</strong>: projection error is the sum of square of blue segement above.  </p>
</blockquote>
<p><strong>Reduce from n-dimension to k-dimension:</strong> find $k$ vectors $\mu^{(1)}, \mu^{(2)},\mu^{(3)}, \dots, \mu^{(k)}$ onto which to project the data, so as to minimize the projection error.  </p>
<h4 id="PCA_vs_Linear_Regression">PCA vs Linear Regression</h4><ul>
<li>In linear regression, given $x$, predict value $y$ with the hypothesis function.  </li>
<li>In PCA, only input data are $x_1, x_2, \dots, x_m$</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/pca-vs-linear-regression.png" alt=""></p>
<ul>
<li>PCA, the sum of projection error (blue segment) is <strong>perpendicular</strong> to the direction vector.   </li>
<li>While, in linear regression, the square error (blue segment), is <strong>perpendicular</strong> to the x-axis.  </li>
</ul>
<p>Next, we will learn how to find that lower dimension surface which we could project the data.  </p>
<h3 id="Principal_Component_Analysis_Algorithm"><center>Principal Component Analysis Algorithm</center></h3><ul>
<li>PCA algorithm steps introduce  </li>
<li>Data preprocessing (feature scaling/mean normalization)  </li>
<li>PCA algorithm: reduce feature in $n$ dimension to $k$ dimension.  </li>
<li>Two task for PCA algorithm: 1. figure direction vector$u^{(1)}, \dots, u^{(k)}$, and 2. find $z_1, z_2, \dots, z_m$  </li>
<li>PCA Algorithm in three steps.   </li>
</ul>
<h4 id="Data_Preprocessing">Data Preprocessing</h4><p>Training set: $x^{(1)},x^{(2)}, \dots x^{(m)}$<br>Preprocessing (feature scaling/mean normalization):  </p>
<p><strong>Feature Scaling:</strong>  </p>
<ol>
<li><p>Mean of feature $j$: $$\mu_j=\frac{1}{m} \sum_{i=1}^m x_j^{(i)}$$</p>
</li>
<li><p>Replace each $x_j^{(i)}$ with $x_j-\mu_j$. </p>
</li>
</ol>
<p><strong>Mean normalization:</strong>  </p>
<p>$$\frac{x_j^{(i)} - \mu_j}{s_j}$$</p>
<p>scale features to have comparable range of values.  </p>
<h4 id="PCA_Algorithm_Introduction">PCA Algorithm Introduction</h4><p>Reduce feature from $n$ dimensional to $k$ dimensional as follow:  </p>
<p>$$x^{(i)} \in R^n -&gt; z^{(i)}  \in R^k $$</p>
<p>where $n$ is the number of feature, and $k$ is the number of reduced feature.  </p>
<p>PCA has following <strong>two</strong> tasks:  </p>
<ol>
<li>figure out $u^{(1)}, \dots, u^{(k)}$, where $k$ denotes the dimension of the reduced <strong>direction vector</strong>.  </li>
<li>find $z_1, z_2, \dots, z_m$, where $m$ is the number of the data points.  </li>
</ol>
<h4 id="PCA_Algorithm">PCA Algorithm</h4><h5 id="1-_Compute_“covariance_matrix”">1. Compute “covariance matrix”</h5><p>$$\large Σ=\frac{1}{m}\sum_{i=1}^m(x^{(i)})(x^{(i)})^T$$</p>
<p>Correspond Octave code:  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">Sigma = (<span class="number">1</span>/m) * X<span class="operator">'</span> * X;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: $x^{(i)}$ is $n \times 1$ vector. $(x^{(i)})^T$ is $1 \times n$ vector, and $X$ is $m \times n$ vector. Dimension of $\Sigma$ is $n \times n$.  </p>
</blockquote>
<h5 id="2-_Compute_“eigenvectors”_of_covariance_matrix_Σ">2. Compute “eigenvectors” of covariance matrix Σ</h5><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma);</span><br></pre></td></tr></table></figure>
<p><code>svd()</code> is the ‘singular value decomposition’, a built-in Octave function. We are only interested in the <code>U</code> matrix of the Sigma covariance matrix: $U \in R^{n \times n}$  </p>
<h5 id="3-_Take_the_first_k_columns_of_the_U_matrix_and_compute_z">3. Take the first k columns of the U matrix and compute z</h5><p>reduce $n$ feature dimension to $k$ feature dimension.  </p>
<p>$$z^{(i)} = Ureduce^T*x^{(i)}$$</p>
<p>$Ureduce^T$ is in $k \times n$, while $x^{(i)}$ will have $n \times 1$ dimension. And the product of $Ureduce^T*x^{(i)}$ will have dimension $k \times 1$</p>
<h4 id="Summary">Summary</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">Sigma = (<span class="number">1</span>/m) * X<span class="operator">'</span> * X;  <span class="comment">% compute the covariance matrix</span></span><br><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma);    <span class="comment">% compute our projected directions</span></span><br><span class="line">Ureduce = U(:,<span class="number">1</span>:k);      <span class="comment">% take the first k directions</span></span><br><span class="line">Z = X * Ureduce;         <span class="comment">% compute the projected data points</span></span><br></pre></td></tr></table></figure>
<h1 id="Week_8:_Lesson_5">Week 8: Lesson 5</h1><h2 id="Applying_PCA">Applying PCA</h2><h3 id="Reconstruction_from_Compressed_Representation"><center>Reconstruction from Compressed Representation</center></h3><ul>
<li>PCA compression algorithm: $n$ dimensional feature compress to $k$ dimensional feature.  </li>
<li>Introduce way to reconstruct the approximate $n$ dimensional feature from the compressed $k$ dimensional feature.  </li>
<li>how to go back from $z^{(i)}$ back to $x^{(i)}$</li>
</ul>
<h4 id="Reconstruction_from_compressed_representation">Reconstruction from compressed representation</h4><p>PCA compressed algorithm:  </p>
<p>$$z=U_{reduce}^T * x$$</p>
<p>To go from 1D back to 2D, we do $z \in R -&gt; x \in R^2$  </p>
<p>Approx. reconstruction from compressed representation:  </p>
$$x_{approx}^{(1)}=U_{reduce} * z^{(1)}$$
<h3 id="Choosing_the_Number_of_Principal_Components"><center>Choosing the Number of Principal Components</center></h3><ul>
<li>$k$ : a parameter of PCA algorithm  </li>
<li>how to choose the $k$ principle compoent  </li>
</ul>
<h4 id="Choosing_$k$_(number_of_principal_components)">Choosing $k$ (number of principal components)</h4><p>How to choose $k$, the dimension we are reducing the feature to, which is also called the <em>number of principal components</em>?  </p>
<p>One way to choose $k$ is the following formula:  </p>
<p><strong>Average squared projection error:</strong> </p>
$$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$$
<p><strong>Total variation in the data:</strong> (how far is each data point away from the origin <code>(0, 0)</code>)</p>
<p>$$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$$</p>
<p><strong>Choose the smallest $k$ that satisfy:</strong><br>
$$\large \dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$$
</p>
<p>Equation above wants the squared projection error divided by the total <strong>variation should be less than 1%</strong>, which is also called <strong>99% of the variance is retained</strong>.  </p>
<h4 id="Algorithm_for_choosing_$k$_with_loop">Algorithm for choosing $k$ with loop</h4><ol>
<li>Try PCA with $k=1,2, \dots$  </li>
<li>Compute $U_{reduce}, z, x_{approx}$  </li>
<li>check if $\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$ (99% of the variance retained)  </li>
<li>repeat step 1-3 until smallest $k$ found that satisfy 99% of the variance retained.  </li>
</ol>
<p>The algorithm above works, however, it is <strong>inefficient</strong>.  </p>
<h4 id="Algorithm_for_choosing_$k$_with_matrix_S">Algorithm for choosing $k$ with matrix S</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma)</span><br></pre></td></tr></table></figure>
<p>It is much efficent to check for 99% of retained variance with S matrix with following:  </p>

$$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$

<p>which we only need to run <code>svd()</code> once for choosing the smallest $k$ value that retain 99% of variance.  </p>
<p>Quiz: </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-q.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-a.png" alt=""></p>
<h3 id="Advice_for_Applying_PCA"><center>Advice for Applying PCA</center></h3><h4 id="Supervised_learning_speedup">Supervised learning speedup</h4><p>The most common use of PCA, an <strong>unsupervised</strong> learning algorithm, is to speed up a <strong>supervised</strong> learning algorithm.  </p>
<p>Note: PCA only on training set, but not on the cv or test sets???  </p>
<h4 id="Application_of_PCA">Application of PCA</h4><ul>
<li>Compression (reduce disk space/ memory use &amp; speed up algorithm)  </li>
<li>Visualization ($k = 2 or k = 3$)  </li>
</ul>
<h4 id="Bad_use_of_PCA:_To_prevent_overfitting">Bad use of PCA: To prevent overfitting</h4><p>Even PCA might help to reduce number of features for training set, which seems possible benefit to prevent overfitting. It might work, but it is not recommended because it does not consider the values of our result $y$. It is better to use regularization.  </p>
<h4 id="PCA_is_used_when_it_shouldn’t_be">PCA is used when it shouldn’t be</h4><p>try with original data set first before deciding to use PCA (when memory requirement not meet/disk space require too large) with strong reasoning and argument.  </p>
<h4 id="Benefit_of_PCA_compression_:">Benefit of PCA compression    :</h4><ul>
<li>compress the data so it take <strong>less</strong> disk space/ computer memory  </li>
<li>to reduce the dimension of input data so as to <strong>speed up</strong> the learning algorithm (Andrew use for this purpose most of time) </li>
<li>to visualize high-dimensional data ($k=2 or k = 3$)</li>
</ul>
<h1 id="Week_9:_Lesson_1">Week 9: Lesson 1</h1><h2 id="Density_Estimation">Density Estimation</h2><h3 id="Problem_Motivation"><center> Problem Motivation </center></h3><ul>
<li>how to automatically detect and flag un-normal data point.  </li>
<li>understand the model $p(x)$ to detect abnormal data point.  </li>
<li>If  $p(X_{test}) < \epsilon $, flag anomaly, If $p(X_{test}) \geq \epsilon $, flag OK.  </li>
</ul>
<h4 id="Anomaly_detection_example">Anomaly detection example</h4><p><strong>Aircraft engine features:</strong>  </p>
<ul>
<li>$x_1 = $ heat generated  </li>
<li>$x_2 = $ vibration intensity  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/anormal-example.png" alt=""></p>
<p>The red-x-cross above represent previous data set from normal aircraft engine. and the green-x-cross represnts newly built engine. If the new data point is too far away from the previous cluster (vibration and heat parameter far away from normal data), that example is call anomaly and indicates the engine might need to further testing to ensure its quality.  </p>
<h4 id="Density_estimation">Density estimation</h4><p>Given Dataset $x^{(1)}, x^{(2)}, \dots ,x^{(m)}$?<br>Want to know: Is $x_{test}$ anomalous? </p>
<p>Model $p(x)$  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/p-model.png" alt="">  </p>
<p>Function $p(x) $ is the probability that a new data point is OK.<br>If $p(X_{test}) &lt; \epsilon $, flag anomaly<br>If $p(X_{test}) \geq \epsilon $, flag OK.  </p>
<h4 id="Example_of_anomaly_detection:_Fraud_detection">Example of anomaly detection: Fraud detection</h4><ul>
<li>Coursera identification check, stolen account check, etc.</li>
<li>$x^{(i)}$ = features of user’s activities. ($x_1$ = user’s typing speed, $x_2$ = normal login time. etc)  </li>
<li>Model $p(x)$ from data  </li>
<li>Identify unusual users by checking which has $p(x) &lt; \epsilon$  </li>
</ul>
<h4 id="Example_of_anomaly_detection:_Monitoring_computers_in_a_data_center">Example of anomaly detection: Monitoring computers in a data center</h4><ul>
<li>$x^{(i)}$ = features of machine $i$.  </li>
<li>$x_1$ = memory use, $x_2$ = number of disk accesses/sec, $x_3$ = CPU load.  </li>
<li>if any of the data point (machine) result in $p(x) &lt; \epsilon$, system will flag that machine anomaly (that will let us know there is possible failure for the machine).  </li>
</ul>
<h3 id="Gaussian_Distribution"><center> Gaussian Distribution </center></h3><ul>
<li>review gaussian distribution  </li>
<li>understand the meaning of gaussian distribution parameters $\sigma, \mu, \sigma^2$  </li>
<li>given a set of data points, able to estimate the gaussian distribution (estimate the $\mu, \sigma^2$ parameters).  </li>
</ul>
<h4 id="Gaussian_(Normal)_distribution">Gaussian (Normal) distribution</h4><p><strong>Parameters:</strong><br>$\sigma$ - witdth of the gaussian, width of one standard diviation<br>$\sigma^2$ - variance<br>$\mu$ - mean, location of the peak  </p>
<p><strong>Property:</strong>  </p>
<ul>
<li>area under the bell curve is always intergated to 1.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-distribution.png" alt=""></p>

$\large p(x; \mu, \sigma^2)=\frac{1}{\sqrt{2pi} * \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$  

<p>read as probability of $x$ parameterized by $\mu$ and $\sigma^2$.  </p>
<h4 id="Gaussian_distribution_example">Gaussian distribution example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-example.png" alt=""></p>
<h4 id="Parameter_estimation">Parameter estimation</h4><p>Given set of data, can you estimate the gaussain distribution parameters? (how the data set distribute).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/parameter-estimation.png" alt=""></p>
<p>Estimate the parameter $\mu$ from a given data set by simply taking the avaerage of all the examples:  </p>
<p>$$\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}$$</p>
<p>Parameter $\sigma^2$ can be estimated by:  </p>
<p>$$\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/gaussian-estimation-example.png" alt=""></p>
<h3 id="Algorithm"><center> Algorithm </center></h3><ul>
<li>density estimation $p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)$  </li>
<li>Anomaly detection algorithm  </li>
</ul>
<h4 id="Density_estimation-1">Density estimation</h4><p>How to implenet $p(x)$ base on feature of training examples $\lbrace x^{(1)},\dots,x^{(m)}\rbrace$?</p>
<p>For <strong>each</strong> of the training example $x$ (total number of $m$)</p>

$$p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)$$

<p>where $x_1$ denotes the first feature of $x$, and there are total $n$ number of features.  And $\mu_1$, and $\sigma_1^2$ denotes the gaussian distribution of feature 1.  </p>
<h4 id="Anomaly_detection_algorithm">Anomaly detection algorithm</h4><ul>
<li>1.Choose features $x_i$ that might be <strong>evidence</strong> of abnormal activities (ex. vibration and temperature for flight engine).  </li>
<li>2.Fit parameters $\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2$ Calculate:  </li>
</ul>
$$\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}$$  
$$\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2$$  
<ul>
<li>3.Given new example $x$, compute $p(x)$  </li>
</ul>
$$p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})$$  
<p>Flag anomaly if $p(x)&lt;ϵ$  </p>
<h4 id="Anomaly_detection_example-1">Anomaly detection example</h4><p>Data $x$ with two features $x_1$, and $x_2$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160808/anomal-detection-example.png" alt=""></p>
<h1 id="Week_9:_Lesson_2">Week 9: Lesson 2</h1><h2 id="Building_an_Anomaly_Detection_System">Building an Anomaly Detection System</h2><h3 id="Developing_and_Evaluating_an_Anomaly_Detection_System"><center> Developing and Evaluating an Anomaly Detection System </center></h3><ul>
<li>focus on evaluating anomaly detection system.  </li>
<li><strong>60%, 20%, 20% split</strong> of normal data for training, CV, test set data.  </li>
<li><strong>50%, 50% split</strong> of anomalous ($y=1$) for CV, and test set data.  </li>
<li><strong>Skew Data:</strong> the number of one type of data is significantly larger than the other type of data.  </li>
<li><strong>Evaluation matrics</strong> for skew data set.  </li>
<li>use cv set to choose parameter $\epsilon$.  </li>
</ul>
<h4 id="Important_Property">Important Property</h4><ul>
<li>Fit model $p(x)$ on training set ${x(1),…,x(m)}$  </li>
<li>On a cross validation/test example x, predict:  </li>
<li>If $p(x)&lt;ϵ$ (<strong>anomaly</strong>), then $y=1$  </li>
<li>If $p(x)≥ϵ$ (<strong>normal</strong>), then $y=0$</li>
</ul>
<h4 id="The_importance_of_real-number_evaluation">The importance of real-number evaluation</h4><ul>
<li>lots of design choice: features, etc.  </li>
<li>much easier if we have a way to evaluating our learning algorithm.  </li>
<li>unlabelled training set: only has $x$ value, no $y$ value needed (assume normal examples).  </li>
<li>cross validation/test set: has both $x$ and $y$ values, where $y=0$ denotes normal data, and $y=1$ denotes anomalous data. (check if learning algorithm correctly classify them).  </li>
</ul>
<h4 id="Aircraft_engines_motivating_example">Aircraft engines motivating example</h4><ul>
<li>60%, 20%, 20% split of normal data for training, CV, test set data.  </li>
<li>50%, 50% split of anomalous ($y=1$) for CV, and test set data.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160810/Anomaly%20Detection-aircraft-example.png" alt=""></p>
<h4 id="Algorithm_evaluation">Algorithm evaluation</h4><ul>
<li>fit model $p(x)$ with unlabelled training set $x^{(1)}, \dots, x^{(m)}$  </li>
<li>Use data from cv/test set to test against $p(x)$ model just learned.  </li>
<li>What will be a good evaluation metrics?  </li>
</ul>
<p>Since the number of normal data is far more than the number of anomaly data, algorithm that always predicting $y=0$(normal) will result in a very high accuracy. Therefore, we need some other evaluation metrics for [<strong>skewed</strong>] data set.  </p>
<p><strong>Possible evaluation metrics:</strong>  </p>
<ul>
<li>True positive, false positive, false negative, true negative  </li>
<li>Precision/Recall  </li>
<li>$F_1$ score  </li>
</ul>
<p>We can also use cv set to choose parameter $\epsilon$.  </p>
<h3 id="Anomaly_Detection_vs-_Supervised_Learning"><center> Anomaly Detection vs. Supervised Learning </center></h3><ul>
<li>decision for anomaly detection OR supervised learning  </li>
<li>Examples of Anomaly Detection/supervised learning  </li>
</ul>
<h4 id="Use_anomaly_detection_when:">Use anomaly detection when:</h4><ul>
<li>small number of postive examples ($y=1$) [0-20 examples] and a large number of negative ($y=0$) examples.  </li>
<li>many different “types” of anomailies (since lots of ways to go wrong, can’t list out all combination of way to go wrong, but we know the range of negative example ($y=0$ normal), we say the one is smaller than $\epsilon$ is abnormal).  </li>
<li>other way to say for above condition: future anomalies may look nothing like any of the anomalous examples we have seen so far.  </li>
</ul>
<h4 id="Use_supervised_learning_when:">Use supervised learning when:</h4><ul>
<li>We have <strong>large</strong> number of <strong>both</strong> postive and negative examples.  </li>
<li>We have <strong>enough</strong> positive examples for the algorithm to <strong>know</strong> how does new positives examples look like.  </li>
<li>Other word, the future positive example are likely <strong>similar</strong> to the ones in the training set.  </li>
</ul>
<p>Spam was think as supervised learning since it has lots of postive example to learn spam samples well.  </p>
<h4 id="Examples_of_Anomaly_Detection">Examples of Anomaly Detection</h4><ul>
<li>fraud detection (assume NOT many abnormal user action)  </li>
<li>Manufacturing (e.g. aircraft engines failure)  </li>
<li>Monitoring machines in data center (detect machine failure)  <h4 id="Examples_of_Supervised_Learning">Examples of Supervised Learning</h4></li>
<li>Email spam classification  </li>
<li>Weather prediction (sunny/rainny/etc)  </li>
<li>Cancer classification  </li>
</ul>
<h3 id="Choosing_What_Features_to_Use"><center> Choosing What Features to Use </center></h3><ul>
<li>how to choose features for Anomaly detection algorithm  </li>
<li><strong>transforamtion</strong> for un-gaussaion feature  </li>
<li>adding more <strong>features/combination of current features</strong> allow you to better imporve the learning algorithm to flag abnormal example.  </li>
<li><code>hist()</code> to find best transformation to make feature $x$ gaussian liked.   </li>
</ul>
<h4 id="Non-gaussian_features">Non-gaussian features</h4><ul>
<li>first: need to check if the feature you choose looks like a gaussian.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160810/gaussian.png" alt=""></p>
<p><strong>Non-gaussian feature</strong></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160810/non-gaussian-look.png" alt=""></p>
<h4 id="Non-Gaussian_feature_transformation">Non-Gaussian feature transformation</h4><p><strong>Transfer Non-gaussian feature to gaussian feature</strong></p>
<p><strong>$Log(x)$ transform</strong><br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160810/non-gaussain-to-gaussian.png" alt=""></p>
<p><strong>Other transform:</strong>  </p>
<ul>
<li>$log(x+1)$  </li>
<li>$log(x+c)$ for some constant  </li>
<li>$\sqrt(x)$  </li>
<li>$x^{\frac{1}{3}}$</li>
</ul>
<h4 id="Plot_hist_on_matlab">Plot hist on matlab</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">hist(x) <span class="comment">% plot feature x in histgarm</span></span><br><span class="line">hist(x, <span class="number">50</span>) <span class="comment">% plot feature x in histgarm with fine grain</span></span><br></pre></td></tr></table></figure>
<h4 id="Error_analysis_for_anomaly_detection">Error analysis for anomaly detection</h4><p>If my algorithm with feature $x_1$ failing to flag a failure engine:  </p>
<ul>
<li>1.located the $x_1$ value of the failure engine.  </li>
<li>2.find another feature $x_2$ that can give a small $p(x_2)$ value, so that the overall $p(x)$ value will be smaller than $\epsilon$ to flag the anomaly engine (green cross below).  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160810/anomaly-error-analysis.png" alt=""></p>
<h4 id="Monitoring_computers_in_a_data_center">Monitoring computers in a data center</h4><ul>
<li>Choose featuers that might take on <strong>unusually</strong> <strong>large</strong> or <strong>small</strong> values in the event of an anomaly.  </li>
</ul>
<p><strong>Current featuers:</strong>  </p>
<p>$x_1$ = memory use of computer<br>$x_2$ = number of disk accesses/sec<br>$x_3$ = CPU load<br>$x_4$ = network traffic  </p>
<p>Adding <strong>more features</strong> allow you to <strong>locate</strong> the casue of error <strong>more accurately</strong>.  </p>
<p>$x_5$ = $\frac{CPU load}{Netwrok traffic}$  % a large value of $x_5$ allows you to locate there is a possible infinte loop in the program that use up lots of CPU since the network traffic is not high.  </p>
<h1 id="Week_9:_Lesson_5">Week 9: Lesson 5</h1><h2 id="Predicting_Movie_Ratings">Predicting Movie Ratings</h2><h3 id="Recommander_System_Problem_Formulation"><center> Recommander System Problem Formulation </center></h3><ul>
<li>important ML area: imporovement of recommander system (netflix, amazon, etc.)  </li>
<li>less study in academic, but has large market value.  </li>
<li>Introduce some notation used in [recommander system]</li>
</ul>
<h4 id="Example:_predicting_movie_ratings">Example: predicting movie ratings</h4><p><strong>Notation:</strong><br>$n_u$ = number of users<br>$n_m$ = number of movies<br>$r(i,j)$ = 1 if user $j$ has rated movie $i$<br>$y^{(i,j)}$ = rating given by user $j$ to movie $i$ (defined only if $r(i,j) = 1$)  </p>
<h3 id="Content_Based_Recommendations"><center> Content Based Recommendations </center></h3><ul>
<li>content based recommandation to build system.  </li>
<li>content based: we alreay know all the features of the movie in advance.  </li>
<li><strong>cost function</strong> and <strong>gradient descent</strong> for content base recommander system.  </li>
<li>given features value $x^{(i)}$ of movie and $\theta^{(j)}$ for user’s perference, perdict user’s unwatched moive rating.  </li>
</ul>
<h4 id="Content-based_recommaender_systems">Content-based recommaender systems</h4><p>Movie recommander system examples with two features $x_1(romance)$ and $x_2(action)$.  (interset term $x_0$ = 1)</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommand-system-movie.png" alt=""></p>
<p>For each user $j$, learn a parameter $\theta^{(j)} \in \mathbb{R}^{n+1}$. Predict user $j$ as rating movie $i$ with $(\theta^{(j)})^Tx^{(i)}$ stars.  </p>
<p>Given first user’s parameter $\theta^{(1)}$, we can use the equation above to perdict how will user rate an unrated moive.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/movie-rating.png" alt="">  </p>
<h4 id="Learn_parameter_$\theta^{(j)}$_for_user_$j$:">Learn parameter $\theta^{(j)}$ for user $j$:</h4><ul>
<li>given $x^{(i)}, \dots, x^{(n_m)}$ (movie features value) + movie ratings ($y^{(i,j)}$), we can estimate $\theta^{(1)}, \dots, \theta^{(n_u)}$   </li>
</ul>
<p>To learn/estimate $\theta^{(j)}$ (parameter for user $j$):  </p>

$$\large min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2$$

<p><strong>More notations:</strong><br>$θ^{(j)}$= parameter vector for user $j$<br>$x^{(i)}$= feature vector for movie $i$<br>For user $j$, movie $i$, predicted rating: $(θ^{(j)})^T(x^{(i)})$<br>$m^{(j)}$= number of movies <strong>rated</strong> by user $j$  </p>
<p>The left side of the equation is the <strong>square error term</strong>, and the right side is the <strong>regularization term</strong>.  </p>
<h4 id="Learn_parameter_$\theta^{(j)}$_for_all_users)">Learn parameter $\theta^{(j)}$ for all users)</h4><p>To learn/estimate $\theta^{(j)}$ (parameter for all users):<br>
$$\large min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u}  \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$$
</p>
<h4 id="Optimazation_algorithm:">Optimazation algorithm:</h4>
$$\large J(\theta^{(1)},\theta^{(2)}, \dots, \theta^{(n_u)})= \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u}  \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$$

<p><strong>Gradient descent</strong> update:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/gradient-descent-update.png" alt=""></p>
<p>Very similar to linear regression update process.  </p>
<h1 id="Week_9:_Lesson_6">Week 9: Lesson 6</h1><h2 id="Collaborative_Filtering">Collaborative Filtering</h2><h3 id="Collaborative_Filtering-1"><center> Collaborative Filtering </center></h3><ul>
<li>collaborative filtering : an approach to build recommander system.  </li>
<li><strong>learn</strong> itself what feature to use.  </li>
<li>given $\theta^{(i)}, \dots, \theta^{(n_u)}$ (movie features value) + movie ratings ($y^{(i,j)}$), we can estimate $x^{(1)}, dots, x^{(n_m)}$  </li>
<li><strong>collaborative</strong>: every users contribute a little bit (rating few movies myself) and combine to the overall good (recommander system learn better features).  </li>
</ul>
<h4 id="Problem_motivation">Problem motivation</h4><ul>
<li>time comsuming to watch every movie and tell how romantic/action this movie is.  </li>
<li>we want something more </li>
</ul>
<p>How many stars will user $j$ rate movie $i$<br>$$User_{rating}=(\theta^{(j)})^Tx^{(i)}$$.  </p>
<h4 id="Algorithm-1">Algorithm</h4>
Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(i)}$:  

$$min_{x^{(i)}} \dfrac{1}{2} \displaystyle  \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^{n} (x_k^{(i)})^2$$

Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(1)}, \dots x^{(n_m)}$:  

$$min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m}  \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2$$


<blockquote>
<p>Note: You can also <strong>randomly guess</strong> the values for theta to guess the features repeatedly. You will actually converge to a good set of features.  </p>
</blockquote>
<h3 id="Collaborative_Filtering_Algorithm"><center> Collaborative Filtering Algorithm </center></h3><ul>
<li>3 steps for collaborative filtering algorithm  </li>
</ul>
<h4 id="Random_$\theta$,_and_back-and-forward_between_$x$_and_$\theta$">Random $\theta$, and back-and-forward between $x$ and $\theta$</h4>Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(i)}$:   
Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(1)}, \dots x^{(n_m)}$:  

<h4 id="Combine_cost_function_in_terms_of_both_$x$_and_$\theta$">Combine cost function in terms of both $x$ and $\theta$</h4><ul>
<li>instead of going back and forward bewteen $\theta$ and $x$ for optimal solution  </li>
<li>minimize both sets of parameters $x$ and $\theta$ simultaneously.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommander-update-simultanously.png" alt=""></p>
<blockquote>
<p>Note: for the combination version above, we no longer need to hard code $x_0 = 1$, (the bias unit) same as to $\theta_0$. The reason behind that is we are learning both $x$ and $\theta$ together, no need to hard code for either of them. The dimension of both $x$ and $\theta$ become $\in \mathbb{R}^n$, where $n$ is the number of feature of the movie $x$.  </p>
</blockquote>
<h4 id="Collaborative_filtering_algorithm">Collaborative filtering algorithm</h4><p>These are the steps in the algorithm:  </p>
<ol>
<li>Initialize $x^{(i)},…,x^{(n_m)},θ^{(1)},…,θ^{(n_u)}$ to small random values. This serves to <strong>break symmetry</strong> and ensures that the algorithm learns features $x^{(i)},…,x^{(n_m)}$ that are <strong>different</strong> from each other.  </li>
<li>Minimize $J(x^{(i)},…,x^{(n_m)},θ^{(1)},…,θ^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).  </li>
</ol>
<p>E.g. for every $j=1,…,n_u$,$i=1,…n_m$:  </p>

$$x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)$$

$$\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)$$

<ol>
<li>For a user with parameters $θ$ and a movie with (learned) features $x$, predict a star rating of $(θ^{(j)})^Tx^{(i)}$.  </li>
</ol>
<h1 id="Week_9:_Lesson_7">Week 9: Lesson 7</h1><h2 id="Low_Rank_Matrix_Factorization">Low Rank Matrix Factorization</h2><h3 id="Vectorization:_Low_Rank_Matrix_Factorization"><center> Vectorization: Low Rank Matrix Factorization </center></h3><ul>
<li>vectorized implemtation of Collaborative Filtering Algorithm for perdicted rating.    </li>
<li>recommandation of related/similar products with similiar features.  </li>
</ul>
<h4 id="Vectorized_Collaborative_filtering">Vectorized Collaborative filtering</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/collaborate-filtering.png" alt=""> </p>
<p>Given matrices $X$ (each row containing features of a particular movie) and $Θ$ (each row containing the weights for those features for a given user), then the full matrix $Y$ of all predicted ratings of all movies by all users is given simply by: $Y=XΘ^T$.</p>
<blockquote>
<p>Note: In linear algebra, $X\Theta^T$ is called low rank matrix factorization.  </p>
</blockquote>
<h4 id="Find_related_products_(similiar_movie,_etc)">Find related products (similiar movie, etc)</h4><ul>
<li>measure how similiar two products base on the similiarity of the features.  </li>
</ul>
<p>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for the smallest value of $∣∣x(i)−x(j)∣∣$.</p>
<h3 id="Implementational_Detail:_Mean_Normalization"><center> Implementational Detail: Mean Normalization </center></h3><h4 id="Users_who_have_not_rated_any_movies">Users who have not rated any movies</h4><ul>
<li>if user, like me, did not rate any of movie, how will the recommander system response?  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160811/recommander-system-unrate-response.png" alt=""></p>

$$\large
J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2$$

<p>Since user Eve has no ratings to any of the moive, the square error term will be <strong>zero</strong>. The regularization term for $\theta$ will become <strong>zeros</strong>, since we are try to minimize the whole term.  </p>
<p>$\theta^{(5)} = [0; 0]$, therefore, the equation to estimate rating $(\theta^{(5)})^Tx^(i)$ will result in all <strong>zeros</strong> rating for all movie. However, zero rating to all moive does not help user Eve with recommanded moive. Maybe the <strong>average</strong> ratings of all user who rated the moive will help to better estimate the rating for unrated user.  </p>
<blockquote>
<p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned $θ$ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies $0$, which does not seem intuitively correct.</p>
</blockquote>
<h4 id="mean_normalization">mean normalization</h4><p>We rectify this problem by normalizing the data relative to the mean. First, we use a matrix $Y$ to store the data from previous ratings, where the ith row of $Y$ is the ratings for the $i$th movie and the $j$th column corresponds to the ratings for the jth user.  </p>
<p>First. define a vector<br>$$\mu  = [\mu_1, \mu_2, \dots , \mu_{n_m}]$$<br>such that<br>$$\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}$$</p>
<p><strong>Normalize</strong> data by subtracting $u$.  </p>
<p>$$Y =<br>\begin{bmatrix}<br>    5 &amp; 5 &amp; 0 &amp; 0  \newline<br>    4 &amp; ? &amp; ? &amp; 0  \newline<br>    0 &amp; 0 &amp; 5 &amp; 4 \newline<br>    0 &amp; 0 &amp; 5 &amp; 0 \newline<br>\end{bmatrix}, \quad<br> \mu =<br>\begin{bmatrix}<br>    2.5 \newline<br>    2  \newline<br>    2.25 \newline<br>    1.25 \newline<br>\end{bmatrix}$$</p>
<p>The resulting $Y′$ vector is:  </p>
<p>$$Y’ =<br>\begin{bmatrix}<br>  2.5    &amp; 2.5   &amp; -2.5 &amp; -2.5 \newline<br>  2      &amp; ?     &amp; ?    &amp; -2 \newline<br>  -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \newline<br>  -1.25  &amp; -1.25 &amp; 3.75 &amp; -1.25<br>\end{bmatrix}$$</p>
<p>After normailizing data, we can now use the update perdiction formula to better perdict user rating:  </p>
<p>$$(\theta^{(j)})^T x^{(i)} + \mu_i$$</p>
<p>Instead of simply initialized all <strong>zeros</strong> rating. Now, the perdicted rating for moive $i$ will base on the average of the other users on the same moive. This make more sense since if other people like moive $i$ a lot, it is very likely you will like it as well.  </p>
<h1 id="Logs">Logs</h1><ul>
<li>07/31/2016: Week 7 completed.  </li>
<li>08/05/2016: Week 8 Lesson 1, 3 completed.  </li>
<li>08/06/2016: Week 8 Lesson 4, 5 completed.  </li>
<li>08/08/2016: Week 9 Lesson 1 completed.  </li>
<li>08/09/2016: Week 9 Lesson 2 completed.  </li>
<li>08/11/2016: Week 9 Lesson 5, 6, 7 completed. </li>
</ul>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag">#Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/07/15/Recursion-vs-Iteration/" rel="next" title="Recursion vs Iteration">
                <i class="fa fa-chevron-left"></i> Recursion vs Iteration
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/07/18/ML4/" rel="prev" title="[Completed] Coursera: Machine Learning by Stanford - Part4">
                [Completed] Coursera: Machine Learning by Stanford - Part4 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/07/18/ML3/"
     data-title="[Completed] Coursera: Machine Learning by Stanford - Part3"
     data-content=""
     data-url="http://www.xuyiruan.com/2016/07/18/ML3/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/07/18/ML3/"
           data-title="[Completed] Coursera: Machine Learning by Stanford - Part3" data-url="http://www.xuyiruan.com/2016/07/18/ML3/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/author.jpg"
               alt="🍣之神" />
          <p class="site-author-name" itemprop="name">🍣之神</p>
          <p class="site-description motion-element" itemprop="description">阮先生’s blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">40</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ruanxuyi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/xuyiruan/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/xuyi-ruan-a728a889" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://mszhuchinese.com" title="MsZhuChinese" target="_blank">MsZhuChinese</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.winniebabe.com" title="WinnieBabe" target="_blank">WinnieBabe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://suncuss.me" title="BossSun" target="_blank">BossSun</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_1"><span class="nav-number">1.</span> <span class="nav-text">Week 7: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Large_Margin_Classification"><span class="nav-number">1.1.</span> <span class="nav-text">Large Margin Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization_Objective"><span class="nav-number">1.1.1.</span> <span class="nav-text"> Optimization Objective </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Alternative_logistic_regression_to_SVM"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Alternative logistic regression to SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_cost_function"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">SVM cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_hypothesis_function"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">SVM hypothesis function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Large_Margin_Intuition"><span class="nav-number">1.1.2.</span> <span class="nav-text">Large Margin Intuition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Support_Vector_Machine"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Support Vector Machine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Large_Margin_Classifier"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Large Margin Classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_constant_value_C"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">SVM constant value C</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mathematics_Behind_Large_Margin_Classification"><span class="nav-number">1.1.3.</span> <span class="nav-text"> Mathematics Behind Large Margin Classification </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vector_Inner_Product"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Vector Inner Product</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_2"><span class="nav-number">2.</span> <span class="nav-text">Week 7: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernels"><span class="nav-number">2.1.</span> <span class="nav-text">Kernels</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernels_I"><span class="nav-number">2.1.1.</span> <span class="nav-text"> Kernels I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hypothesis_function_for_SVM"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Hypothesis function for SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian_Kernels:_The_similarity_function"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Gaussian Kernels: The similarity function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Properties_of_the_similarity_function"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">Properties of the similarity function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Effect_of_sigma_value"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">Effect of sigma value</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernels_II"><span class="nav-number">2.1.2.</span> <span class="nav-text"> Kernels II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_landmarks"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">Choosing the landmarks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_parameters_$\Theta$"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">Choosing the parameters $\Theta$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_parameters"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">SVM parameters</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_3"><span class="nav-number">3.</span> <span class="nav-text">Week 7: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SVMs_in_Practice"><span class="nav-number">3.1.</span> <span class="nav-text">SVMs in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Using_An_SVM"><span class="nav-number">3.1.1.</span> <span class="nav-text">Using An SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kernel_(similarity)_functions:"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Kernel (similarity) functions:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementation_choice_for_SVM"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">Implementation choice for SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-class_classification"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">Multi-class classification</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_regression_vs-_SVMs"><span class="nav-number">3.1.1.4.</span> <span class="nav-text">Logistic regression vs. SVMs</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_1"><span class="nav-number">4.</span> <span class="nav-text">Week 8: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering"><span class="nav-number">4.1.</span> <span class="nav-text">Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised_Learning:_Introduction"><span class="nav-number">4.1.1.</span> <span class="nav-text">Unsupervised Learning: Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#First_unsupervised_learning_algorithm:_clustering"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">First unsupervised learning algorithm: clustering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application_of_Clustering"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">Application of Clustering</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means_algorithm"><span class="nav-number">4.1.2.</span> <span class="nav-text">K-means algorithm </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_introudction:"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">K-means introudction:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_algorithm_input:"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">K-means algorithm input:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_algorithm:"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">K-means algorithm:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_for_non-well-separated_clusters"><span class="nav-number">4.1.2.4.</span> <span class="nav-text">K-means for non-well-separated clusters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization_Objective-1"><span class="nav-number">4.1.3.</span> <span class="nav-text"> Optimization Objective </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_optimization_objective"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">K-means optimization objective</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random_Initialization"><span class="nav-number">4.1.4.</span> <span class="nav-text"> Random Initialization </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Random_initialization"><span class="nav-number">4.1.4.1.</span> <span class="nav-text">Random initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Avoid_local_optima"><span class="nav-number">4.1.4.2.</span> <span class="nav-text">Avoid local optima</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple_Random_initialization_algorithm"><span class="nav-number">4.1.4.3.</span> <span class="nav-text">Multiple Random initialization algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing_the_Number_of_Clusters"><span class="nav-number">4.1.5.</span> <span class="nav-text"> Choosing the Number of Clusters </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Elbow_method"><span class="nav-number">4.1.5.1.</span> <span class="nav-text">Elbow method</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_3"><span class="nav-number">5.</span> <span class="nav-text">Week 8: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">5.1.</span> <span class="nav-text">Motivation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivation_I:_Data_Compression"><span class="nav-number">5.1.1.</span> <span class="nav-text"> Motivation I: Data Compression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data_Compression_From_2D_to_1D"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">Data Compression From 2D to 1D</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivation_II:_Data_Visualization"><span class="nav-number">5.1.2.</span> <span class="nav-text"> Motivation II: Data Visualization </span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_4"><span class="nav-number">6.</span> <span class="nav-text">Week 8: Lesson 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Principal_Component_Analysis"><span class="nav-number">6.1.</span> <span class="nav-text">Principal Component Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal_Component_Analysis_Problem_Formulation"><span class="nav-number">6.1.1.</span> <span class="nav-text">Principal Component Analysis Problem Formulation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Principal_Component_Analysis_(PCA)_problem_formulation"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">Principal Component Analysis (PCA) problem formulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_vs_Linear_Regression"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">PCA vs Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal_Component_Analysis_Algorithm"><span class="nav-number">6.1.2.</span> <span class="nav-text">Principal Component Analysis Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data_Preprocessing"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_Algorithm_Introduction"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">PCA Algorithm Introduction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_Algorithm"><span class="nav-number">6.1.2.3.</span> <span class="nav-text">PCA Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-_Compute_“covariance_matrix”"><span class="nav-number">6.1.2.3.1.</span> <span class="nav-text">1. Compute “covariance matrix”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-_Compute_“eigenvectors”_of_covariance_matrix_Σ"><span class="nav-number">6.1.2.3.2.</span> <span class="nav-text">2. Compute “eigenvectors” of covariance matrix Σ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-_Take_the_first_k_columns_of_the_U_matrix_and_compute_z"><span class="nav-number">6.1.2.3.3.</span> <span class="nav-text">3. Take the first k columns of the U matrix and compute z</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summary"><span class="nav-number">6.1.2.4.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_5"><span class="nav-number">7.</span> <span class="nav-text">Week 8: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Applying_PCA"><span class="nav-number">7.1.</span> <span class="nav-text">Applying PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reconstruction_from_Compressed_Representation"><span class="nav-number">7.1.1.</span> <span class="nav-text">Reconstruction from Compressed Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reconstruction_from_compressed_representation"><span class="nav-number">7.1.1.1.</span> <span class="nav-text">Reconstruction from compressed representation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing_the_Number_of_Principal_Components"><span class="nav-number">7.1.2.</span> <span class="nav-text">Choosing the Number of Principal Components</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_$k$_(number_of_principal_components)"><span class="nav-number">7.1.2.1.</span> <span class="nav-text">Choosing $k$ (number of principal components)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm_for_choosing_$k$_with_loop"><span class="nav-number">7.1.2.2.</span> <span class="nav-text">Algorithm for choosing $k$ with loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm_for_choosing_$k$_with_matrix_S"><span class="nav-number">7.1.2.3.</span> <span class="nav-text">Algorithm for choosing $k$ with matrix S</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advice_for_Applying_PCA"><span class="nav-number">7.1.3.</span> <span class="nav-text">Advice for Applying PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Supervised_learning_speedup"><span class="nav-number">7.1.3.1.</span> <span class="nav-text">Supervised learning speedup</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application_of_PCA"><span class="nav-number">7.1.3.2.</span> <span class="nav-text">Application of PCA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bad_use_of_PCA:_To_prevent_overfitting"><span class="nav-number">7.1.3.3.</span> <span class="nav-text">Bad use of PCA: To prevent overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_is_used_when_it_shouldn’t_be"><span class="nav-number">7.1.3.4.</span> <span class="nav-text">PCA is used when it shouldn’t be</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Benefit_of_PCA_compression_:"><span class="nav-number">7.1.3.5.</span> <span class="nav-text">Benefit of PCA compression    :</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_9:_Lesson_1"><span class="nav-number">8.</span> <span class="nav-text">Week 9: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Density_Estimation"><span class="nav-number">8.1.</span> <span class="nav-text">Density Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Problem_Motivation"><span class="nav-number">8.1.1.</span> <span class="nav-text"> Problem Motivation </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Anomaly_detection_example"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">Anomaly detection example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Density_estimation"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Density estimation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example_of_anomaly_detection:_Fraud_detection"><span class="nav-number">8.1.1.3.</span> <span class="nav-text">Example of anomaly detection: Fraud detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example_of_anomaly_detection:_Monitoring_computers_in_a_data_center"><span class="nav-number">8.1.1.4.</span> <span class="nav-text">Example of anomaly detection: Monitoring computers in a data center</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gaussian_Distribution"><span class="nav-number">8.1.2.</span> <span class="nav-text"> Gaussian Distribution </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian_(Normal)_distribution"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">Gaussian (Normal) distribution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian_distribution_example"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">Gaussian distribution example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parameter_estimation"><span class="nav-number">8.1.2.3.</span> <span class="nav-text">Parameter estimation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Algorithm"><span class="nav-number">8.1.3.</span> <span class="nav-text"> Algorithm </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Density_estimation-1"><span class="nav-number">8.1.3.1.</span> <span class="nav-text">Density estimation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Anomaly_detection_algorithm"><span class="nav-number">8.1.3.2.</span> <span class="nav-text">Anomaly detection algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Anomaly_detection_example-1"><span class="nav-number">8.1.3.3.</span> <span class="nav-text">Anomaly detection example</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_9:_Lesson_2"><span class="nav-number">9.</span> <span class="nav-text">Week 9: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Building_an_Anomaly_Detection_System"><span class="nav-number">9.1.</span> <span class="nav-text">Building an Anomaly Detection System</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Developing_and_Evaluating_an_Anomaly_Detection_System"><span class="nav-number">9.1.1.</span> <span class="nav-text"> Developing and Evaluating an Anomaly Detection System </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Important_Property"><span class="nav-number">9.1.1.1.</span> <span class="nav-text">Important Property</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The_importance_of_real-number_evaluation"><span class="nav-number">9.1.1.2.</span> <span class="nav-text">The importance of real-number evaluation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Aircraft_engines_motivating_example"><span class="nav-number">9.1.1.3.</span> <span class="nav-text">Aircraft engines motivating example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm_evaluation"><span class="nav-number">9.1.1.4.</span> <span class="nav-text">Algorithm evaluation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Anomaly_Detection_vs-_Supervised_Learning"><span class="nav-number">9.1.2.</span> <span class="nav-text"> Anomaly Detection vs. Supervised Learning </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Use_anomaly_detection_when:"><span class="nav-number">9.1.2.1.</span> <span class="nav-text">Use anomaly detection when:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Use_supervised_learning_when:"><span class="nav-number">9.1.2.2.</span> <span class="nav-text">Use supervised learning when:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Examples_of_Anomaly_Detection"><span class="nav-number">9.1.2.3.</span> <span class="nav-text">Examples of Anomaly Detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Examples_of_Supervised_Learning"><span class="nav-number">9.1.2.4.</span> <span class="nav-text">Examples of Supervised Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing_What_Features_to_Use"><span class="nav-number">9.1.3.</span> <span class="nav-text"> Choosing What Features to Use </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-gaussian_features"><span class="nav-number">9.1.3.1.</span> <span class="nav-text">Non-gaussian features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-Gaussian_feature_transformation"><span class="nav-number">9.1.3.2.</span> <span class="nav-text">Non-Gaussian feature transformation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Plot_hist_on_matlab"><span class="nav-number">9.1.3.3.</span> <span class="nav-text">Plot hist on matlab</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Error_analysis_for_anomaly_detection"><span class="nav-number">9.1.3.4.</span> <span class="nav-text">Error analysis for anomaly detection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Monitoring_computers_in_a_data_center"><span class="nav-number">9.1.3.5.</span> <span class="nav-text">Monitoring computers in a data center</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_9:_Lesson_5"><span class="nav-number">10.</span> <span class="nav-text">Week 9: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Predicting_Movie_Ratings"><span class="nav-number">10.1.</span> <span class="nav-text">Predicting Movie Ratings</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Recommander_System_Problem_Formulation"><span class="nav-number">10.1.1.</span> <span class="nav-text"> Recommander System Problem Formulation </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example:_predicting_movie_ratings"><span class="nav-number">10.1.1.1.</span> <span class="nav-text">Example: predicting movie ratings</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Content_Based_Recommendations"><span class="nav-number">10.1.2.</span> <span class="nav-text"> Content Based Recommendations </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Content-based_recommaender_systems"><span class="nav-number">10.1.2.1.</span> <span class="nav-text">Content-based recommaender systems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learn_parameter_$\theta^{(j)}$_for_user_$j$:"><span class="nav-number">10.1.2.2.</span> <span class="nav-text">Learn parameter $\theta^{(j)}$ for user $j$:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Learn_parameter_$\theta^{(j)}$_for_all_users)"><span class="nav-number">10.1.2.3.</span> <span class="nav-text">Learn parameter $\theta^{(j)}$ for all users)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimazation_algorithm:"><span class="nav-number">10.1.2.4.</span> <span class="nav-text">Optimazation algorithm:</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_9:_Lesson_6"><span class="nav-number">11.</span> <span class="nav-text">Week 9: Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Collaborative_Filtering"><span class="nav-number">11.1.</span> <span class="nav-text">Collaborative Filtering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Collaborative_Filtering-1"><span class="nav-number">11.1.1.</span> <span class="nav-text"> Collaborative Filtering </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem_motivation"><span class="nav-number">11.1.1.1.</span> <span class="nav-text">Problem motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm-1"><span class="nav-number">11.1.1.2.</span> <span class="nav-text">Algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Collaborative_Filtering_Algorithm"><span class="nav-number">11.1.2.</span> <span class="nav-text"> Collaborative Filtering Algorithm </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Random_$\theta$,_and_back-and-forward_between_$x$_and_$\theta$"><span class="nav-number">11.1.2.1.</span> <span class="nav-text">Random $\theta$, and back-and-forward between $x$ and $\theta$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Combine_cost_function_in_terms_of_both_$x$_and_$\theta$"><span class="nav-number">11.1.2.2.</span> <span class="nav-text">Combine cost function in terms of both $x$ and $\theta$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Collaborative_filtering_algorithm"><span class="nav-number">11.1.2.3.</span> <span class="nav-text">Collaborative filtering algorithm</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_9:_Lesson_7"><span class="nav-number">12.</span> <span class="nav-text">Week 9: Lesson 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Low_Rank_Matrix_Factorization"><span class="nav-number">12.1.</span> <span class="nav-text">Low Rank Matrix Factorization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vectorization:_Low_Rank_Matrix_Factorization"><span class="nav-number">12.1.1.</span> <span class="nav-text"> Vectorization: Low Rank Matrix Factorization </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vectorized_Collaborative_filtering"><span class="nav-number">12.1.1.1.</span> <span class="nav-text">Vectorized Collaborative filtering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Find_related_products_(similiar_movie,_etc)"><span class="nav-number">12.1.1.2.</span> <span class="nav-text">Find related products (similiar movie, etc)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementational_Detail:_Mean_Normalization"><span class="nav-number">12.1.2.</span> <span class="nav-text"> Implementational Detail: Mean Normalization </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Users_who_have_not_rated_any_movies"><span class="nav-number">12.1.2.1.</span> <span class="nav-text">Users who have not rated any movies</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mean_normalization"><span class="nav-number">12.1.2.2.</span> <span class="nav-text">mean normalization</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logs"><span class="nav-number">13.</span> <span class="nav-text">Logs</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">🍣之神</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"xruan"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("oKoPG1XTXkC7T1oasGQwau2g-gzGzoHsz", "JFJCQtIsUKhrX6S7Gvgqdqgk");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
