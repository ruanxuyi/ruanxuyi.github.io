<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Bree Serif:300,300italic,400,400italic,700,700italic|Arimo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/sushi.png?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="article">
<meta property="og:title" content="Coursera: Machine Learning by Stanford - Part2">
<meta property="og:url" content="http://www.xuyiruan.com/2016/07/18/ML2/index.html">
<meta property="og:site_name" content="阮先生de小窝">
<meta property="og:description">
<meta property="og:image" content="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/diagnoise-benefit.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-training-set-polting.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-split-data.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/test-set-error-compute.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/misclassification-error.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/model-selection.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/split-three-section.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/three-part-model-selection.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/bias-variance.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/error-degree-polynomial.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/high-varience-bias.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/regularzation-parameter.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/choose-best-lambda.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/lambda-vs-cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/learning-curve-def.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-bias.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-variance.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/small-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/large-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/exercise.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/spam-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/precission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/percission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/compare-percission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/recall-percission-average.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/F-SCORE.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/large-amout-data.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/hypothesis.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-costfunction.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y0.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/large-margin-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/overfitting-underfitting-svm.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-bad-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-good-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/similarity-function-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160731/sigma-value.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-mean-non-separate.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-dataset.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-global-cluster.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-elbow-method.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/elbow-method-bad.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/data-compression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/pca.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/pca-vs-linear-regression.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-q.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-a.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Coursera: Machine Learning by Stanford - Part2">
<meta name="twitter:description">




<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> Coursera: Machine Learning by Stanford - Part2 | 阮先生de小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">阮先生de小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">多读书 多思考 少吃零食 多睡觉</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'TVdsuUVez9DD1Mx-gjc8','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Coursera: Machine Learning by Stanford - Part2
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-07-18T10:15:48+08:00" content="07-18-2016">
              07-18-2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/18/ML2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/07/18/ML2/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/07/18/ML2/" class="leancloud_visitors" data-flag-title="Coursera: Machine Learning by Stanford - Part2">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" alt=""></p>
<a id="more"></a>
<h1 id="Week_6:_Lesson_1">Week 6: Lesson 1</h1><h2 id="Evaluating_a_Learning_Algorithm">Evaluating a Learning Algorithm</h2><h3 id="Deciding_What_to_Try_Next"><center> Deciding What to Try Next </center></h3><ul>
<li>practical issue while implementing your learning algorithm  </li>
<li>Potential things you can try to make it better  </li>
<li>Diagnostics - a techical way to help you to analyize and help you effectly select on <strong>specific</strong> <strong>things</strong> to try and <strong>improve</strong> for your learning algorithm    </li>
</ul>
<h4 id="Problem_in_real_practice:">Problem in real practice:</h4><p>Lets say you test your hypothesis on a new set of houses with regularized linear regression.  </p>
$$J(\theta) =  \frac 1 {2m}[\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j=1}^{n}\theta_j^2] $$  
<blockquote>
<p>What if you find that it makes unacceptably large errors in its predictions, what could you try next?  </p>
</blockquote>
<ul>
<li>get <strong>more</strong> training examples (not work well as always)  </li>
<li><strong>smaller</strong> sets of features(prevent overfitting)  </li>
<li>Try getting <strong>additional</strong> features  </li>
<li>Try adding <strong>polynomial</strong> features  </li>
<li>Try <strong>increasing/decreasing</strong> $\lambda$, regularization parameter  </li>
</ul>
<p>However, which one above to choose to try? Randomly will waste your time after 6 months of your effort.  </p>
<h4 id="Machine_learning_diagnostic:">Machine learning diagnostic:</h4><ul>
<li>a test on what is <strong>NOT</strong> working in the current learning algorithm  </li>
<li>provides guidence to imporve its performance  </li>
<li>Diagnostics <strong>can take time</strong> to implement, <strong>but</strong> doing so can be <strong>worth</strong> use of your time.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/diagnoise-benefit.png" alt=""></p>
<h3 id="Evaluating_a_Hypothesis"><center> Evaluating a Hypothesis </center></h3><ul>
<li>how to evalulate a hypothesis learned by your algorithm  </li>
<li>how to prevent overfiitng/underfitting  </li>
<li>evalulate a hypothesis : plotting  </li>
<li>evalulate a hypothesis : Spliting data set into training(70%) and testing(30%) set.  </li>
<li>know how to compute <strong>test-set-error</strong> and <strong>Misclassification error</strong>.   </li>
</ul>
<h4 id="Evaluating_your_hypothesis_(Ploting)">Evaluating your hypothesis (Ploting)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-training-set-polting.png" alt="">  </p>
<p>plotting hypothesis works well for small number of features, however it is impossible to plot for lots of features.  </p>
<h4 id="Evaluating_your_hypothesis_(Spliting_your_data_set)">Evaluating your hypothesis (Spliting your data set)</h4><ul>
<li>70% for training set  </li>
<li>30% for test set  </li>
<li><strong>randomly sort</strong> the datasets before data spliting  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-split-data.png" alt="">  </p>
<p><strong>Training/testing procedure</strong>  </p>
<ul>
<li>learning parameter $\theta$ from <strong>traning data</strong>.  </li>
<li>compute <strong>test set error</strong>. (average-square-error, same equation to compute the cost function, we use test-set instead of training set)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/test-set-error-compute.png" alt="">  </p>
<p>For <strong>logistic regression</strong>, beside computing <strong>test set error</strong>, we can also try to compute <strong>misclassification error</strong>( a.k.a. 0/1 misclassification error)  </p>
<h4 id="Misclassification_error_(a-k-a-_0/1_misclassification_error)">Misclassification error (a.k.a. 0/1 misclassification error)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/misclassification-error.png" alt="">  </p>
<h3 id="Model_Selection_and_Train/Validation/Test_Sets"><center>Model Selection and Train/Validation/Test Sets</center></h3><ul>
<li>decide degree of polynomical  </li>
<li>what feature to include for the learning algoirthm   </li>
<li>model selection problem  </li>
<li>split data set into: train(60%), validation(20%), and test set(20%).  </li>
</ul>
<h4 id="Overfitting_problem">Overfitting problem</h4><ul>
<li>hypothesis function perfectly fit training data does not mean it will predicting the un-seen data well (not able to generalization)  </li>
</ul>
<h4 id="Model_selection">Model selection</h4><ul>
<li>degree of polynomial (d ) </li>
<li>linear, qudratic, cubic function  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/model-selection.png" alt="">  </p>
<h4 id="Evaluating_your_hypothesis">Evaluating your hypothesis</h4><ul>
<li>we split data sets into <strong>three</strong> sections to better helping to select the best model.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/split-three-section.png" alt=""></p>
<ul>
<li>training set (60%)  </li>
<li>cross validation set - CV(20%)  </li>
<li>test set (20%)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/three-part-model-selection.png" alt="">  </p>
<p>Now, we are able to use <strong>training set</strong> to find $Θ$ for each polynomial degree, we then use <strong>cross validation set</strong> to select the lowest  $J_{cv}$  cost. We finally use <strong>test set</strong> to estimate generalization error as $J_{test}$. </p>
<h1 id="Week_6:_Lesson_2">Week 6: Lesson 2</h1><h2 id="Bias_vs-_Variance">Bias vs. Variance</h2><h3 id="Diagnosing_Bias_vs-_Variance"><center> Diagnosing Bias vs. Variance </center></h3><ul>
<li>identify underfiting/overfiting  </li>
<li>evaluate problem of bias/variance issues  </li>
<li><strong>bias</strong>: high  $J_{train}(\theta)$ and $ J_{cv}(\theta)$  </li>
<li><strong>variance</strong>: low $J_{train}(\theta)$ high $ J_{cv}(\theta)$ </li>
</ul>
<h4 id="Bias/Variance">Bias/Variance</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/bias-variance.png" alt=""></p>
<p>Bias - underfit<br>variance - overfit  </p>
<h4 id="Diagnosing_bias_vs-_variance">Diagnosing bias vs. variance</h4><p>Suppose your learning algorithm is not as well as you have perdict, how can we know if it is a bias problem or a variance problem?  </p>
<p>First, we need to plot
 $J_{cv}(\theta)$  (cross-validation error) and $J_{train} (\theta)$ (training error) in a error vs degree of polynomial d.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/error-degree-polynomial.png" alt="">  </p>
<p><strong>Bias(underfit):</strong>  </p>
<ul>
<li><strong>both</strong>  $J_{train}(\theta)$ and $J_{cv}(\theta)$  will be <strong>high</strong>  </li>
<li> $J_{train}(\theta) \approx J_{cv}(\theta)$  
</li>
</ul>
<p><strong>Variance(overfit):</strong>  </p>
<ul>
<li>$J_{train}(\theta)$ will be <strong>low</strong>.  </li>
<li> $J_{train}(\theta) \gg J_{cv}(\theta)$ 
</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/high-varience-bias.png" alt=""></p>
<h3 id="Regularization_and_Bias/Variance"><center> Regularization and Bias/Variance </center></h3><ul>
<li>how regularization affect bias and variance  </li>
<li>how to automatically find $\lambda$ value: the regularzation parameter  </li>
<li>recall the procedures in utilizing $J_{cv}(\theta)$ to pick the most optimized $\lambda$ value.  </li>
</ul>
<h4 id="Linear_Regression_with_Regularization">Linear Regression with Regularization</h4><p>$\lambda$ = is the regularization parameter. A large value of $\lambda$ will heavily penalize all the $\theta$ values, which result in high bias(underfit). On the other hand, a too small $\lambda$ value will result in small effect by the regularization terms, which will result in high varience (overfit).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/regularzation-parameter.png" alt=""></p>
<p>How to automatically select best regularzation parameter $\lambda$ for the regularization term.  </p>
<h4 id="Choosing_the_regularization_parameter_$\lambda$">Choosing the regularization parameter $\lambda$</h4><p>Steps and procedures to automatically choose the best $\lambda$ value.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/choose-best-lambda.png" alt=""></p>
<h4 id="Bias/variance_as_a_function_of_the_regularization_parameter_$\lambda$-">Bias/variance as a function of the regularization parameter $\lambda$.</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/lambda-vs-cost-function.png" alt="">  </p>
<blockquote>
<p><strong>Note</strong>:<br> We first use $J_{(\theta)}$ with the regularzaiton term to find the $minJ(\theta)$ which will give us the most optimal $Θ$ values base on the setting of degree of polynomial and regularization value $\lambda$. Even though the calculation of $J_{train}(\theta)$ is not directly assoicate with the looking for the best $\lambda$ value for the regularization term. The best $\lambda$ value directly associate with where smallest value of $J_{cv}(\theta)$ appear. However, the calcuation of $J_{train}(\theta)$, and $J_{cv}(\theta)$ together will result in a nice graph above. <br>With the present of graph above, we will be able to <strong>graphically determine</strong> whether bias/variance issues remain. </p>
</blockquote>
<h3 id="Learning_Curves"><center> Learning Curves </center></h3><ul>
<li>Learning Curves helps to diagnoise if a learning algorthm is suffering from bias/variance  </li>
<li>A learning curves make up of  $J_{train}(\theta)$ and $J_{cv}(\theta)$   </li>
<li>Poltting the learning curve will usually tells you if the learning algorithm is suffering from bias or variance.  </li>
</ul>
<h4 id="Learning_curves">Learning curves</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/learning-curve-def.png" alt=""></p>
<p>Lerning curve consisit of following:   </p>
<ul>
<li>$J_{train}(\theta)$ vs $m$  </li>
<li>$J_{cv}(\theta)$ vs $m$   </li>
</ul>
<p>where $m$ represents traning set size.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-bias.png" alt=""></p>
<p>If suffering from <strong>high bias</strong>, getting more training examples doesn’t help much. (change of model/degree of polynomial)  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-variance.png" alt=""></p>
<p>If suffering from <strong>high variance</strong> problem, getting more traning examples is likely to help. ( $J_{train}(\theta)$ is low, as more getting more data, $J_{cv}(\theta)$ is likely to approaching $J_{train}(\theta)$ )</p>
<h3 id="Deciding_What_to_Do_Next_Revisited"><center>Deciding What to Do Next Revisited</center></h3><ul>
<li>what we can do with what we have learned so far from week 6.  </li>
</ul>
<h4 id="Debugging_a_learning_algorithm:">Debugging a learning algorithm:</h4><p>Previously, we talked about few things we could possible try to improve our learning algorithm when the the hypothesis function did not give us a accurate prediction for un-seen data points.  </p>
<p>Now we everything we learned above, we could easily nail down half the unrelated options base on our analysis.  </p>
<ul>
<li>get <strong>more</strong> training examples (fix high variance)  </li>
<li><strong>smaller</strong> sets of features (fix high variance)  </li>
<li>Try getting <strong>additional</strong> features (fix high bias: function is too simple -&gt; need more complex function to better fitting the hypothesis function)</li>
<li>Try adding <strong>polynomial</strong> features (fix high bias) </li>
<li>Try <strong>increasing</strong> $\lambda$ (fix high variance)</li>
<li>Try <strong>decreasing</strong> $\lambda$ (fix high bias)</li>
</ul>
<h4 id="Neural_Network_and_Overfitting">Neural Network and Overfitting</h4><ul>
<li>take what we learned and <strong>relate</strong> back to <strong>neural netowrk</strong>.  </li>
<li>small neural network (underfitting) vs. large neural network(overfitting)  </li>
</ul>
<p><strong>Small neural network:</strong>  </p>
<ul>
<li><strong>fewer</strong> parameters  </li>
<li>more prone to <strong>underfitting</strong>  </li>
<li>computationally cheaper  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/small-neural-network.png" alt=""></p>
<p><strong>Large neural network:</strong>  </p>
<ul>
<li>large: either more activation units per layer or more layers.  </li>
<li><strong>more</strong> parameters  </li>
<li>more prone to <strong>overfitting</strong> (solve by regularization term($\lambda$))  </li>
<li>computationally more <strong>expensive</strong>.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/large-neural-network.png" alt=""></p>
<p> Larger network + regularzaiton($\lambda$) $\gg$ small neural network  </p>
<h4 id="How_to_decide_how_many_layers_for_hidden_layer?">How to decide how many layers for hidden layer?</h4><p>We could use the similiar techniques we used to determine the best $\lambda$ value.  </p>
<p>We split the data into three different sections and use the cross-validation technique with 1-layer, 2-layer, 3-layer and choose the layer with minimum $J_{cv}(Θ)$.  </p>
<h4 id="Exercise:">Exercise:</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/exercise.png" alt=""><br> $J_{cv}(Θ)$ $\gg$ $J_{train}(Θ)$ , we know that the learning algorithm is suffering from high variance (overfitting). Because of that, increasing the number of hidden layer (increasing number of $\theta$ parameter) will possiblly not doing any benefit to the learning algorithm.  </p>
<p>I would suggest <strong>decreasing</strong> number of hidden layer units OR getting <strong>more</strong> training examples OR adding <strong>regularization</strong> terms to imporve the learning algorithm.  </p>
<h1 id="Week_6:_Lesson_4">Week 6: Lesson 4</h1><h2 id="Building_a_Spam_Classifier">Building a Spam Classifier</h2><h3 id="Prioritizing_What_to_Work_On"><center> Prioritizing What to Work On </center></h3><ul>
<li>prioritize what you need to work on to save your time.  </li>
</ul>
<h4 id="Building_a_spam_classifier">Building a spam classifier</h4><ul>
<li>how to determine the $x=$ feature of email.  </li>
</ul>
<p>Maybe feature $x$: choose 100 words indicative of spam/not spam.  </p>
<p><code>E.g: deal, buy, discount, andrew, now...</code>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/spam-classifier.png" alt=""></p>
<blockquote>
<p>Note: in practice, take most frequently occurring $n$ words (10,00 to 50,000) in training set, rather than manually pick 100 words.  </p>
</blockquote>
<h4 id="Building_a_spam_classifier-1">Building a spam classifier</h4><p>How to spend your time to make it have low error?  </p>
<ul>
<li>collect lots of data (“honeypot” project.)  </li>
<li>better features based on email routing information (from <strong>email header</strong>)  </li>
<li>better features for message body (deal, Dealers? features about punctuation)  </li>
<li>Develope sophisticated algorithm to detect misspellings (ex. w4thches, mad1cine, m0rtgage.)  </li>
</ul>
<h3 id="Error_Analysis"><center> Error Analysis </center></h3><ul>
<li>a way to make decision more <strong>systematically</strong>  </li>
<li><strong>start</strong> with <strong>simple implementation</strong> and start testing the result  </li>
<li>perform <strong>error analysis</strong> on result data.  </li>
<li><strong>numerical evaluation</strong> to decide whether it is good or not about the changes you made to your algorithm  </li>
</ul>
<h4 id="Recommended_approach">Recommended approach</h4><ul>
<li>start with a <strong>simple algorithm</strong> that you can implement quickly.  </li>
<li>implement it and test on <strong>cv data</strong> set.  </li>
<li>plot <strong>learning curves</strong> to decide if more data, more features, etc. are likely to help.  </li>
<li>Error analyisis: <strong>manually</strong> spot <strong>systemtically pattern</strong> of mis-classifying email.  </li>
</ul>
<h4 id="Example_of_Error_Analysis">Example of Error Analysis</h4><p>$m_{cv}$ = 500 examples in cross validation set.  </p>
<p>Algorithm misclassifies 100 emails.<br>Manually examine the 100 errors, and categorize them based on:<br>(i) what <strong>type</strong> of email it is.<br>(ii) what <strong>cues (features)</strong> you think would have helped the algorithm classify them correctly.  </p>
<p>In the 100 misclassified emails, we collected following types:  </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Pharma: <span class="number">12</span>  </span><br><span class="line">Replica/fake: <span class="number">4</span>  </span><br><span class="line">Steal Password: <span class="number">53</span>  </span><br><span class="line">Other: <span class="number">31</span></span><br></pre></td></tr></table></figure>
<p>In this case, we will possible put more efforts on spending more time in analyzing type of <code>steal password</code> email to improve the algorithm.  </p>
<h4 id="The_important_of_numerical_evaluation">The important of numerical evaluation</h4><p><code>Stemming</code>: (software: “Porter Stemmer”)  </p>
<p>Idea of stemming: treating words with same <code>stem</code> with same meaning. (discount/discounts/discounted/discounting)  </p>
<p>Disadvantage: even same <code>stem</code>, however, the meaning of the words might be totally different. (universe/university)  </p>
<p>The performance of the use of <code>stemming</code> is unknown unless we run <strong>numerical evaluation</strong> (e.g. <strong>cross validation error</strong>) of algorithm’s performance with and without stemming.  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">without stemming: <span class="number">5</span>% error</span><br><span class="line"></span><br><span class="line">with stemming: <span class="number">3</span>% error.</span><br><span class="line"></span><br><span class="line">stemming has better performance</span><br></pre></td></tr></table></figure>
<h1 id="Week_6:_Lesson_5">Week 6: Lesson 5</h1><h2 id="Handling_Skewed_Data">Handling Skewed Data</h2><h3 id="Error_Metrics_for_Skewed_Classes"><center> Error Metrics for Skewed Classes </center></h3><ul>
<li>definition of <strong>skew class</strong>  </li>
<li><strong>definition</strong> of recall and percission  </li>
<li><strong>equation</strong> for recall and percission  </li>
<li>how can we use recall and percission to detect ‘cheating’ classifier  </li>
</ul>
<h4 id="Skew_class">Skew class</h4><p><strong>Definition</strong>: the number of data sets is significant smaller than the data number of other class. The data sets with smaller number is called the <strong>skewed class</strong>.   </p>
<h4 id="Precision/Recall">Precision/Recall</h4><p>$y=1$ in presence of rare class that we want to detect.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/precission-recall.png" alt=""></p>
<p><strong>Precision:</strong><br>Patient who <strong>don’t have</strong> cancer, but you <strong>predicted</strong> they have cancer.  </p>
<p>$Percission = \frac{True Positive}{Predicted Positive} = \frac{TruePositive}{TruePos+FalsePos}$</p>
<p><strong>Recall:</strong><br>Patient who <strong>has</strong> cancer, you <strong>predicted</strong> they <strong>don’t have</strong> cancer.  </p>
<p>$Recall = \frac{TruePositive}{ActualPositives} = \frac{TruePositive}{TruePos+FalseNeg}$</p>
<p>A cheating classifer with always predicting <code>y=0</code>(always not cancer) or <code>y=1</code> (always cancer) will not have high recall and high percission <strong>at the same time</strong>.</p>
<p>A classifer with always predicting <code>y=1</code> all the time will have a <strong>high</strong> <strong>recall</strong> (recall = 1) -&gt; lower percision.  </p>
<p>A classifer with always predicting <code>y=0</code> all the time will have a <strong>high</strong> <strong>percision</strong> (percision = 1) -&gt; lower recall.</p>
<h3 id="Trading_Off_Precision_and_Recall"><center>Trading Off Precision and Recall </center></h3><ul>
<li>how do we evaluate the trade-off between precision and recall?  </li>
<li>how to use F-score to find the best trade-off between precision and recall parameter  </li>
<li>automatically set threshold with F-score. </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/percission-recall.png" alt=""></p>
<p>How to automatically pick the <strong>threshold</strong> above.  </p>
<p><strong>Single</strong> matrix comparison is easy (pick the <strong>max</strong> or <strong>min</strong> of the parameter)</p>
<p>However, in precision and recall matrix, we have <strong>two</strong> variables (precission(P) &amp; recall(R)). </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/compare-percission-recall.png" alt=""></p>
<p>Potential solutions: </p>
<h4 id="Determine_good_recall/variance:_Average:">Determine good recall/variance: Average:</h4><p>$$Average=\frac{P+R}{2}$$  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/recall-percission-average.png" alt="">  </p>
<p>Not good, since algorithm 3: y=1 all the time has highest average. Not a good algorithm to evaluate the values of percision/recall numbers.  </p>
<h4 id="Determine_good_recall/variance:_F_Score">Determine good recall/variance: F Score</h4><p>$$F_{score} = 2 \frac{PR}{P+R}$$ </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/F-SCORE.png" alt=""></p>
<h4 id="automatcially_determine_the_threshold_for_classifier:_$h(x)$_function">automatcially determine the threshold for classifier: $h(x)$ function</h4><p>try out few different thresholds and test on cv data set, and evaluate the highest F-score. </p>
<h3 id="Data_For_Machine_Learning"><center> Data For Machine Learning </center></h3><ul>
<li>Understanding of how large data set help improve learning algorithm.  </li>
<li>What is the userful test for the assumption for the idea of large data win.  </li>
</ul>
<blockquote>
<p>It’s not who has the best algorithm that wins. It’s who has the most data.  - Andrew Ng.  </p>
</blockquote>
<p>All the idea about enhancing your learning algorithm with very large amount of data is based on the assumption below: </p>
<p><strong>Assumption:</strong>  </p>
<blockquote>
<p>Assume feature $x\in R^{n+1}$ has <strong>sufficient</strong> information to predict $y$ correctly</p>
</blockquote>
<p>How can we determine whether or not the feature provide is <strong>sufficient</strong>?  </p>
<p>There is a simple test: Given the input $x$, can a human expert confidently predict $y$?  </p>
<h4 id="Userful_test">Userful test</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/large-amout-data.png" alt=""></p>
<p>Given the sentence above, an expert in English speaking will be easily fill in the sentence with word ‘two’ (not ‘to’ or ‘too’) base on the information (features) given in the sentence.  </p>
<p>However, an counter example of given only the size of a house (without the location, age), even an estitate expert will not be able to predict a fair price (y) for the house.  </p>
<h4 id="Improve_performance_with_large_data_set">Improve performance with large data set</h4><p>Having a large training set can significantly improve the learning algorithm with the <strong>assumption</strong> of “The features has enough information to predict y accurately” held on.  </p>
<p>Using a learning algorithm with <strong>many parameters</strong> will still be fine when we have a large set of training data. $J_{train}(\theta)$  will be small. And since we are using a <strong>very large</strong> training set, it is <strong>unlikely</strong> to overfit. Therefore, $J_{train(\theta)} \approx J_{test}(\theta)$, which means $J_{test}(\theta)$ will likely be small.  </p>
<h1 id="Week_7:_Lesson_1">Week 7: Lesson 1</h1><h2 id="Large_Margin_Classification">Large Margin Classification</h2><h3 id="Optimization_Objective"><center> Optimization Objective </center></h3><ul>
<li>introduction of SVM  </li>
<li>more powerful way to learn complex non-linear function  </li>
<li>supervised learning algorithm: linear regression, neural network, SVM.  </li>
</ul>
<h4 id="Alternative_logistic_regression_to_SVM">Alternative logistic regression to SVM</h4><p><strong>Recall for logistic regression:</strong>  </p>

Hypothesis function: $h_{\theta}(x) = \frac{1}{1+e^{-\theta T x}}$

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/hypothesis.png" alt=""></p>
<p><strong>Cost Function:</strong> </p>

$Cost(h_\theta(x),y) = 
 -y\;log(h_\theta(x)) 
 -(1-y	)log(1-h_\theta(x))$

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y1.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160701/cost-function-y0.png" alt=""></p>
<p><strong>1. Change Cost function for SVM:</strong>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-costfunction.png" alt="">  </p>
<p>Pink line denotes new cost function for SVM. $Cost_1(z)$ is cost fucntion when $y=1$. $Cost_0(z)$ is cost fucntion when $y=0$. </p>
<p><strong>2. Change constant parameter for SVM:</strong>  </p>
<p>$1/m$ and $\frac{\lambda}{2m}$ terms goes away by multipling equation by $m$, which will not affect the optimzation process. Later we further multiply equation with $\frac{1}{\lambda}$ which result in following.</p>
<h4 id="SVM_cost_function">SVM cost function</h4>
$$J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$$

<p>As we increase $C$, we regularize <strong>less</strong> (less weight on regularzation term - reduce overfitting). If we decrease $C$, we regularize <strong>more</strong> (more weight on regularzation term - reduce underfitting).</p>
<h4 id="SVM_hypothesis_function">SVM hypothesis function</h4><p>The hypothesis of the SVN is not interpreted as the probability of $y$ being 1 or 0 (as it is for the hypothesis of logistic regression).  </p>

$$
h_\theta(x) = 
\begin{cases}
    1 & \text{if} \ \Theta^Tx \geq 0 \\
    0 & \text{otherwise}
\end{cases}
$$

<h3 id="Large_Margin_Intuition"><center>Large Margin Intuition</center></h3><ul>
<li>introduction for large margin classifier  </li>
<li>value $C$’s effect on position of the classifier  </li>
<li>$C = \frac{1}{\lambda}$ relationship   </li>
</ul>
<h4 id="Support_Vector_Machine">Support Vector Machine</h4><p>The cost function for SVM looks like following:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y0.png" alt=""><br>If $y=0$, we want $\theta^Tx \leq -1$ (not just $\leq 0$).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-y1.png" alt=""><br>If $y=1$, we want $\theta^Tx \geq 1$ (not just $\geq 0$).  </p>
<p>SVM create a better <strong>precision</strong> by setting $\theta^Tx \leq -1$ and $\theta^Tx \geq 1$  </p>
<h4 id="Large_Margin_Classifier">Large Margin Classifier</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/large-margin-classifier.png" alt=""></p>
<p>Instead of learning the green/pink classifer, SVM learns a more robust classifer (with more margin to either of the <strong>blue</strong> line), which marked as the <strong>black</strong> line.  </p>
<h4 id="SVM_constant_value_C">SVM constant value C</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/overfitting-underfitting-svm.png" alt="">  </p>
<p>In logistic regression, large $\lambda$ result in underfitting, and small $\lambda$ result in overfitting. </p>
<p>Similarly, in SVM, large $C$ (small $\lambda$) value results in overfitting, and small $C$ (large $\lambda$) value results in underfitting.  </p>
<p>$$C = \frac{1}{\lambda}$$  </p>
<h3 id="Mathematics_Behind_Large_Margin_Classification"><center> Mathematics Behind Large Margin Classification </center></h3><ul>
<li>why/how svm large margin classifier ???</li>
</ul>
<h4 id="Vector_Inner_Product">Vector Inner Product</h4><p>why $\theta$ line is vertical to classifier line.  </p>
<p>SVM decision boundary has to satisfy with following two conditions:  </p>

$\begin{align*}
&min_\Theta \dfrac{1}{2}\sum_{j=1}^n \Theta_j^2
\end{align*}$

<p>for minimum of $Θ$ values</p>
<p>and  </p>
<p>So we now have a new optimization objective by substituting $p^{(i)} \cdot ||\Theta ||$ in for $\Theta^Tx^{(i)}$:<br>If y=1, we want $p^{(i)} \cdot ||\Theta || \geq 1$<br>If y=0, we want $p^{(i)} \cdot ||\Theta || \leq -1$  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-bad-classifier.png" alt=""></p>

We want $p^{(i)} \cdot ||\Theta || \geq 1$, if $p^{(i)}$ is small(distance of `x` or `o` to blue line), $||\Theta ||$ will be large (which is NOT good, we want to minimize $\theta$ values).  

<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/svm-good-classifier.png" alt=""></p>

We want $p^{(i)} \cdot ||\Theta || \geq 1$, if $p^{(i)}$ is large, $||\Theta ||$ could be small (which is good, we want to minimize $\theta$ values).  

<h1 id="Week_7:_Lesson_2">Week 7: Lesson 2</h1><h2 id="Kernels">Kernels</h2><h3 id="Kernels_I"><center> Kernels I</center></h3><ul>
<li>Introduction to new form for hypothesis function for SVM  </li>
<li>Introduction to Gaussian Kernels (similarity function)  </li>
<li>properity of Gaussian Kernels  </li>
<li>Effect of sigma value in Gaussian Kernels  </li>
</ul>
<h4 id="Hypothesis_function_for_SVM">Hypothesis function for SVM</h4><p>Instead of using the hypothesis function in form below (for <strong>high level polynomial</strong> logistic regression):  </p>

$$\begin{align*}
h_\Theta(x) = \Theta_1x_1 + \Theta_2x_2 + \Theta_3x_1x_2 + \Theta_4x_1^2 + \dots
\end{align*}$$

<p>We use the following hypothesis function form for SVM:  </p>

$$\begin{align*}
h_\Theta(x) = \Theta_1f_1 + \Theta_2f_2 + \Theta_3f_3 + \dots
\end{align*}$$

<p>Where:  </p>

$$\begin{align*}
f_1 = x_1 \newline
f_2 = x_2 \newline
f_3 = x_1x_2 \newline
f_4 = x_1^2 \newline
\dots \newline
\end{align*}$$


<h4 id="Gaussian_Kernels:_The_similarity_function">Gaussian Kernels: The similarity function</h4><p>The similarity of data point $x$ and some landmark $l$ define as:  </p>

$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2})$$

<h4 id="Properties_of_the_similarity_function">Properties of the similarity function</h4><p>If $x \approx l^{(i)}$, then $f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1$  </p>
<p>If $x$ is far from $l^{(i)}$, then $f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0$  </p>
<p>In other word, if data point $x$ is close to the landmark $i$, then the value of $f_i$ will close to $1$. Other wise, if data point $x$ is far from the landmark $i$, then the value of $f_i$ will close to $0$.</p>
<p><strong>Example:</strong>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/similarity-function-example.png" alt=""></p>
<h4 id="Effect_of_sigma_value">Effect of sigma value</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160731/sigma-value.png" alt=""></p>
<p>If $\sigma$ value increase, the shape will become wider. If $\sigma$ value decrease, the shape will become narrower.</p>
<h3 id="Kernels_II"><center> Kernels II</center></h3><ul>
<li>where to get landmark $l^{(1)}, l^{(2)}, l^{(3)},..?$</li>
<li>how SVM with kernels can learn complex non-linear function.  </li>
<li>how to get parameter $\Theta$  </li>
<li>SVM parameters $C$ and $\sigma^2$  </li>
</ul>
<h4 id="Choosing_the_landmarks">Choosing the landmarks</h4><p>We can choose landmarks $l_i$ from exact same location as the training data $x_i$.  </p>
<p>Given training data $x$: </p>

$$f_1 = similarity(x,l^{(1)}), f_2 = similarity(x,l^{(2)}), f_3 = similarity(x,l^3) \dots$$

<p>Each training data $x^{(i)}$ has $m$ features (total of $m$ training datas), which denotes as a “feature vector” $f^{(i)}$. We may also have $f_0=1$ to correspond with $\Theta_0$. </p>

$$x^{(i)} \rightarrow \begin{bmatrix}
f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \newline
f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \newline
\dots \newline
f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \newline
\end{bmatrix}$$


<h4 id="Choosing_the_parameters_$\Theta$">Choosing the parameters $\Theta$</h4><p>we can use the SVM minimization algorithm but with $f(i)$ substituted in for $x(i)$:</p>

$\large
min_\Theta C \sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tf^{(i)}) + (1 - y^{(i)})\text{cost}_0(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$

<blockquote>
<p><strong>Note</strong>: Kernals could also use with logistic regression but it works much faster with SVM because of computational optimizations on SVMs. </p>
</blockquote>
<h4 id="SVM_parameters">SVM parameters</h4><p><strong>Choosing $C$ ($C=\frac{1}{\lambda}$)</strong>:  </p>
<p>If $C$ is large: higher variance/lower bias/overfitting<br>If $C$ is small: lower variance/higher bias/underfitting  </p>
<p><strong>Choosing $\sigma^2$</strong></p>
<p>With a large $\sigma^2$: the features $f_i$ vary <strong>more</strong> smoothly, higher bias/lower variance/underfitting<br>With a small $\sigma^2$: the features $f_i$ vary <strong>less</strong> smoothly, lower bias/higher variance/overfitting  </p>
<h1 id="Week_7:_Lesson_3">Week 7: Lesson 3</h1><h2 id="SVMs_in_Practice">SVMs in Practice</h2><h3 id="Using_An_SVM"><center>Using An SVM</center></h3><ul>
<li>not recommand writing own implementation for solving $\Theta$ value, use the SVM software package (<code>liblinear</code>, <code>libsvm</code>) instead.  </li>
</ul>
<h4 id="Kernel_(similarity)_functions:">Kernel (similarity) functions:</h4><p>$$f = exp(-\frac{\Vert x1 - x2 \Vert ^2}{2\lambda})$$</p>
<h4 id="Implementation_choice_for_SVM">Implementation choice for SVM</h4><ul>
<li>Choice of parameter $C$.  </li>
<li>Choice of kernel $\sigma^2$(similarity function).  </li>
<li>Linear kernel (No Kernel) - (<code>n</code> large, <code>m</code> small)</li>
<li>Complex non-linear kernel (Gaussian Kernel) (<code>n</code> small, <code>m</code> large)  </li>
</ul>
<p>Why # of feature n = # of training example m??? </p>
<h4 id="Multi-class_classification">Multi-class classification</h4><ol>
<li><p>SVM package already build-in multi-class classification</p>
</li>
<li><p>use one-vs-all method</p>
</li>
</ol>
<h4 id="Logistic_regression_vs-_SVMs">Logistic regression vs. SVMs</h4><p><code>n</code> = number of features.<br><code>m</code> = number of training examples.  </p>
<p>quiz question???<br>TODO: notes for last lecture</p>
<h1 id="Week_8:_Lesson_1">Week 8: Lesson 1</h1><h2 id="Clustering">Clustering</h2><h3 id="Unsupervised_Learning:_Introduction"><center>Unsupervised Learning: Introduction</center></h3><ul>
<li>learned from un-labeled data  </li>
<li>in unsupervised learning, training set is of the form ${x^{(1)},x^{(2)},x^{(3)}, \dots x^{(m)}, }$ without labels $y^{(i)}$  </li>
</ul>
<h4 id="First_unsupervised_learning_algorithm:_clustering">First unsupervised learning algorithm: clustering</h4><p>give un-labelled training sets to learn coherent structure/cluster/subset.  </p>
<h4 id="Application_of_Clustering">Application of Clustering</h4><ul>
<li>market segementation  </li>
<li>social netowrk analysis: clustering algorithm to find conherent groups of friends   </li>
<li>organize computing clusters: how to design data center efficiently  </li>
<li>astronomical data analysis  </li>
</ul>
<h3 id="K-means_algorithm"><center>K-means algorithm </center></h3><ul>
<li>K-means : most popluar clustering algorithm   </li>
<li>k-means algorithm sudo code    </li>
<li>K-means algorithm for non-well-separated data   </li>
</ul>
<h4 id="K-means_introudction:">K-means introudction:</h4><p><strong>Step 1:</strong> initialize $K$ cluster centroids (ex. K=2 clusters -&gt; 2 points [red/blue])  </p>
<p><strong>Step 2:</strong> cluster assignments: loop through all the training sets data (1 to $m$) and mark data sets to index of the closer cluster centroids. In $K = 2$, mark as red (1) if it is closer to red cluster centroid, mark as blue (2), if closer to blue cluster centroid.<br><strong>Step 3:</strong> move the <strong>cluster centroids</strong> by doing mean of locations of all red/blue <strong>data points</strong> seperately<br><strong>Step 4:</strong> repeat step 2-3 to update cluster assignments based on current updated cluster centroids’ location until it saturtated.  </p>
<h4 id="K-means_algorithm_input:">K-means algorithm input:</h4><p>Input:  </p>
<ul>
<li>$K$ (number of clusters)  </li>
<li>Training set ${x^{(1)},x^{(2)},x^{(3)}, \dots x^{(m)}}$  </li>
</ul>
<p>$x^{(i)} \in R^n$ where $n$ represents number of features (drop $x_0 = 1$ convention)</p>
<h4 id="K-means_algorithm:">K-means algorithm:</h4>

Randomly initialize $K$ cluster centroids $\mu_1,\mu_1,\mu_1, \dots \mu_K \in R^n [vector \ \mu]$  
Repeat {  
	for $i = 1$ to $m$: [data points cluster/color  assignemnt]  
		$c^{(i)}$ [vector] = index (from 1 to $K$) of cluster centroid closes to $x^{(i)}$  
	for $k=1$ to $K$: [move centroid]  
		$\mu_k$ = average (mean) of points, update $K$ centroids' positions   
}  


<h4 id="K-means_for_non-well-separated_clusters">K-means for non-well-separated clusters</h4><p>The following example shows how K-means algorithm separate non-well-separated clusters:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-mean-non-separate.png" alt="">  </p>
<p>After running K-mean on the data set above, the algorithm will automatically cluster the data points into three clusters.  </p>
<h3 id="Optimization_Objective-1"><center> Optimization Objective </center></h3><ul>
<li>cost function : optimization objective  </li>
<li>notation for  $c^{(i)},$ $x^{(i)},$ $\mu_k $, $\mu_{c^{(i)}}$ </li>
</ul>
<h4 id="K-means_optimization_objective">K-means optimization objective</h4><p>$c^{(i)} = $ index of cluster ($1,2, \dots K$) to which data point $x^{(i)}$ is currently assigned.<br> $\mu_k = $ location of cluster centroid $k \ (\mu_k \in R^n)$  <br>$\mu_{c^{(i)}} = $ location of <strong>cluster centroid</strong> of cluster to which example data $x^{(i)}$ has been assigned.  </p>
<p>Optimization objective:<br> $$J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots \mu_K) = \frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} - \mu_{c^{(i)}} \Vert^2$$ </p>
<p>$$min \ J(cost \ function)  $$ -&gt; distortion k-mean cost function </p>
<h3 id="Random_Initialization"><center> Random Initialization </center></h3><ul>
<li>how to random initialization cluster centroids  </li>
<li>how to avoid local minimum  </li>
<li>multiple random initialization algorithm  </li>
</ul>
<h4 id="Random_initialization">Random initialization</h4><ul>
<li>should have $K &lt; m$  </li>
<li><strong>randomly</strong> pick $K$ training examples  </li>
<li><strong>set</strong> $\mu_1, \dots, \mu_K$ equal to these $K$ examples  </li>
</ul>
<blockquote>
<p><strong>Notes</strong>: Depends on location of random initialization, K-means algorithm can actually end up in different solution.  </p>
</blockquote>
<h4 id="Avoid_local_optima">Avoid local optima</h4><p>Example training set:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-dataset.png" alt=""></p>
<p>Global minimum (Good):<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal-global-cluster.png" alt=""></p>
<p>Local optima (Not good):<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/local-optimal.png" alt=""></p>
<p>Solution: Try <strong>multiple random initialization</strong> ($50-1000$ times) to get global optima.  </p>
<h4 id="Multiple_Random_initialization_algorithm">Multiple Random initialization algorithm</h4><p>For $i=1 \ to \ 100 $ {  </p>
<ol>
<li>Randomly initialize K-means  </li>
<li>Run K-means. Get $c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k$  </li>
<li>Compute cost function (distortion) $J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k)$  </li>
</ol>
<p>}  </p>
<p>Pick clustering htat gave <strong>lowest</strong> cost $J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k)$  </p>
<blockquote>
<p><strong>Note</strong>: multiple random initialization is especially helpful for small number of cluster ($K = 2-&gt;10$). With a large $K$, multiple random initialization is not necessary.  </p>
</blockquote>
<h3 id="Choosing_the_Number_of_Clusters"><center> Choosing the Number of Clusters </center></h3><ul>
<li>how to choose the number of clusters $K$  </li>
</ul>
<h4 id="Elbow_method">Elbow method</h4><p>Graph Value of the cost function verus the no. of the cluster $K$. The location where “elbow” appear is the location of the best $K$ value.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/k-elbow-method.png" alt="">  </p>
<p>However, elbow method does not always works well:  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/elbow-method-bad.png" alt=""></p>
<p>Choose by hand <strong>manually</strong> (tshirt size: xs, s, m, l, xl OR s, m, l) is still the best approach for choosing $K$.  </p>
<h1 id="Week_8:_Lesson_3">Week 8: Lesson 3</h1><h2 id="Motivation">Motivation</h2><h3 id="Motivation_I:_Data_Compression"><center> Motivation I: Data Compression </center></h3><ul>
<li>compressed and reduce dimension of redundant data -&gt; less computer memory, disk space  </li>
<li>speed up the learning algorithm  </li>
</ul>
<h4 id="Data_Compression_From_2D_to_1D">Data Compression From 2D to 1D</h4><ul>
<li>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will help to reduce the memory and disk usage.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/data-compression.png" alt="">  </p>
<blockquote>
<p><strong>Note</strong>: in dimensionality reduction, variable $m$ will stay the same size; $n$ the number of features each example carries, will be reduced.  </p>
</blockquote>
<h3 id="Motivation_II:_Data_Visualization"><center> Motivation II: Data Visualization </center></h3><ul>
<li>better understand our data with data visualization.  </li>
<li>with large number of features, $n=50$ for example, we could select two or three features, which can effectivly summarize all other features, and plot it in 2D or 3D.  </li>
</ul>
<h1 id="Week_8:_Lesson_4">Week 8: Lesson 4</h1><h2 id="Principal_Component_Analysis">Principal Component Analysis</h2><h3 id="Principal_Component_Analysis_Problem_Formulation"><center>Principal Component Analysis Problem Formulation</center></h3><ul>
<li>PCA introduction: an compression algorithm  </li>
<li>PCA definition  </li>
<li>PCA terms: direction vector (+, - direction all work)  </li>
<li>PCA vs. Linear Regression  </li>
</ul>
<h4 id="Principal_Component_Analysis_(PCA)_problem_formulation">Principal Component Analysis (PCA) problem formulation</h4><p><strong>Reduce from 2D to 1D:</strong> Find a direction vector, which can be used to project data onto it so as to <strong>minimize</strong> the projection error.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/pca.png" alt=""></p>
<p>PCA try to find a <strong>lower dimensional surface</strong> (3D to 2D) to project the data which minimize the square distance between each points and lower the projection error.  </p>
<blockquote>
<p><strong>Note</strong>: projection error is the sum of square of blue segement above.  </p>
</blockquote>
<p><strong>Reduce from n-dimension to k-dimension:</strong> find $k$ vectors $\mu^{(1)}, \mu^{(2)},\mu^{(3)}, \dots, \mu^{(k)}$ onto which to project the data, so as to minimize the projection error.  </p>
<h4 id="PCA_vs_Linear_Regression">PCA vs Linear Regression</h4><ul>
<li>In linear regression, given $x$, predict value $y$ with the hypothesis function.  </li>
<li>In PCA, only input data are $x_1, x_2, \dots, x_m$</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/pca-vs-linear-regression.png" alt=""></p>
<ul>
<li>PCA, the sum of projection error (blue segment) is <strong>perpendicular</strong> to the direction vector.   </li>
<li>While, in linear regression, the square error (blue segment), is <strong>perpendicular</strong> to the x-axis.  </li>
</ul>
<p>Next, we will learn how to find that lower dimension surface which we could project the data.  </p>
<h3 id="Principal_Component_Analysis_Algorithm"><center>Principal Component Analysis Algorithm</center></h3><ul>
<li>PCA algorithm steps introduce  </li>
<li>Data preprocessing (feature scaling/mean normalization)  </li>
<li>PCA algorithm: reduce feature in $n$ dimension to $k$ dimension.  </li>
<li>Two task for PCA algorithm: 1. figure direction vector$u^{(1)}, \dots, u^{(k)}$, and 2. find $z_1, z_2, \dots, z_m$  </li>
<li>PCA Algorithm in three steps.   </li>
</ul>
<h4 id="Data_Preprocessing">Data Preprocessing</h4><p>Training set: $x^{(1)},x^{(2)}, \dots x^{(m)}$<br>Preprocessing (feature scaling/mean normalization):  </p>
<p><strong>Feature Scaling:</strong>  </p>
<ol>
<li><p>Mean of feature $j$: $$\mu_j=\frac{1}{m} \sum_{i=1}^m x_j^{(i)}$$</p>
</li>
<li><p>Replace each $x_j^{(i)}$ with $x_j-\mu_j$. </p>
</li>
</ol>
<p><strong>Mean normalization:</strong>  </p>
<p>$$\frac{x_j^{(i)} - \mu_j}{s_j}$$</p>
<p>scale features to have comparable range of values.  </p>
<h4 id="PCA_Algorithm_Introduction">PCA Algorithm Introduction</h4><p>Reduce feature from $n$ dimensional to $k$ dimensional as follow:  </p>
<p>$$x^{(i)} \in R^n -&gt; z^{(i)}  \in R^k $$</p>
<p>where $n$ is the number of feature, and $k$ is the number of reduced feature.  </p>
<p>PCA has following <strong>two</strong> tasks:  </p>
<ol>
<li>figure out $u^{(1)}, \dots, u^{(k)}$, where $k$ denotes the dimension of the reduced <strong>direction vector</strong>.  </li>
<li>find $z_1, z_2, \dots, z_m$, where $m$ is the number of the data points.  </li>
</ol>
<h4 id="PCA_Algorithm">PCA Algorithm</h4><h5 id="1-_Compute_“covariance_matrix”">1. Compute “covariance matrix”</h5><p>$$\large Σ=\frac{1}{m}\sum_{i=1}^m(x^{(i)})(x^{(i)})^T$$</p>
<p>Correspond Octave code:  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">Sigma = (<span class="number">1</span>/m) * X<span class="operator">'</span> * X;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: $x^{(i)}$ is $n \times 1$ vector. $(x^{(i)})^T$ is $1 \times n$ vector, and $X$ is $m \times n$ vector. Dimension of $\Sigma$ is $n \times n$.  </p>
</blockquote>
<h5 id="2-_Compute_“eigenvectors”_of_covariance_matrix_Σ">2. Compute “eigenvectors” of covariance matrix Σ</h5><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma);</span><br></pre></td></tr></table></figure>
<p><code>svd()</code> is the ‘singular value decomposition’, a built-in Octave function. We are only interested in the <code>U</code> matrix of the Sigma covariance matrix: $U \in R^{n \times n}$  </p>
<h5 id="3-_Take_the_first_k_columns_of_the_U_matrix_and_compute_z">3. Take the first k columns of the U matrix and compute z</h5><p>reduce $n$ feature dimension to $k$ feature dimension.  </p>
<p>$$z^{(i)} = Ureduce^T*x^{(i)}$$</p>
<p>$Ureduce^T$ is in $k \times n$, while $x^{(i)}$ will have $n \times 1$ dimension. And the product of $Ureduce^T*x^{(i)}$ will have dimension $k \times 1$</p>
<h4 id="Summary">Summary</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">Sigma = (<span class="number">1</span>/m) * X<span class="operator">'</span> * X;  <span class="comment">% compute the covariance matrix</span></span><br><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma);    <span class="comment">% compute our projected directions</span></span><br><span class="line">Ureduce = U(:,<span class="number">1</span>:k);      <span class="comment">% take the first k directions</span></span><br><span class="line">Z = X * Ureduce;         <span class="comment">% compute the projected data points</span></span><br></pre></td></tr></table></figure>
<h1 id="Week_8:_Lesson_5">Week 8: Lesson 5</h1><h2 id="Applying_PCA">Applying PCA</h2><h3 id="Reconstruction_from_Compressed_Representation"><center>Reconstruction from Compressed Representation</center></h3><ul>
<li>PCA compression algorithm: $n$ dimensional feature compress to $k$ dimensional feature.  </li>
<li>Introduce way to reconstruct the approximate $n$ dimensional feature from the compressed $k$ dimensional feature.  </li>
<li>how to go back from $z^{(i)}$ back to $x^{(i)}$</li>
</ul>
<h4 id="Reconstruction_from_compressed_representation">Reconstruction from compressed representation</h4><p>PCA compressed algorithm:  </p>
<p>$$z=U_{reduce}^T * x$$</p>
<p>To go from 1D back to 2D, we do $z \in R -&gt; x \in R^2$  </p>
<p>Approx. reconstruction from compressed representation:  </p>
$$x_{approx}^{(1)}=U_{reduce} * z^{(1)}$$
<h3 id="Choosing_the_Number_of_Principal_Components"><center>Choosing the Number of Principal Components</center></h3><ul>
<li>$k$ : a parameter of PCA algorithm  </li>
<li>how to choose the $k$ principle compoent  </li>
</ul>
<h4 id="Choosing_$k$_(number_of_principal_components)">Choosing $k$ (number of principal components)</h4><p>How to choose $k$, the dimension we are reducing the feature to, which is also called the <em>number of principal components</em>?  </p>
<p>One way to choose $k$ is the following formula:  </p>
<p><strong>Average squared projection error:</strong> </p>
$$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$$
<p><strong>Total variation in the data:</strong> (how far is each data point away from the origin <code>(0, 0)</code>)</p>
<p>$$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$$</p>
<p><strong>Choose the smallest $k$ that satisfy:</strong><br>
$$\large \dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$$
</p>
<p>Equation above wants the squared projection error divided by the total <strong>variation should be less than 1%</strong>, which is also called <strong>99% of the variance is retained</strong>.  </p>
<h4 id="Algorithm_for_choosing_$k$_with_loop">Algorithm for choosing $k$ with loop</h4><ol>
<li>Try PCA with $k=1,2, \dots$  </li>
<li>Compute $U_{reduce}, z, x_{approx}$  </li>
<li>check if $\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$ (99% of the variance retained)  </li>
<li>repeat step 1-3 until smallest $k$ found that satisfy 99% of the variance retained.  </li>
</ol>
<p>The algorithm above works, however, it is <strong>inefficient</strong>.  </p>
<h4 id="Algorithm_for_choosing_$k$_with_matrix_S">Algorithm for choosing $k$ with matrix S</h4><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="matrix">[U,S,V]</span> = svd(Sigma)</span><br></pre></td></tr></table></figure>
<p>It is much efficent to check for 99% of retained variance with S matrix with following:  </p>

$$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$

<p>which we only need to run <code>svd()</code> once for choosing the smallest $k$ value that retain 99% of variance.  </p>
<p>Quiz: </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-q.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160806/quiz1-a.png" alt=""></p>
<h3 id="Advice_for_Applying_PCA"><center>Advice for Applying PCA</center></h3><h4 id="Supervised_learning_speedup">Supervised learning speedup</h4><p>The most common use of PCA, an <strong>unsupervised</strong> learning algorithm, is to speed up a <strong>supervised</strong> learning algorithm.  </p>
<p>Note: PCA only on training set, but not on the cv or test sets???  </p>
<h4 id="Application_of_PCA">Application of PCA</h4><ul>
<li>Compression (reduce disk space/ memory use &amp; speed up algorithm)  </li>
<li>Visualization ($k = 2 or k = 3$)  </li>
</ul>
<h4 id="Bad_use_of_PCA:_To_prevent_overfitting">Bad use of PCA: To prevent overfitting</h4><p>Even PCA might help to reduce number of features for training set, which seems possible benefit to prevent overfitting. It might work, but it is not recommended because it does not consider the values of our result $y$. It is better to use regularization.  </p>
<h4 id="PCA_is_used_when_it_shouldn’t_be">PCA is used when it shouldn’t be</h4><p>try with original data set first before deciding to use PCA (when memory requirement not meet/disk space require too large) with strong reasoning and argument.  </p>
<h4 id="Benefit_of_PCA_compression_:">Benefit of PCA compression    :</h4><ul>
<li>compress the data so it take <strong>less</strong> disk space/ computer memory  </li>
<li>to reduce the dimension of input data so as to <strong>speed up</strong> the learning algorithm (Andrew use for this purpose most of time) </li>
<li>to visualize high-dimensional data ($k=2 or k = 3$)</li>
</ul>
<h1 id="Logs">Logs</h1><ul>
<li>07/18/2016: New post for week 6 onward.  </li>
<li>07/18/2016: Week 6 Lesson 1 completed.  </li>
<li>07/19/2016: Week 6 Lesson 2 completed.  </li>
<li>07/19/2016: Week 6 Lesson 3，4，5 completed.  </li>
<li>07/31/2016: Week 7 completed.  </li>
<li>08/05/2016: Week 8 Lesson 1, 3 completed.  </li>
<li>08/06/2016: Week 8 Lesson 4, 5 completed.  </li>
</ul>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag">#Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/07/15/Recursion-vs-Iteration/" rel="next" title="Recursion vs Iteration">
                <i class="fa fa-chevron-left"></i> Recursion vs Iteration
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/07/18/Binary-Tree-Level-Order-Traversal/" rel="prev" title="Binary Tree Level Order Traversal">
                Binary Tree Level Order Traversal <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/07/18/ML2/"
     data-title="Coursera: Machine Learning by Stanford - Part2"
     data-content=""
     data-url="http://www.xuyiruan.com/2016/07/18/ML2/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/07/18/ML2/"
           data-title="Coursera: Machine Learning by Stanford - Part2" data-url="http://www.xuyiruan.com/2016/07/18/ML2/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/author.jpg"
               alt="🍣之神" />
          <p class="site-author-name" itemprop="name">🍣之神</p>
          <p class="site-description motion-element" itemprop="description">阮先生’s blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">31</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">32</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ruanxuyi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/xuyiruan/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/xuyi-ruan-a728a889" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://mszhuchinese.com" title="MsZhuChinese" target="_blank">MsZhuChinese</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.winniebabe.com" title="WinnieBabe" target="_blank">WinnieBabe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://suncuss.me" title="BossSun" target="_blank">BossSun</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_1"><span class="nav-number">1.</span> <span class="nav-text">Week 6: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluating_a_Learning_Algorithm"><span class="nav-number">1.1.</span> <span class="nav-text">Evaluating a Learning Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deciding_What_to_Try_Next"><span class="nav-number">1.1.1.</span> <span class="nav-text"> Deciding What to Try Next </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem_in_real_practice:"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Problem in real practice:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Machine_learning_diagnostic:"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">Machine learning diagnostic:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating_a_Hypothesis"><span class="nav-number">1.1.2.</span> <span class="nav-text"> Evaluating a Hypothesis </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis_(Ploting)"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Evaluating your hypothesis (Ploting)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis_(Spliting_your_data_set)"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Evaluating your hypothesis (Spliting your data set)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Misclassification_error_(a-k-a-_0/1_misclassification_error)"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">Misclassification error (a.k.a. 0/1 misclassification error)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Selection_and_Train/Validation/Test_Sets"><span class="nav-number">1.1.3.</span> <span class="nav-text">Model Selection and Train/Validation/Test Sets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting_problem"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Overfitting problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model_selection"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Model selection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">Evaluating your hypothesis</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_2"><span class="nav-number">2.</span> <span class="nav-text">Week 6: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bias_vs-_Variance"><span class="nav-number">2.1.</span> <span class="nav-text">Bias vs. Variance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Diagnosing_Bias_vs-_Variance"><span class="nav-number">2.1.1.</span> <span class="nav-text"> Diagnosing Bias vs. Variance </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias/Variance"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Bias/Variance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Diagnosing_bias_vs-_variance"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Diagnosing bias vs. variance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization_and_Bias/Variance"><span class="nav-number">2.1.2.</span> <span class="nav-text"> Regularization and Bias/Variance </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear_Regression_with_Regularization"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">Linear Regression with Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_regularization_parameter_$\lambda$"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">Choosing the regularization parameter $\lambda$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias/variance_as_a_function_of_the_regularization_parameter_$\lambda$-"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">Bias/variance as a function of the regularization parameter $\lambda$.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning_Curves"><span class="nav-number">2.1.3.</span> <span class="nav-text"> Learning Curves </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning_curves"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">Learning curves</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deciding_What_to_Do_Next_Revisited"><span class="nav-number">2.1.4.</span> <span class="nav-text">Deciding What to Do Next Revisited</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Debugging_a_learning_algorithm:"><span class="nav-number">2.1.4.1.</span> <span class="nav-text">Debugging a learning algorithm:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network_and_Overfitting"><span class="nav-number">2.1.4.2.</span> <span class="nav-text">Neural Network and Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How_to_decide_how_many_layers_for_hidden_layer?"><span class="nav-number">2.1.4.3.</span> <span class="nav-text">How to decide how many layers for hidden layer?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise:"><span class="nav-number">2.1.4.4.</span> <span class="nav-text">Exercise:</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_4"><span class="nav-number">3.</span> <span class="nav-text">Week 6: Lesson 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Building_a_Spam_Classifier"><span class="nav-number">3.1.</span> <span class="nav-text">Building a Spam Classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prioritizing_What_to_Work_On"><span class="nav-number">3.1.1.</span> <span class="nav-text"> Prioritizing What to Work On </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Building_a_spam_classifier"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Building a spam classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Building_a_spam_classifier-1"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">Building a spam classifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Error_Analysis"><span class="nav-number">3.1.2.</span> <span class="nav-text"> Error Analysis </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recommended_approach"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Recommended approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example_of_Error_Analysis"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">Example of Error Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The_important_of_numerical_evaluation"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">The important of numerical evaluation</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_5"><span class="nav-number">4.</span> <span class="nav-text">Week 6: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Handling_Skewed_Data"><span class="nav-number">4.1.</span> <span class="nav-text">Handling Skewed Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Error_Metrics_for_Skewed_Classes"><span class="nav-number">4.1.1.</span> <span class="nav-text"> Error Metrics for Skewed Classes </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skew_class"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">Skew class</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision/Recall"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">Precision/Recall</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trading_Off_Precision_and_Recall"><span class="nav-number">4.1.2.</span> <span class="nav-text">Trading Off Precision and Recall </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Determine_good_recall/variance:_Average:"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">Determine good recall/variance: Average:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Determine_good_recall/variance:_F_Score"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">Determine good recall/variance: F Score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#automatcially_determine_the_threshold_for_classifier:_$h(x)$_function"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">automatcially determine the threshold for classifier: $h(x)$ function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data_For_Machine_Learning"><span class="nav-number">4.1.3.</span> <span class="nav-text"> Data For Machine Learning </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Userful_test"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">Userful test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Improve_performance_with_large_data_set"><span class="nav-number">4.1.3.2.</span> <span class="nav-text">Improve performance with large data set</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_1"><span class="nav-number">5.</span> <span class="nav-text">Week 7: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Large_Margin_Classification"><span class="nav-number">5.1.</span> <span class="nav-text">Large Margin Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization_Objective"><span class="nav-number">5.1.1.</span> <span class="nav-text"> Optimization Objective </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Alternative_logistic_regression_to_SVM"><span class="nav-number">5.1.1.1.</span> <span class="nav-text">Alternative logistic regression to SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_cost_function"><span class="nav-number">5.1.1.2.</span> <span class="nav-text">SVM cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_hypothesis_function"><span class="nav-number">5.1.1.3.</span> <span class="nav-text">SVM hypothesis function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Large_Margin_Intuition"><span class="nav-number">5.1.2.</span> <span class="nav-text">Large Margin Intuition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Support_Vector_Machine"><span class="nav-number">5.1.2.1.</span> <span class="nav-text">Support Vector Machine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Large_Margin_Classifier"><span class="nav-number">5.1.2.2.</span> <span class="nav-text">Large Margin Classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_constant_value_C"><span class="nav-number">5.1.2.3.</span> <span class="nav-text">SVM constant value C</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mathematics_Behind_Large_Margin_Classification"><span class="nav-number">5.1.3.</span> <span class="nav-text"> Mathematics Behind Large Margin Classification </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Vector_Inner_Product"><span class="nav-number">5.1.3.1.</span> <span class="nav-text">Vector Inner Product</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_2"><span class="nav-number">6.</span> <span class="nav-text">Week 7: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kernels"><span class="nav-number">6.1.</span> <span class="nav-text">Kernels</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernels_I"><span class="nav-number">6.1.1.</span> <span class="nav-text"> Kernels I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hypothesis_function_for_SVM"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">Hypothesis function for SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gaussian_Kernels:_The_similarity_function"><span class="nav-number">6.1.1.2.</span> <span class="nav-text">Gaussian Kernels: The similarity function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Properties_of_the_similarity_function"><span class="nav-number">6.1.1.3.</span> <span class="nav-text">Properties of the similarity function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Effect_of_sigma_value"><span class="nav-number">6.1.1.4.</span> <span class="nav-text">Effect of sigma value</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernels_II"><span class="nav-number">6.1.2.</span> <span class="nav-text"> Kernels II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_landmarks"><span class="nav-number">6.1.2.1.</span> <span class="nav-text">Choosing the landmarks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_parameters_$\Theta$"><span class="nav-number">6.1.2.2.</span> <span class="nav-text">Choosing the parameters $\Theta$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM_parameters"><span class="nav-number">6.1.2.3.</span> <span class="nav-text">SVM parameters</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_7:_Lesson_3"><span class="nav-number">7.</span> <span class="nav-text">Week 7: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SVMs_in_Practice"><span class="nav-number">7.1.</span> <span class="nav-text">SVMs in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Using_An_SVM"><span class="nav-number">7.1.1.</span> <span class="nav-text">Using An SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kernel_(similarity)_functions:"><span class="nav-number">7.1.1.1.</span> <span class="nav-text">Kernel (similarity) functions:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementation_choice_for_SVM"><span class="nav-number">7.1.1.2.</span> <span class="nav-text">Implementation choice for SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-class_classification"><span class="nav-number">7.1.1.3.</span> <span class="nav-text">Multi-class classification</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic_regression_vs-_SVMs"><span class="nav-number">7.1.1.4.</span> <span class="nav-text">Logistic regression vs. SVMs</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_1"><span class="nav-number">8.</span> <span class="nav-text">Week 8: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering"><span class="nav-number">8.1.</span> <span class="nav-text">Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised_Learning:_Introduction"><span class="nav-number">8.1.1.</span> <span class="nav-text">Unsupervised Learning: Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#First_unsupervised_learning_algorithm:_clustering"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">First unsupervised learning algorithm: clustering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application_of_Clustering"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Application of Clustering</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means_algorithm"><span class="nav-number">8.1.2.</span> <span class="nav-text">K-means algorithm </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_introudction:"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">K-means introudction:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_algorithm_input:"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">K-means algorithm input:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_algorithm:"><span class="nav-number">8.1.2.3.</span> <span class="nav-text">K-means algorithm:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_for_non-well-separated_clusters"><span class="nav-number">8.1.2.4.</span> <span class="nav-text">K-means for non-well-separated clusters</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimization_Objective-1"><span class="nav-number">8.1.3.</span> <span class="nav-text"> Optimization Objective </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means_optimization_objective"><span class="nav-number">8.1.3.1.</span> <span class="nav-text">K-means optimization objective</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random_Initialization"><span class="nav-number">8.1.4.</span> <span class="nav-text"> Random Initialization </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Random_initialization"><span class="nav-number">8.1.4.1.</span> <span class="nav-text">Random initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Avoid_local_optima"><span class="nav-number">8.1.4.2.</span> <span class="nav-text">Avoid local optima</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiple_Random_initialization_algorithm"><span class="nav-number">8.1.4.3.</span> <span class="nav-text">Multiple Random initialization algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing_the_Number_of_Clusters"><span class="nav-number">8.1.5.</span> <span class="nav-text"> Choosing the Number of Clusters </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Elbow_method"><span class="nav-number">8.1.5.1.</span> <span class="nav-text">Elbow method</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_3"><span class="nav-number">9.</span> <span class="nav-text">Week 8: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation"><span class="nav-number">9.1.</span> <span class="nav-text">Motivation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivation_I:_Data_Compression"><span class="nav-number">9.1.1.</span> <span class="nav-text"> Motivation I: Data Compression </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data_Compression_From_2D_to_1D"><span class="nav-number">9.1.1.1.</span> <span class="nav-text">Data Compression From 2D to 1D</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivation_II:_Data_Visualization"><span class="nav-number">9.1.2.</span> <span class="nav-text"> Motivation II: Data Visualization </span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_4"><span class="nav-number">10.</span> <span class="nav-text">Week 8: Lesson 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Principal_Component_Analysis"><span class="nav-number">10.1.</span> <span class="nav-text">Principal Component Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal_Component_Analysis_Problem_Formulation"><span class="nav-number">10.1.1.</span> <span class="nav-text">Principal Component Analysis Problem Formulation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Principal_Component_Analysis_(PCA)_problem_formulation"><span class="nav-number">10.1.1.1.</span> <span class="nav-text">Principal Component Analysis (PCA) problem formulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_vs_Linear_Regression"><span class="nav-number">10.1.1.2.</span> <span class="nav-text">PCA vs Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Principal_Component_Analysis_Algorithm"><span class="nav-number">10.1.2.</span> <span class="nav-text">Principal Component Analysis Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data_Preprocessing"><span class="nav-number">10.1.2.1.</span> <span class="nav-text">Data Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_Algorithm_Introduction"><span class="nav-number">10.1.2.2.</span> <span class="nav-text">PCA Algorithm Introduction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_Algorithm"><span class="nav-number">10.1.2.3.</span> <span class="nav-text">PCA Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-_Compute_“covariance_matrix”"><span class="nav-number">10.1.2.3.1.</span> <span class="nav-text">1. Compute “covariance matrix”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-_Compute_“eigenvectors”_of_covariance_matrix_Σ"><span class="nav-number">10.1.2.3.2.</span> <span class="nav-text">2. Compute “eigenvectors” of covariance matrix Σ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-_Take_the_first_k_columns_of_the_U_matrix_and_compute_z"><span class="nav-number">10.1.2.3.3.</span> <span class="nav-text">3. Take the first k columns of the U matrix and compute z</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Summary"><span class="nav-number">10.1.2.4.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_8:_Lesson_5"><span class="nav-number">11.</span> <span class="nav-text">Week 8: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Applying_PCA"><span class="nav-number">11.1.</span> <span class="nav-text">Applying PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reconstruction_from_Compressed_Representation"><span class="nav-number">11.1.1.</span> <span class="nav-text">Reconstruction from Compressed Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reconstruction_from_compressed_representation"><span class="nav-number">11.1.1.1.</span> <span class="nav-text">Reconstruction from compressed representation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing_the_Number_of_Principal_Components"><span class="nav-number">11.1.2.</span> <span class="nav-text">Choosing the Number of Principal Components</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_$k$_(number_of_principal_components)"><span class="nav-number">11.1.2.1.</span> <span class="nav-text">Choosing $k$ (number of principal components)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm_for_choosing_$k$_with_loop"><span class="nav-number">11.1.2.2.</span> <span class="nav-text">Algorithm for choosing $k$ with loop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Algorithm_for_choosing_$k$_with_matrix_S"><span class="nav-number">11.1.2.3.</span> <span class="nav-text">Algorithm for choosing $k$ with matrix S</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Advice_for_Applying_PCA"><span class="nav-number">11.1.3.</span> <span class="nav-text">Advice for Applying PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Supervised_learning_speedup"><span class="nav-number">11.1.3.1.</span> <span class="nav-text">Supervised learning speedup</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application_of_PCA"><span class="nav-number">11.1.3.2.</span> <span class="nav-text">Application of PCA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bad_use_of_PCA:_To_prevent_overfitting"><span class="nav-number">11.1.3.3.</span> <span class="nav-text">Bad use of PCA: To prevent overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PCA_is_used_when_it_shouldn’t_be"><span class="nav-number">11.1.3.4.</span> <span class="nav-text">PCA is used when it shouldn’t be</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Benefit_of_PCA_compression_:"><span class="nav-number">11.1.3.5.</span> <span class="nav-text">Benefit of PCA compression    :</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logs"><span class="nav-number">12.</span> <span class="nav-text">Logs</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">🍣之神</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"xruan"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
  






  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("oKoPG1XTXkC7T1oasGQwau2g-gzGzoHsz", "JFJCQtIsUKhrX6S7Gvgqdqgk");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
