<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Bree Serif:300,300italic,400,400italic,700,700italic|Arimo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Coursera,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/sushi.png?v=5.0.1" />






<meta name="description">
<meta property="og:type" content="article">
<meta property="og:title" content="[Completed] Coursera: Machine Learning by Stanford - Part2">
<meta property="og:url" content="http://www.xuyiruan.com/2016/07/18/ML2/index.html">
<meta property="og:site_name" content="阮先生de小窝">
<meta property="og:description">
<meta property="og:image" content="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/car.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neuron.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/simple-neuron-model.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/math-layer-represnetiation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/learn-its-own-feature.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/other-structure.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p1.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-and.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/or-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/negation.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-xor.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/handwritten-digit-recognization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/multiclass-classification.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise2.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/logisitc-regression-regularization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/cost-function-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/forward.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/backprop.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/step.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160715/bprop-multiple-traning-set.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/back-prop.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/advanced-optimization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/unroll-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/unroll-example.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/advanced-optimzation-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/gradient-approximation-check.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/vector-theta.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/implementation-note.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/zero-initalization-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/random-initialization.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/hidden-layer-choice.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/for-back-prop-exampe.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160716/graphical-idea-backprop.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/diagnoise-benefit.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-training-set-polting.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-split-data.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/test-set-error-compute.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/misclassification-error.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/model-selection.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/split-three-section.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/three-part-model-selection.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/bias-variance.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/error-degree-polynomial.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160718/high-varience-bias.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/regularzation-parameter.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/choose-best-lambda.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/lambda-vs-cost-function.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/learning-curve-def.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-bias.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-variance.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/small-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/large-neural-network.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160719/exercise.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/spam-classifier.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/precission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/percission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/compare-percission-recall.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/recall-percission-average.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/F-SCORE.png">
<meta property="og:image" content="http://7xihzu.com1.z0.glb.clouddn.com/20160721/large-amout-data.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[Completed] Coursera: Machine Learning by Stanford - Part2">
<meta name="twitter:description">




<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>

  <title> [Completed] Coursera: Machine Learning by Stanford - Part2 | 阮先生de小窝 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">阮先生de小窝</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">多读书 多思考 少吃零食 多睡觉</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                [Completed] Coursera: Machine Learning by Stanford - Part2
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-07-18T10:15:48-04:00" content="07-18-2016">
              07-18-2016
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/07/18/ML2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/07/18/ML2/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/07/18/ML2/" class="leancloud_visitors" data-flag-title="[Completed] Coursera: Machine Learning by Stanford - Part2">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="https://d3njjcbhbojbot.cloudfront.net/api/utilities/v1/imageproxy/https://coursera.s3.amazonaws.com/topics/ml/large-icon.png" alt=""></p>
<a id="more"></a>
<h1 id="Week_4:_Lesson_1">Week 4: Lesson 1</h1><h2 id="Motivations"><center>Motivations</center></h2><h3 id="Non-linear_Hypotheses">Non-linear Hypotheses</h3><ul>
<li>new learning algorithm: <strong>neural network</strong>  </li>
</ul>
<h4 id="Why_neural_network:_new_learning_algorithm?">Why <strong>neural network</strong>: new learning algorithm?</h4><ul>
<li>non-linear classification(logistic regress ex) works well with only <strong>two features</strong>  </li>
<li>more interesting problems will invovle more than two features  </li>
<li>Too many features (quadratic order -&gt; $O(N^2)$, cubic order -&gt; $O(N^3)$) will result in <strong>overfitting</strong>  </li>
<li>Use <strong>subset</strong> of features will not provide best model to fit all the training set(underfit)  </li>
</ul>
<h4 id="Car_identify_example">Car identify example</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/car.png" alt=""></p>
<ul>
<li>$50 * 50$ picture = $2500$ pixels (in grayscale).  </li>
<li>RGB -&gt; $2500 * 3 = 7500$ pixels  </li>
<li>the $x$ vector represent pixel intensity(0-255)  </li>
<li>if we use non-linear hypothesis with quadratic features $(x_i \times x_j)$, 3 millions features (computational expensive!)  </li>
<li>simple <strong>logicstic regression</strong> with quadratic/cubic features <strong>not</strong> a good way to learn complex non-linear hypothesis when number of n(feature) is large.   </li>
</ul>
<h3 id="Neurons_and_the_Brain">Neurons and the Brain</h3><ul>
<li>neural networks are old algorithm  </li>
<li>background in neural networks  </li>
<li>how to apply them into problem  </li>
</ul>
<h4 id="Neural_Networks">Neural Networks</h4><ul>
<li>Origins: Algorithms that try to mimic the brain.  </li>
<li>Was widely used in 80s, and diminished in late 90s.(lack of computational power)  </li>
<li>Recent resurgence: state-of-art technique for many applicaitons  </li>
</ul>
<blockquote>
<p><strong>Auditory Cortex</strong>: part of your brain that allows you to lean to hear.  </p>
<p><strong>Somatosensory Cortex</strong>: part of your brain that allows you to process sense of touch. </p>
</blockquote>
<h1 id="Week_4:_Lesson_2">Week 4: Lesson 2</h1><h2 id="Neural_Networks-1"><center>Neural Networks</center></h2><h3 id="Model_Representation_I">Model Representation I</h3><ul>
<li>how to represent hypothesis and model with neural network  </li>
</ul>
<h4 id="Simulation_of_neuron_in_the_brain">Simulation of neuron in the brain</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neuron.png" alt=""></p>
<ul>
<li><strong>dendrite</strong> - “input wires”  </li>
<li><strong>axon</strong> - “output wires” send signal to other neurons  </li>
<li><strong>neuron cell body</strong> - processing inputs signal and output signal to other neurons  </li>
</ul>
<h4 id="Single_neuron_model:_logistic_unit">Single neuron model: logistic unit</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/simple-neuron-model.png" alt=""></p>
<h4 id="Neural_Network">Neural Network</h4><ul>
<li>group of single neuron models  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png" alt="">  </p>
<ul>
<li><strong>Input layer</strong>: layer 1  </li>
<li><strong>Output layer</strong>: layer 3  </li>
<li><strong>Hidden layer</strong>: anything that is NOT an input/output layer  </li>
</ul>
<h4 id="Computation_steps_of_neural_netwrok">Computation steps of neural netwrok</h4><p><strong>Notations</strong>:<br>$a_i^{(j)} = $ “activation” of unit $i$ in layer $j$.<br>$\theta^{(j)} = $ matrix of weights controlling function mapping from layer $j$ to layer $j+1$.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-example.png" alt=""></p>
<p><strong>Mathmatical representation:</strong></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/math-layer-represnetiation.png" alt=""></p>


If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\theta^{(j)}$ will be of dimension $$s_{j+1} \times (s_j +1)$$. 


<p><strong>Example</strong>:<br><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise.png" alt=""></p>
<h3 id="Model_Representation_II">Model Representation II</h3><ul>
<li>how to do vectorized computation    </li>
<li>how neural network can that help us to learn complex non-linear hyphothesis  </li>
</ul>
<h4 id="Forward_propagation:_Vectorized_implementation">Forward propagation: Vectorized implementation</h4><ul>
<li>efficient way to compute $h(x)$  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/vectorize-neural-network.png" alt=""></p>
<h4 id="Neural_Network_learning_its_own_features???">Neural Network learning its own features???</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/learn-its-own-feature.png" alt=""></p>
<h4 id="Other_network_architecures">Other network architecures</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/other-structure.png" alt=""></p>
<h4 id="exercise">exercise</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p1.png" alt=""></p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/ex-p2.png" alt=""></p>
<h3 id="Examples_and_Intuitions_I">Examples and Intuitions I</h3><ul>
<li>example shows how neural network to compute non-linear function  </li>
</ul>
<h4 id="Simple_example:_AND">Simple example: AND</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-network-and.png" alt=""></p>
<h4 id="Simple_example2:_OR">Simple example2: OR</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/or-function.png" alt=""></p>
<h3 id="Examples_and_Intuitions_II">Examples and Intuitions II</h3><h4 id="Negation">Negation</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/negation.png" alt=""></p>
<h4 id="XNOR">XNOR</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/neural-xor.png" alt=""></p>
<p>$XNOR = (x_1 AND x_2) OR ((NOT x_1) AND (NOT x_2))$ </p>
<h4 id="More_application:Handwritten_digit_classification">More application:Handwritten digit classification</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/handwritten-digit-recognization.png" alt=""></p>
<h3 id="Multiclass_Classification">Multiclass Classification</h3><ul>
<li><strong>more than one categories</strong> we tried to distinguish  </li>
<li>ex. handwritten digits recognization problem  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/multiclass-classification.png" alt=""></p>
<h4 id="Exercise">Exercise</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160707/exercise2.png" alt=""></p>
<p>$$s_{j+1} \times (s_j +1)$$. </p>
<p>Solution: sAccodring to equation above: $S_{j+1} = 10 $, and $S_j = 5$, therefore, $\theta^{(2)} = 10 \times 6 = 60 $.  </p>
<h1 id="Week_5:_Lesson_1">Week 5: Lesson 1</h1><h2 id="Cost_Function_and_Backpropagation"><center>Cost Function and Backpropagation</center></h2><h3 id="Cost_Function">Cost Function</h3><ul>
<li>fiting the parameter of the neural network  </li>
<li>backward propagtion? </li>
</ul>
<h4 id="Neural_Network(Classification)">Neural Network(Classification)</h4><p>$L$ = total no. of layers in network<br>$S_l$ = number of units in layer <code>l</code><br>$K$ = number of units in the output layer (K classes).   </p>
<h4 id="Cost_Function_for_Logisitc_regression:">Cost Function for Logisitc regression:</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/logisitc-regression-regularization.png" alt=""></p>
<h4 id="Cost_Function_for_Neural_network:">Cost Function for Neural network:</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/cost-function-neural-network.png" alt=""></p>
<blockquote>
<p>the bias terms $\theta<em>{10}, \theta</em>{20} … $ are NOT regularized</p>
</blockquote>
<ul>
<li>similar to logistic regression, we sum over all the values for <code>y</code> : ($1\times k$ <strong>vector</strong>) to find a minimum cost value.  </li>
<li>similar to the logstic regression, we also sum over all the $\theta$ values in the <strong>matrics</strong> $θ$ $(m \times n)$ matrics.  </li>
<li>the change (smaller $\theta$) of $\theta$ values in the regularzation term help to prevent <strong>overfitting</strong>.  </li>
<li>the change of $\theta$ values will also affect the hypothesis function $h_\theta(x)$, which will also directly affect the cost function $J(\theta)$.  </li>
<li>the regularzation term, elegantly contorl $\theta$ values to 1. <strong>prevent</strong>  overfitting of the hypothesis function, as well as 2.<strong>minimize</strong> the cost function. </li>
</ul>
<h3 id="Backpropagation_Algorithm">Backpropagation Algorithm</h3><ul>
<li>cost function for neural network $J_\theta$  </li>
<li>find the most <strong>optimal theta values</strong> with backpropagation algorithm.  </li>
</ul>
<h4 id="Gradient_Computation">Gradient Computation</h4><p>To find the optimal theta values, With the advanced optimzation algorithm, we just <strong>need code</strong> for following:   </p>
<ol>
<li>$J(\theta)$  </li>
<li>$\frac{\partial}{\partialΘ^{(l)_{ij}}}J(Θ)$  </li>
</ol>
<p>We already have the equation for $J(\theta)$, we will focus on how to find the partial derivative for second equation.  </p>
<h4 id="step_1_:_compute_Forward_Propagation">step 1 : compute Forward Propagation</h4><p>we need to perform forward propagation to find $a^{(1)},a^{(2)}…$.  </p>
<p>Simple example with only <strong>one training</strong> example.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/forward.png" alt=""></p>
<h4 id="step_2:_compute_Backward_Propagation">step 2: compute Backward Propagation</h4><p>$\delta_j^{(l)}$ = “error” of cost for activation unit $a_j^{(l)}$ node $j$ in layer $l$. (where l = 2…L).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/backprop.png" alt=""></p>
<p>For each <strong>output layer</strong>: ($L = 4$ e.x. four layers neural network)  </p>
<p>$$\delta_j^{(4)} = a_j^{(4)} - y_j$$</p>
<p>find error for each element j in the column vector $a$(activtion unit) and $y$(result of hypothesis function)</p>
<p>Vetorized form for equation above: </p>
<p>$$\delta^{(4)} = a^{(4)} - y$$</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/step.png" alt=""></p>
<blockquote>
<p>Note: there is <strong>NO</strong> $\delta^{(1)}$ term for backpropgation. </p>
</blockquote>
<h4 id="Backprogation_Algorithm_for_multiple_training_data">Backprogation Algorithm for multiple training data</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160715/bprop-multiple-traning-set.png" alt=""></p>
<p>Need to loop through all the $l$ ? </p>
<blockquote>
<p>For error $\delta$ and activation unit $alpha$, we need to loop through $l = 2…L$.<br>For $\Delta$, in a three-layers neural network, $\Delta^1$ and $\Delta^2$ have same dimension as <code>Theta1</code> and <code>Theta2</code>. </p>
</blockquote>
<h3 id="Backprogation_Intuition">Backprogation Intuition</h3><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/back-prop.png" alt=""></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/WZDMNM36PsM" frameborder="0" allowfullscreen></iframe>


<h1 id="Week_5:_Lesson_2">Week 5: Lesson 2</h1><h2 id="Backpropagation_in_Practice"><center>Backpropagation in Practice</center></h2><h3 id="Implementation_Note:_Unrolling_Parameters">Implementation Note: Unrolling Parameters</h3><ul>
<li>previously, we went over how to use backpropgation to find the derivative of cost function.  </li>
<li>this video focus on <strong>unrolling</strong> parameters.   </li>
</ul>
<h4 id="Unroll_into_vectors">Unroll into vectors</h4><ul>
<li>$Θ$ and $D$ are all matrics, we want to unroll them into <strong>vector</strong> to fit the cost function for advanced optimization.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/advanced-optimization.png" alt=""></p>
<p>Both <code>theat</code> and <code>gradient</code> are $n\times 1$ vector.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/unroll-neural-network.png" alt=""></p>
<p>we want to unroll matrices $Θ$ and $D$ to <strong>vector</strong> so that we can use as input for the cost function, and stay consistent with the same dimension with output gradient as well.  </p>
<h4 id="Example:">Example:</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/unroll-example.png" alt=""></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">% unroll to vector</span></span><br><span class="line"></span><br><span class="line">thetaVec = <span class="matrix">[ Theta1(:); Theta2(:); Theta3(:)]</span>;</span><br><span class="line">DVec = <span class="matrix">[D1(:), D2(:); D3(:)]</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">% recover back to matrics</span></span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(thetaVec(<span class="number">1</span>:<span class="number">110</span>), <span class="number">10</span>, <span class="number">11</span>); <span class="comment">% take first 1-110 elemnts, recover back to 10x11 matrics</span></span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(thetaVec(<span class="number">111</span>:<span class="number">220</span>), <span class="number">10</span>, <span class="number">11</span>); </span><br><span class="line">Theta3 = <span class="built_in">reshape</span>(thetaVec(<span class="number">221</span>:<span class="number">231</span>), <span class="number">1</span>, <span class="number">11</span>);</span><br></pre></td></tr></table></figure>
<p>Batter intutation of the unroll function in matlab:  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; a = <span class="matrix">[<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span></span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; a(:)</span><br><span class="line"></span><br><span class="line"><span class="comment">% unroll into column vector by column</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>Reshape function in matlab:  </p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">&gt;&gt; a</span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; b</span><br><span class="line"></span><br><span class="line">b =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">1</span>     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>     <span class="number">1</span>     <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; c = <span class="matrix">[a(:); b(:)]</span></span><br><span class="line"></span><br><span class="line">c =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">     <span class="number">2</span></span><br><span class="line">     <span class="number">4</span></span><br><span class="line">     <span class="number">6</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; recoverA = <span class="built_in">reshape</span>(c(<span class="number">1</span>:<span class="number">6</span>),<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">recoverA =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h4 id="Advanced_optimzation_algorithm_(neural_network)">Advanced optimzation algorithm (neural network)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/advanced-optimzation-neural-network.png" alt="">  </p>
<ul>
<li>inside the cost fucntion, we recover all vectors back to matrics to maximize the <strong>vectorized</strong> computation for forward and backward propagation.  </li>
<li>while outside the cost function, we unroll it back to vectors for input and output since the advanced optimzation algorithm assume input and output are in long vector form.  </li>
</ul>
<h3 id="Gradient_Checking">Gradient Checking</h3><ul>
<li>backpropagation is error prompting  </li>
<li>has bug even cost function is decreasing  </li>
<li>subtle bug can be catched by <strong>gradient checking</strong>  </li>
</ul>
<blockquote>
<p>Gradient = is the derivative of the cost function $J(\theta)$</p>
</blockquote>
<h4 id="Numerical_estimation_of_gradients">Numerical estimation of gradients</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/gradient-approximation-check.png" alt=""></p>
<blockquote>
<p>Note:  $\epsilon = 10^{(-4)}$ is a common use value for $\epsilon$</p>
</blockquote>
<h4 id="Parameter_vector_-_partial_derivative_of_Cost_function">Parameter vector - partial derivative of Cost function</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/vector-theta.png" alt="">  </p>
<p><strong>Code for Acutal implementation:</strong></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</span><br><span class="line">	thetaPlus = theta;</span><br><span class="line">	thetaPlus(<span class="built_in">i</span>) = thetaPlus(<span class="built_in">i</span>) + EPSILON;</span><br><span class="line">	thetaMinus = theta;</span><br><span class="line">	thetaMinus(<span class="built_in">i</span>) = thetaMinus(<span class="built_in">i</span>) - EPSILON;</span><br><span class="line">	gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus)) / (<span class="number">2</span>*EPSILO);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>Finally we check that <code>gradApprox ~= DVec</code>, where <code>DVec</code> calculated from backpropagation.  </p>
<h4 id="Implementation_note:">Implementation note:</h4><p>Once you finish gradient checking for back-propagation, <strong>TURN OFF</strong> the numerical gradient checking, since gradient checking is a very heavy computation.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/implementation-note.png" alt=""></p>
<h3 id="Random_Initialization">Random Initialization</h3><ul>
<li>how to pick initial theta value for advanced optimzation.  </li>
</ul>
<p>setting initial theta values to zeros, <code>initialTheta = zeros(n,1)</code> works for gradient descent, however, it does not work for neural network.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/zero-initalization-neural-network.png" alt=""></p>
<p>Zero initialization for neural network, after each update, parameters corresponding to inputs going into each of two <strong>hidden units are identical</strong>.  </p>
<h4 id="Random_Initalizaiton_alogrithm:_Symmetry_breaking">Random Initalizaiton alogrithm: Symmetry breaking</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/random-initialization.png" alt=""></p>
<p>?? Symmetry breaking ?? </p>
<h3 id="Putting_It_Together">Putting It Together</h3><h4 id="Step_1:_Pick_a_network_architecture">Step 1: Pick a network architecture</h4><ul>
<li>number of input units:Dimension of features $x^{(i)}  </li>
<li>number of output units:Number of classes  </li>
<li>number of hidden layers: default one layer.(even more hidden layers, better)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/hidden-layer-choice.png" alt="">  </p>
<h4 id="Step_2:_Randomly_initialze_weights:_small_values_near_zeros">Step 2: Randomly initialze weights: small values near zeros</h4><h4 id="Step_3:_Implement_forward_propgation_to_get_$h_Θ(x^{(i)})$_for_any_$x^{(i)}$">Step 3: Implement forward propgation to get $h_Θ(x^{(i)})$ for any $x^{(i)}$</h4><h4 id="Step_4:_Implement_code_to_compute_cost_function_$J(Θ)$">Step 4: Implement code to compute cost function $J(Θ)$</h4><h4 id="Step_5:_Implement_backprop_to_compute_partial_derivatives_$\frac{\partial}{\partial{θ}_{jk}^{(l)}}_J(Θ)$">Step 5: Implement backprop to compute partial derivatives $\frac{\partial}{\partial{θ}_{jk}^{(l)}} J(Θ)$</h4><p>?? why perform regularization when doing back-prop??</p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/for-back-prop-exampe.png" alt=""></p>
<h4 id="Step_6:_Numerical_Gradient_checking">Step 6: Numerical Gradient checking</h4><ul>
<li>disable gradient checking code after checking  </li>
</ul>
<h4 id="Step_7:_Gradient_descent/Advanced_Optimzation_with_backpropagation_to_try_to_minimize_$J(Θ)$-">Step 7: Gradient descent/Advanced Optimzation with backpropagation to try to minimize $J(Θ)$.</h4><p>$J(Θ)$ - not convex -&gt; only local optimal, NOT global global optimal.<br>However, in practice, the local optimal solution by backprop usually works pretty well.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160716/graphical-idea-backprop.png" alt=""></p>
<p>back-prop calculate the <strong>direction</strong> of going down hill, gradient descent is taking the <strong>actual step</strong> down hill.  </p>
<h1 id="Week_5:_Lesson_3">Week 5: Lesson 3</h1><h2 id="Application_of_Neural_Networks"><center>Application of Neural Networks</center></h2><h3 id="Autonomous_Driving">Autonomous Driving</h3><h1 id="Week_5:_Programnig_Assignemnt_Tips">Week 5: Programnig Assignemnt Tips</h1><p>Follow steps below to better implement the programing assigment with <strong>vectorized</strong> implementation for back-propagation.  </p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">Let:</span><br><span class="line"></span><br><span class="line">m = <span class="keyword">the</span> <span class="type">number</span> <span class="keyword">of</span> training examples</span><br><span class="line"></span><br><span class="line">n = <span class="keyword">the</span> <span class="type">number</span> <span class="keyword">of</span> training features, including <span class="keyword">the</span> initial bias unit.</span><br><span class="line"></span><br><span class="line">h = <span class="keyword">the</span> <span class="type">number</span> <span class="keyword">of</span> units <span class="keyword">in</span> <span class="keyword">the</span> hidden layer - NOT including <span class="keyword">the</span> bias unit</span><br><span class="line"></span><br><span class="line">r = <span class="keyword">the</span> <span class="type">number</span> <span class="keyword">of</span> output classifications</span><br><span class="line"></span><br><span class="line"><span class="comment">-------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>: Perform forward propagation, see <span class="keyword">the</span> separate tutorial <span class="keyword">if</span> necessary.</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>: δ<span class="number">3</span> <span class="keyword">or</span> d3 <span class="keyword">is</span> <span class="keyword">the</span> difference <span class="keyword">between</span> a3 <span class="keyword">and</span> <span class="keyword">the</span> y_matrix. The dimensions are <span class="keyword">the</span> same <span class="keyword">as</span> both, (m x r).</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>: z2 came <span class="keyword">from</span> <span class="keyword">the</span> forward propagation process - <span class="keyword">it</span>'s <span class="keyword">the</span> product <span class="keyword">of</span> a1 <span class="keyword">and</span> Theta1, prior <span class="keyword">to</span> applying <span class="keyword">the</span> sigmoid() function. Dimensions are (m x n) ⋅ (n x h) <span class="comment">--&gt; (m x h)</span></span><br><span class="line"></span><br><span class="line"><span class="number">4</span>: δ<span class="number">2</span> <span class="keyword">or</span> d2 <span class="keyword">is</span> tricky. It uses <span class="keyword">the</span> (:,<span class="number">2</span>:<span class="keyword">end</span>) columns <span class="keyword">of</span> Theta2. d2 <span class="keyword">is</span> <span class="keyword">the</span> product <span class="keyword">of</span> d3 <span class="keyword">and</span> Theta2(no bias), <span class="keyword">then</span> element-wise scaled <span class="keyword">by</span> sigmoid gradient <span class="keyword">of</span> z2. The size <span class="keyword">is</span> (m x r) ⋅ (r x h) <span class="comment">--&gt; (m x h). The size is the same as z2, as must be.</span></span><br><span class="line"></span><br><span class="line"><span class="number">5</span>: Δ<span class="number">1</span> <span class="keyword">or</span> Delta1 <span class="keyword">is</span> <span class="keyword">the</span> product <span class="keyword">of</span> d2 <span class="keyword">and</span> a1. The size <span class="keyword">is</span> (h x m) ⋅ (m x n) <span class="comment">--&gt; (h x n)</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>: Δ<span class="number">2</span> <span class="keyword">or</span> Delta2 <span class="keyword">is</span> <span class="keyword">the</span> product <span class="keyword">of</span> d3 <span class="keyword">and</span> a2. The size <span class="keyword">is</span> (r x m) ⋅ (m x [h+<span class="number">1</span>]) <span class="comment">--&gt; (r x [h+1])</span></span><br><span class="line"></span><br><span class="line"><span class="number">7</span>: Theta1_grad <span class="keyword">and</span> Theta2_grad are <span class="keyword">the</span> same size <span class="keyword">as</span> their respective Deltas, just scaled <span class="keyword">by</span> <span class="number">1</span>/m.</span><br><span class="line"></span><br><span class="line">Now you have <span class="keyword">the</span> unregularized gradients. Check your results using ex4.m, <span class="keyword">and</span> submit this portion <span class="keyword">to</span> <span class="keyword">the</span> grader.</span><br><span class="line"></span><br><span class="line">===== Regularization <span class="keyword">of</span> <span class="keyword">the</span> gradient ===========</span><br><span class="line"></span><br><span class="line">Since Theta1 <span class="keyword">and</span> Theta2 are <span class="keyword">local</span> copies, <span class="keyword">and</span> we've already computed our hypothesis value during forward-propagation, we're free <span class="keyword">to</span> modify them <span class="keyword">to</span> make <span class="keyword">the</span> gradient regularization easy <span class="keyword">to</span> compute.</span><br><span class="line"></span><br><span class="line"><span class="number">8</span>: So, <span class="keyword">set</span> <span class="keyword">the</span> <span class="keyword">first</span> column <span class="keyword">of</span> Theta1 <span class="keyword">and</span> Theta2 <span class="keyword">to</span> all-zeros. Here's a method you can <span class="keyword">try</span> <span class="keyword">in</span> your workspace console:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">Q = rand(<span class="number">3</span>,<span class="number">4</span>)       % create a test matrix</span><br><span class="line">Q(:,<span class="number">1</span>) = <span class="number">0</span>          % <span class="keyword">set</span> <span class="keyword">the</span> <span class="number">1</span>st column <span class="keyword">of</span> all rows <span class="keyword">to</span> <span class="number">0</span></span><br><span class="line"><span class="number">9</span>: Scale each Theta matrix <span class="keyword">by</span> λ/m. Use enough parenthesis so <span class="keyword">the</span> operation <span class="keyword">is</span> correct.</span><br><span class="line"></span><br><span class="line"><span class="number">10</span>: Add each <span class="keyword">of</span> these modified-<span class="keyword">and</span>-scaled Theta matrices <span class="keyword">to</span> <span class="keyword">the</span> un-regularized Theta gradients <span class="keyword">that</span> you computed earlier.</span><br><span class="line"></span><br><span class="line">You're done. Use <span class="keyword">the</span> test case (linked <span class="keyword">below</span>) <span class="keyword">to</span> test your code, <span class="keyword">and</span> <span class="keyword">the</span> ex4 <span class="keyword">script</span>, <span class="keyword">then</span> <span class="command">run</span> <span class="keyword">the</span> submit <span class="keyword">script</span>.</span><br></pre></td></tr></table></figure>
<p><a href="https://www.coursera.org/learn/machine-learning/discussions/all/threads/a8Kce_WxEeS16yIACyoj1Q" target="_blank" rel="external">link</a></p>
<h1 id="Week_6:_Lesson_1">Week 6: Lesson 1</h1><h2 id="Evaluating_a_Learning_Algorithm">Evaluating a Learning Algorithm</h2><h3 id="Deciding_What_to_Try_Next"><center> Deciding What to Try Next </center></h3><ul>
<li>practical issue while implementing your learning algorithm  </li>
<li>Potential things you can try to make it better  </li>
<li>Diagnostics - a techical way to help you to analyize and help you effectly select on <strong>specific</strong> <strong>things</strong> to try and <strong>improve</strong> for your learning algorithm    </li>
</ul>
<h4 id="Problem_in_real_practice:">Problem in real practice:</h4><p>Lets say you test your hypothesis on a new set of houses with regularized linear regression.  </p>
$$J(\theta) =  \frac 1 {2m}[\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 + \lambda \sum_{j=1}^{n}\theta_j^2] $$  
<blockquote>
<p>What if you find that it makes unacceptably large errors in its predictions, what could you try next?  </p>
</blockquote>
<ul>
<li>get <strong>more</strong> training examples (not work well as always)  </li>
<li><strong>smaller</strong> sets of features(prevent overfitting)  </li>
<li>Try getting <strong>additional</strong> features  </li>
<li>Try adding <strong>polynomial</strong> features  </li>
<li>Try <strong>increasing/decreasing</strong> $\lambda$, regularization parameter  </li>
</ul>
<p>However, which one above to choose to try? Randomly will waste your time after 6 months of your effort.  </p>
<h4 id="Machine_learning_diagnostic:">Machine learning diagnostic:</h4><ul>
<li>a test on what is <strong>NOT</strong> working in the current learning algorithm  </li>
<li>provides guidence to imporve its performance  </li>
<li>Diagnostics <strong>can take time</strong> to implement, <strong>but</strong> doing so can be <strong>worth</strong> use of your time.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/diagnoise-benefit.png" alt=""></p>
<h3 id="Evaluating_a_Hypothesis"><center> Evaluating a Hypothesis </center></h3><ul>
<li>how to evalulate a hypothesis learned by your algorithm  </li>
<li>how to prevent overfiitng/underfitting  </li>
<li>evalulate a hypothesis : plotting  </li>
<li>evalulate a hypothesis : Spliting data set into training(70%) and testing(30%) set.  </li>
<li>know how to compute <strong>test-set-error</strong> and <strong>Misclassification error</strong>.   </li>
</ul>
<h4 id="Evaluating_your_hypothesis_(Ploting)">Evaluating your hypothesis (Ploting)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-training-set-polting.png" alt="">  </p>
<p>plotting hypothesis works well for small number of features, however it is impossible to plot for lots of features.  </p>
<h4 id="Evaluating_your_hypothesis_(Spliting_your_data_set)">Evaluating your hypothesis (Spliting your data set)</h4><ul>
<li>70% for training set  </li>
<li>30% for test set  </li>
<li><strong>randomly sort</strong> the datasets before data spliting  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/evaluate-split-data.png" alt="">  </p>
<p><strong>Training/testing procedure</strong>  </p>
<ul>
<li>learning parameter $\theta$ from <strong>traning data</strong>.  </li>
<li>compute <strong>test set error</strong>. (average-square-error, same equation to compute the cost function, we use test-set instead of training set)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/test-set-error-compute.png" alt="">  </p>
<p>For <strong>logistic regression</strong>, beside computing <strong>test set error</strong>, we can also try to compute <strong>misclassification error</strong>( a.k.a. 0/1 misclassification error)  </p>
<h4 id="Misclassification_error_(a-k-a-_0/1_misclassification_error)">Misclassification error (a.k.a. 0/1 misclassification error)</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/misclassification-error.png" alt="">  </p>
<h3 id="Model_Selection_and_Train/Validation/Test_Sets"><center>Model Selection and Train/Validation/Test Sets</center></h3><ul>
<li>decide degree of polynomical  </li>
<li>what feature to include for the learning algoirthm   </li>
<li>model selection problem  </li>
<li>split data set into: train(60%), validation(20%), and test set(20%).  </li>
</ul>
<h4 id="Overfitting_problem">Overfitting problem</h4><ul>
<li>hypothesis function perfectly fit training data does not mean it will predicting the un-seen data well (not able to generalization)  </li>
</ul>
<h4 id="Model_selection">Model selection</h4><ul>
<li>degree of polynomial (d ) </li>
<li>linear, qudratic, cubic function  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/model-selection.png" alt="">  </p>
<h4 id="Evaluating_your_hypothesis">Evaluating your hypothesis</h4><ul>
<li>we split data sets into <strong>three</strong> sections to better helping to select the best model.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/split-three-section.png" alt=""></p>
<ul>
<li>training set (60%)  </li>
<li>cross validation set - CV(20%)  </li>
<li>test set (20%)  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/three-part-model-selection.png" alt="">  </p>
<p>Now, we are able to use <strong>training set</strong> to find $Θ$ for each polynomial degree, we then use <strong>cross validation set</strong> to select the lowest  $J_{cv}$  cost. We finally use <strong>test set</strong> to estimate generalization error as $J_{test}$. </p>
<h1 id="Week_6:_Lesson_2">Week 6: Lesson 2</h1><h2 id="Bias_vs-_Variance">Bias vs. Variance</h2><h3 id="Diagnosing_Bias_vs-_Variance"><center> Diagnosing Bias vs. Variance </center></h3><ul>
<li>identify underfiting/overfiting  </li>
<li>evaluate problem of bias/variance issues  </li>
<li><strong>bias</strong>: high  $J_{train}(\theta)$ and $ J_{cv}(\theta)$  </li>
<li><strong>variance</strong>: low $J_{train}(\theta)$ high $ J_{cv}(\theta)$ </li>
</ul>
<h4 id="Bias/Variance">Bias/Variance</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/bias-variance.png" alt=""></p>
<p>Bias - underfit<br>variance - overfit  </p>
<h4 id="Diagnosing_bias_vs-_variance">Diagnosing bias vs. variance</h4><p>Suppose your learning algorithm is not as well as you have perdict, how can we know if it is a bias problem or a variance problem?  </p>
<p>First, we need to plot
 $J_{cv}(\theta)$  (cross-validation error) and $J_{train} (\theta)$ (training error) in a error vs degree of polynomial d.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/error-degree-polynomial.png" alt="">  </p>
<p><strong>Bias(underfit):</strong>  </p>
<ul>
<li><strong>both</strong>  $J_{train}(\theta)$ and $J_{cv}(\theta)$  will be <strong>high</strong>  </li>
<li> $J_{train}(\theta) \approx J_{cv}(\theta)$  
</li>
</ul>
<p><strong>Variance(overfit):</strong>  </p>
<ul>
<li>$J_{train}(\theta)$ will be <strong>low</strong>.  </li>
<li> $J_{train}(\theta) \gg J_{cv}(\theta)$ 
</li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160718/high-varience-bias.png" alt=""></p>
<h3 id="Regularization_and_Bias/Variance"><center> Regularization and Bias/Variance </center></h3><ul>
<li>how regularization affect bias and variance  </li>
<li>how to automatically find $\lambda$ value: the regularzation parameter  </li>
<li>recall the procedures in utilizing $J_{cv}(\theta)$ to pick the most optimized $\lambda$ value.  </li>
</ul>
<h4 id="Linear_Regression_with_Regularization">Linear Regression with Regularization</h4><p>$\lambda$ = is the regularization parameter. A large value of $\lambda$ will heavily penalize all the $\theta$ values, which result in high bias(underfit). On the other hand, a too small $\lambda$ value will result in small effect by the regularization terms, which will result in high varience (overfit).  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/regularzation-parameter.png" alt=""></p>
<p>How to automatically select best regularzation parameter $\lambda$ for the regularization term.  </p>
<h4 id="Choosing_the_regularization_parameter_$\lambda$">Choosing the regularization parameter $\lambda$</h4><p>Steps and procedures to automatically choose the best $\lambda$ value.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/choose-best-lambda.png" alt=""></p>
<h4 id="Bias/variance_as_a_function_of_the_regularization_parameter_$\lambda$-">Bias/variance as a function of the regularization parameter $\lambda$.</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/lambda-vs-cost-function.png" alt="">  </p>
<blockquote>
<p><strong>Note</strong>:<br> We first use $J_{(\theta)}$ with the regularzaiton term to find the $minJ(\theta)$ which will give us the most optimal $Θ$ values base on the setting of degree of polynomial and regularization value $\lambda$. Even though the calculation of $J_{train}(\theta)$ is not directly assoicate with the looking for the best $\lambda$ value for the regularization term. The best $\lambda$ value directly associate with where smallest value of $J_{cv}(\theta)$ appear. However, the calcuation of $J_{train}(\theta)$, and $J_{cv}(\theta)$ together will result in a nice graph above. <br>With the present of graph above, we will be able to <strong>graphically determine</strong> whether bias/variance issues remain. </p>
</blockquote>
<h3 id="Learning_Curves"><center> Learning Curves </center></h3><ul>
<li>Learning Curves helps to diagnoise if a learning algorthm is suffering from bias/variance  </li>
<li>A learning curves make up of  $J_{train}(\theta)$ and $J_{cv}(\theta)$   </li>
<li>Poltting the learning curve will usually tells you if the learning algorithm is suffering from bias or variance.  </li>
</ul>
<h4 id="Learning_curves">Learning curves</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/learning-curve-def.png" alt=""></p>
<p>Lerning curve consisit of following:   </p>
<ul>
<li>$J_{train}(\theta)$ vs $m$  </li>
<li>$J_{cv}(\theta)$ vs $m$   </li>
</ul>
<p>where $m$ represents traning set size.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-bias.png" alt=""></p>
<p>If suffering from <strong>high bias</strong>, getting more training examples doesn’t help much. (change of model/degree of polynomial)  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/high-variance.png" alt=""></p>
<p>If suffering from <strong>high variance</strong> problem, getting more traning examples is likely to help. ( $J_{train}(\theta)$ is low, as more getting more data, $J_{cv}(\theta)$ is likely to approaching $J_{train}(\theta)$ )</p>
<h3 id="Deciding_What_to_Do_Next_Revisited"><center>Deciding What to Do Next Revisited</center></h3><ul>
<li>what we can do with what we have learned so far from week 6.  </li>
</ul>
<h4 id="Debugging_a_learning_algorithm:">Debugging a learning algorithm:</h4><p>Previously, we talked about few things we could possible try to improve our learning algorithm when the the hypothesis function did not give us a accurate prediction for un-seen data points.  </p>
<p>Now we everything we learned above, we could easily nail down half the unrelated options base on our analysis.  </p>
<ul>
<li>get <strong>more</strong> training examples (fix high variance)  </li>
<li><strong>smaller</strong> sets of features (fix high variance)  </li>
<li>Try getting <strong>additional</strong> features (fix high bias: function is too simple -&gt; need more complex function to better fitting the hypothesis function)</li>
<li>Try adding <strong>polynomial</strong> features (fix high bias) </li>
<li>Try <strong>increasing</strong> $\lambda$ (fix high variance)</li>
<li>Try <strong>decreasing</strong> $\lambda$ (fix high bias)</li>
</ul>
<h4 id="Neural_Network_and_Overfitting">Neural Network and Overfitting</h4><ul>
<li>take what we learned and <strong>relate</strong> back to <strong>neural netowrk</strong>.  </li>
<li>small neural network (underfitting) vs. large neural network(overfitting)  </li>
</ul>
<p><strong>Small neural network:</strong>  </p>
<ul>
<li><strong>fewer</strong> parameters  </li>
<li>more prone to <strong>underfitting</strong>  </li>
<li>computationally cheaper  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/small-neural-network.png" alt=""></p>
<p><strong>Large neural network:</strong>  </p>
<ul>
<li>large: either more activation units per layer or more layers.  </li>
<li><strong>more</strong> parameters  </li>
<li>more prone to <strong>overfitting</strong> (solve by regularization term($\lambda$))  </li>
<li>computationally more <strong>expensive</strong>.  </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/large-neural-network.png" alt=""></p>
<p> Larger network + regularzaiton($\lambda$) $\gg$ small neural network  </p>
<h4 id="How_to_decide_how_many_layers_for_hidden_layer?">How to decide how many layers for hidden layer?</h4><p>We could use the similiar techniques we used to determine the best $\lambda$ value.  </p>
<p>We split the data into three different sections and use the cross-validation technique with 1-layer, 2-layer, 3-layer and choose the layer with minimum $J_{cv}(Θ)$.  </p>
<h4 id="Exercise:">Exercise:</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160719/exercise.png" alt=""><br> $J_{cv}(Θ)$ $\gg$ $J_{train}(Θ)$ , we know that the learning algorithm is suffering from high variance (overfitting). Because of that, increasing the number of hidden layer (increasing number of $\theta$ parameter) will possiblly not doing any benefit to the learning algorithm.  </p>
<p>I would suggest <strong>decreasing</strong> number of hidden layer units OR getting <strong>more</strong> training examples OR adding <strong>regularization</strong> terms to imporve the learning algorithm.  </p>
<h1 id="Week_6:_Lesson_4">Week 6: Lesson 4</h1><h2 id="Building_a_Spam_Classifier">Building a Spam Classifier</h2><h3 id="Prioritizing_What_to_Work_On"><center> Prioritizing What to Work On </center></h3><ul>
<li>prioritize what you need to work on to save your time.  </li>
</ul>
<h4 id="Building_a_spam_classifier">Building a spam classifier</h4><ul>
<li>how to determine the $x=$ feature of email.  </li>
</ul>
<p>Maybe feature $x$: choose 100 words indicative of spam/not spam.  </p>
<p><code>E.g: deal, buy, discount, andrew, now...</code>  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/spam-classifier.png" alt=""></p>
<blockquote>
<p>Note: in practice, take most frequently occurring $n$ words (10,00 to 50,000) in training set, rather than manually pick 100 words.  </p>
</blockquote>
<h4 id="Building_a_spam_classifier-1">Building a spam classifier</h4><p>How to spend your time to make it have low error?  </p>
<ul>
<li>collect lots of data (“honeypot” project.)  </li>
<li>better features based on email routing information (from <strong>email header</strong>)  </li>
<li>better features for message body (deal, Dealers? features about punctuation)  </li>
<li>Develope sophisticated algorithm to detect misspellings (ex. w4thches, mad1cine, m0rtgage.)  </li>
</ul>
<h3 id="Error_Analysis"><center> Error Analysis </center></h3><ul>
<li>a way to make decision more <strong>systematically</strong>  </li>
<li><strong>start</strong> with <strong>simple implementation</strong> and start testing the result  </li>
<li>perform <strong>error analysis</strong> on result data.  </li>
<li><strong>numerical evaluation</strong> to decide whether it is good or not about the changes you made to your algorithm  </li>
</ul>
<h4 id="Recommended_approach">Recommended approach</h4><ul>
<li>start with a <strong>simple algorithm</strong> that you can implement quickly.  </li>
<li>implement it and test on <strong>cv data</strong> set.  </li>
<li>plot <strong>learning curves</strong> to decide if more data, more features, etc. are likely to help.  </li>
<li>Error analyisis: <strong>manually</strong> spot <strong>systemtically pattern</strong> of mis-classifying email.  </li>
</ul>
<h4 id="Example_of_Error_Analysis">Example of Error Analysis</h4><p>$m_{cv}$ = 500 examples in cross validation set.  </p>
<p>Algorithm misclassifies 100 emails.<br>Manually examine the 100 errors, and categorize them based on:<br>(i) what <strong>type</strong> of email it is.<br>(ii) what <strong>cues (features)</strong> you think would have helped the algorithm classify them correctly.  </p>
<p>In the 100 misclassified emails, we collected following types:  </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Pharma: <span class="number">12</span>  </span><br><span class="line">Replica/fake: <span class="number">4</span>  </span><br><span class="line">Steal Password: <span class="number">53</span>  </span><br><span class="line">Other: <span class="number">31</span></span><br></pre></td></tr></table></figure>
<p>In this case, we will possible put more efforts on spending more time in analyzing type of <code>steal password</code> email to improve the algorithm.  </p>
<h4 id="The_important_of_numerical_evaluation">The important of numerical evaluation</h4><p><code>Stemming</code>: (software: “Porter Stemmer”)  </p>
<p>Idea of stemming: treating words with same <code>stem</code> with same meaning. (discount/discounts/discounted/discounting)  </p>
<p>Disadvantage: even same <code>stem</code>, however, the meaning of the words might be totally different. (universe/university)  </p>
<p>The performance of the use of <code>stemming</code> is unknown unless we run <strong>numerical evaluation</strong> (e.g. <strong>cross validation error</strong>) of algorithm’s performance with and without stemming.  </p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">without stemming: <span class="number">5</span>% error</span><br><span class="line"></span><br><span class="line">with stemming: <span class="number">3</span>% error.</span><br><span class="line"></span><br><span class="line">stemming has better performance</span><br></pre></td></tr></table></figure>
<h1 id="Week_6:_Lesson_5">Week 6: Lesson 5</h1><h2 id="Handling_Skewed_Data">Handling Skewed Data</h2><h3 id="Error_Metrics_for_Skewed_Classes"><center> Error Metrics for Skewed Classes </center></h3><ul>
<li>definition of <strong>skew class</strong>  </li>
<li><strong>definition</strong> of recall and percission  </li>
<li><strong>equation</strong> for recall and percission  </li>
<li>how can we use recall and percission to detect ‘cheating’ classifier  </li>
</ul>
<h4 id="Skew_class">Skew class</h4><p><strong>Definition</strong>: the number of data sets is significant smaller than the data number of other class. The data sets with smaller number is called the <strong>skewed class</strong>.   </p>
<h4 id="Precision/Recall">Precision/Recall</h4><p>$y=1$ in presence of rare class that we want to detect.  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/precission-recall.png" alt=""></p>
<p><strong>Precision:</strong><br>Patient who <strong>don’t have</strong> cancer, but you <strong>predicted</strong> they have cancer.  </p>
<p>$Percission = \frac{True Positive}{Predicted Positive} = \frac{TruePositive}{TruePos+FalsePos}$</p>
<p><strong>Recall:</strong><br>Patient who <strong>has</strong> cancer, you <strong>predicted</strong> they <strong>don’t have</strong> cancer.  </p>
<p>$Recall = \frac{TruePositive}{ActualPositives} = \frac{TruePositive}{TruePos+FalseNeg}$</p>
<p>A cheating classifer with always predicting <code>y=0</code>(always not cancer) or <code>y=1</code> (always cancer) will not have high recall and high percission <strong>at the same time</strong>.</p>
<p>A classifer with always predicting <code>y=1</code> all the time will have a <strong>high</strong> <strong>recall</strong> (recall = 1) -&gt; lower percision.  </p>
<p>A classifer with always predicting <code>y=0</code> all the time will have a <strong>high</strong> <strong>percision</strong> (percision = 1) -&gt; lower recall.</p>
<h3 id="Trading_Off_Precision_and_Recall"><center>Trading Off Precision and Recall </center></h3><ul>
<li>how do we evaluate the trade-off between precision and recall?  </li>
<li>how to use F-score to find the best trade-off between precision and recall parameter  </li>
<li>automatically set threshold with F-score. </li>
</ul>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/percission-recall.png" alt=""></p>
<p>How to automatically pick the <strong>threshold</strong> above.  </p>
<p><strong>Single</strong> matrix comparison is easy (pick the <strong>max</strong> or <strong>min</strong> of the parameter)</p>
<p>However, in precision and recall matrix, we have <strong>two</strong> variables (precission(P) &amp; recall(R)). </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/compare-percission-recall.png" alt=""></p>
<p>Potential solutions: </p>
<h4 id="Determine_good_recall/variance:_Average:">Determine good recall/variance: Average:</h4><p>$$Average=\frac{P+R}{2}$$  </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/recall-percission-average.png" alt="">  </p>
<p>Not good, since algorithm 3: y=1 all the time has highest average. Not a good algorithm to evaluate the values of percision/recall numbers.  </p>
<h4 id="Determine_good_recall/variance:_F_Score">Determine good recall/variance: F Score</h4><p>$$F_{score} = 2 \frac{PR}{P+R}$$ </p>
<p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/F-SCORE.png" alt=""></p>
<h4 id="automatcially_determine_the_threshold_for_classifier:_$h(x)$_function">automatcially determine the threshold for classifier: $h(x)$ function</h4><p>try out few different thresholds and test on cv data set, and evaluate the highest F-score. </p>
<h3 id="Data_For_Machine_Learning"><center> Data For Machine Learning </center></h3><ul>
<li>Understanding of how large data set help improve learning algorithm.  </li>
<li>What is the userful test for the assumption for the idea of large data win.  </li>
</ul>
<blockquote>
<p>It’s not who has the best algorithm that wins. It’s who has the most data.  - Andrew Ng.  </p>
</blockquote>
<p>All the idea about enhancing your learning algorithm with very large amount of data is based on the assumption below: </p>
<p><strong>Assumption:</strong>  </p>
<blockquote>
<p>Assume feature $x\in R^{n+1}$ has <strong>sufficient</strong> information to predict $y$ correctly</p>
</blockquote>
<p>How can we determine whether or not the feature provide is <strong>sufficient</strong>?  </p>
<p>There is a simple test: Given the input $x$, can a human expert confidently predict $y$?  </p>
<h4 id="Userful_test">Userful test</h4><p><img src="http://7xihzu.com1.z0.glb.clouddn.com/20160721/large-amout-data.png" alt=""></p>
<p>Given the sentence above, an expert in English speaking will be easily fill in the sentence with word ‘two’ (not ‘to’ or ‘too’) base on the information (features) given in the sentence.  </p>
<p>However, an counter example of given only the size of a house (without the location, age), even an estitate expert will not be able to predict a fair price (y) for the house.  </p>
<h4 id="Improve_performance_with_large_data_set">Improve performance with large data set</h4><p>Having a large training set can significantly improve the learning algorithm with the <strong>assumption</strong> of “The features has enough information to predict y accurately” held on.  </p>
<p>Using a learning algorithm with <strong>many parameters</strong> will still be fine when we have a large set of training data. $J_{train}(\theta)$  will be small. And since we are using a <strong>very large</strong> training set, it is <strong>unlikely</strong> to overfit. Therefore, $J_{train(\theta)} \approx J_{test}(\theta)$, which means $J_{test}(\theta)$ will likely be small.  </p>
<h1 id="Logs">Logs</h1><ul>
<li>07/07/2016: week 4 lesson 1-2 completed.  </li>
<li>07/15/2016: week 5 lesson 1 completed.  </li>
<li>07/15/2016: week 5 lesson 2 &amp; 3 completed. </li>
<li>07/18/2016: New post for week 6 onward.  </li>
<li>07/18/2016: Week 6 Lesson 1 completed.  </li>
<li>07/19/2016: Week 6 Lesson 2 completed.  </li>
<li>07/19/2016: Week 6 Lesson 3，4，5 completed.  </li>
</ul>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coursera/" rel="tag">#Coursera</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/07/18/ML4/" rel="next" title="[Completed] Coursera: Machine Learning by Stanford - Part4">
                <i class="fa fa-chevron-left"></i> [Completed] Coursera: Machine Learning by Stanford - Part4
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/07/18/Binary-Tree-Level-Order-Traversal/" rel="prev" title="Binary Tree Level Order Traversal">
                Binary Tree Level Order Traversal <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/author.jpg"
               alt="🍣之神" />
          <p class="site-author-name" itemprop="name">🍣之神</p>
          <p class="site-description motion-element" itemprop="description">阮先生’s blog</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">69</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">51</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ruanxuyi" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.instagram.com/xuyiruan/" target="_blank" title="Instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  Instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/xuyi-ruan-a728a889" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-block">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://mszhuchinese.com" title="MsZhuChinese" target="_blank">MsZhuChinese</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.winniebabe.com" title="WinnieBabe" target="_blank">WinnieBabe</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://suncuss.me" title="BossSun" target="_blank">BossSun</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_4:_Lesson_1"><span class="nav-number">1.</span> <span class="nav-text">Week 4: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivations"><span class="nav-number">1.1.</span> <span class="nav-text">Motivations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linear_Hypotheses"><span class="nav-number">1.1.1.</span> <span class="nav-text">Non-linear Hypotheses</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Why_neural_network:_new_learning_algorithm?"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Why neural network: new learning algorithm?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Car_identify_example"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">Car identify example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neurons_and_the_Brain"><span class="nav-number">1.1.2.</span> <span class="nav-text">Neurons and the Brain</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Networks"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Neural Networks</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_4:_Lesson_2"><span class="nav-number">2.</span> <span class="nav-text">Week 4: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural_Networks-1"><span class="nav-number">2.1.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Representation_I"><span class="nav-number">2.1.1.</span> <span class="nav-text">Model Representation I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simulation_of_neuron_in_the_brain"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Simulation of neuron in the brain</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Single_neuron_model:_logistic_unit"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Single neuron model: logistic unit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computation_steps_of_neural_netwrok"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">Computation steps of neural netwrok</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Representation_II"><span class="nav-number">2.1.2.</span> <span class="nav-text">Model Representation II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward_propagation:_Vectorized_implementation"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">Forward propagation: Vectorized implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network_learning_its_own_features???"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">Neural Network learning its own features???</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Other_network_architecures"><span class="nav-number">2.1.2.3.</span> <span class="nav-text">Other network architecures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#exercise"><span class="nav-number">2.1.2.4.</span> <span class="nav-text">exercise</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples_and_Intuitions_I"><span class="nav-number">2.1.3.</span> <span class="nav-text">Examples and Intuitions I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple_example:_AND"><span class="nav-number">2.1.3.1.</span> <span class="nav-text">Simple example: AND</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple_example2:_OR"><span class="nav-number">2.1.3.2.</span> <span class="nav-text">Simple example2: OR</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Examples_and_Intuitions_II"><span class="nav-number">2.1.4.</span> <span class="nav-text">Examples and Intuitions II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Negation"><span class="nav-number">2.1.4.1.</span> <span class="nav-text">Negation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#XNOR"><span class="nav-number">2.1.4.2.</span> <span class="nav-text">XNOR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#More_application:Handwritten_digit_classification"><span class="nav-number">2.1.4.3.</span> <span class="nav-text">More application:Handwritten digit classification</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiclass_Classification"><span class="nav-number">2.1.5.</span> <span class="nav-text">Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise"><span class="nav-number">2.1.5.1.</span> <span class="nav-text">Exercise</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_5:_Lesson_1"><span class="nav-number">3.</span> <span class="nav-text">Week 5: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost_Function_and_Backpropagation"><span class="nav-number">3.1.</span> <span class="nav-text">Cost Function and Backpropagation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cost_Function"><span class="nav-number">3.1.1.</span> <span class="nav-text">Cost Function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network(Classification)"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Neural Network(Classification)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost_Function_for_Logisitc_regression:"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">Cost Function for Logisitc regression:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost_Function_for_Neural_network:"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">Cost Function for Neural network:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backpropagation_Algorithm"><span class="nav-number">3.1.2.</span> <span class="nav-text">Backpropagation Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient_Computation"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Gradient Computation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step_1_:_compute_Forward_Propagation"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">step 1 : compute Forward Propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#step_2:_compute_Backward_Propagation"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">step 2: compute Backward Propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backprogation_Algorithm_for_multiple_training_data"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">Backprogation Algorithm for multiple training data</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backprogation_Intuition"><span class="nav-number">3.1.3.</span> <span class="nav-text">Backprogation Intuition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_5:_Lesson_2"><span class="nav-number">4.</span> <span class="nav-text">Week 5: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Backpropagation_in_Practice"><span class="nav-number">4.1.</span> <span class="nav-text">Backpropagation in Practice</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementation_Note:_Unrolling_Parameters"><span class="nav-number">4.1.1.</span> <span class="nav-text">Implementation Note: Unrolling Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Unroll_into_vectors"><span class="nav-number">4.1.1.1.</span> <span class="nav-text">Unroll into vectors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example:"><span class="nav-number">4.1.1.2.</span> <span class="nav-text">Example:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Advanced_optimzation_algorithm_(neural_network)"><span class="nav-number">4.1.1.3.</span> <span class="nav-text">Advanced optimzation algorithm (neural network)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient_Checking"><span class="nav-number">4.1.2.</span> <span class="nav-text">Gradient Checking</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Numerical_estimation_of_gradients"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">Numerical estimation of gradients</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parameter_vector_-_partial_derivative_of_Cost_function"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">Parameter vector - partial derivative of Cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementation_note:"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">Implementation note:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Random_Initialization"><span class="nav-number">4.1.3.</span> <span class="nav-text">Random Initialization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Random_Initalizaiton_alogrithm:_Symmetry_breaking"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">Random Initalizaiton alogrithm: Symmetry breaking</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Putting_It_Together"><span class="nav-number">4.1.4.</span> <span class="nav-text">Putting It Together</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_1:_Pick_a_network_architecture"><span class="nav-number">4.1.4.1.</span> <span class="nav-text">Step 1: Pick a network architecture</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_2:_Randomly_initialze_weights:_small_values_near_zeros"><span class="nav-number">4.1.4.2.</span> <span class="nav-text">Step 2: Randomly initialze weights: small values near zeros</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_3:_Implement_forward_propgation_to_get_$h_Θ(x^{(i)})$_for_any_$x^{(i)}$"><span class="nav-number">4.1.4.3.</span> <span class="nav-text">Step 3: Implement forward propgation to get $h_Θ(x^{(i)})$ for any $x^{(i)}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_4:_Implement_code_to_compute_cost_function_$J(Θ)$"><span class="nav-number">4.1.4.4.</span> <span class="nav-text">Step 4: Implement code to compute cost function $J(Θ)$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_5:_Implement_backprop_to_compute_partial_derivatives_$\frac{\partial}{\partial{θ}_{jk}^{(l)}}_J(Θ)$"><span class="nav-number">4.1.4.5.</span> <span class="nav-text">Step 5: Implement backprop to compute partial derivatives $\frac{\partial}{\partial{θ}_{jk}^{(l)}} J(Θ)$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_6:_Numerical_Gradient_checking"><span class="nav-number">4.1.4.6.</span> <span class="nav-text">Step 6: Numerical Gradient checking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Step_7:_Gradient_descent/Advanced_Optimzation_with_backpropagation_to_try_to_minimize_$J(Θ)$-"><span class="nav-number">4.1.4.7.</span> <span class="nav-text">Step 7: Gradient descent/Advanced Optimzation with backpropagation to try to minimize $J(Θ)$.</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_5:_Lesson_3"><span class="nav-number">5.</span> <span class="nav-text">Week 5: Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Application_of_Neural_Networks"><span class="nav-number">5.1.</span> <span class="nav-text">Application of Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Autonomous_Driving"><span class="nav-number">5.1.1.</span> <span class="nav-text">Autonomous Driving</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_5:_Programnig_Assignemnt_Tips"><span class="nav-number">6.</span> <span class="nav-text">Week 5: Programnig Assignemnt Tips</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_1"><span class="nav-number">7.</span> <span class="nav-text">Week 6: Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluating_a_Learning_Algorithm"><span class="nav-number">7.1.</span> <span class="nav-text">Evaluating a Learning Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deciding_What_to_Try_Next"><span class="nav-number">7.1.1.</span> <span class="nav-text"> Deciding What to Try Next </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem_in_real_practice:"><span class="nav-number">7.1.1.1.</span> <span class="nav-text">Problem in real practice:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Machine_learning_diagnostic:"><span class="nav-number">7.1.1.2.</span> <span class="nav-text">Machine learning diagnostic:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating_a_Hypothesis"><span class="nav-number">7.1.2.</span> <span class="nav-text"> Evaluating a Hypothesis </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis_(Ploting)"><span class="nav-number">7.1.2.1.</span> <span class="nav-text">Evaluating your hypothesis (Ploting)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis_(Spliting_your_data_set)"><span class="nav-number">7.1.2.2.</span> <span class="nav-text">Evaluating your hypothesis (Spliting your data set)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Misclassification_error_(a-k-a-_0/1_misclassification_error)"><span class="nav-number">7.1.2.3.</span> <span class="nav-text">Misclassification error (a.k.a. 0/1 misclassification error)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model_Selection_and_Train/Validation/Test_Sets"><span class="nav-number">7.1.3.</span> <span class="nav-text">Model Selection and Train/Validation/Test Sets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting_problem"><span class="nav-number">7.1.3.1.</span> <span class="nav-text">Overfitting problem</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model_selection"><span class="nav-number">7.1.3.2.</span> <span class="nav-text">Model selection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating_your_hypothesis"><span class="nav-number">7.1.3.3.</span> <span class="nav-text">Evaluating your hypothesis</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_2"><span class="nav-number">8.</span> <span class="nav-text">Week 6: Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bias_vs-_Variance"><span class="nav-number">8.1.</span> <span class="nav-text">Bias vs. Variance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Diagnosing_Bias_vs-_Variance"><span class="nav-number">8.1.1.</span> <span class="nav-text"> Diagnosing Bias vs. Variance </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias/Variance"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">Bias/Variance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Diagnosing_bias_vs-_variance"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Diagnosing bias vs. variance</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization_and_Bias/Variance"><span class="nav-number">8.1.2.</span> <span class="nav-text"> Regularization and Bias/Variance </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear_Regression_with_Regularization"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">Linear Regression with Regularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Choosing_the_regularization_parameter_$\lambda$"><span class="nav-number">8.1.2.2.</span> <span class="nav-text">Choosing the regularization parameter $\lambda$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias/variance_as_a_function_of_the_regularization_parameter_$\lambda$-"><span class="nav-number">8.1.2.3.</span> <span class="nav-text">Bias/variance as a function of the regularization parameter $\lambda$.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning_Curves"><span class="nav-number">8.1.3.</span> <span class="nav-text"> Learning Curves </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Learning_curves"><span class="nav-number">8.1.3.1.</span> <span class="nav-text">Learning curves</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deciding_What_to_Do_Next_Revisited"><span class="nav-number">8.1.4.</span> <span class="nav-text">Deciding What to Do Next Revisited</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Debugging_a_learning_algorithm:"><span class="nav-number">8.1.4.1.</span> <span class="nav-text">Debugging a learning algorithm:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural_Network_and_Overfitting"><span class="nav-number">8.1.4.2.</span> <span class="nav-text">Neural Network and Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How_to_decide_how_many_layers_for_hidden_layer?"><span class="nav-number">8.1.4.3.</span> <span class="nav-text">How to decide how many layers for hidden layer?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise:"><span class="nav-number">8.1.4.4.</span> <span class="nav-text">Exercise:</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_4"><span class="nav-number">9.</span> <span class="nav-text">Week 6: Lesson 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Building_a_Spam_Classifier"><span class="nav-number">9.1.</span> <span class="nav-text">Building a Spam Classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prioritizing_What_to_Work_On"><span class="nav-number">9.1.1.</span> <span class="nav-text"> Prioritizing What to Work On </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Building_a_spam_classifier"><span class="nav-number">9.1.1.1.</span> <span class="nav-text">Building a spam classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Building_a_spam_classifier-1"><span class="nav-number">9.1.1.2.</span> <span class="nav-text">Building a spam classifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Error_Analysis"><span class="nav-number">9.1.2.</span> <span class="nav-text"> Error Analysis </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recommended_approach"><span class="nav-number">9.1.2.1.</span> <span class="nav-text">Recommended approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example_of_Error_Analysis"><span class="nav-number">9.1.2.2.</span> <span class="nav-text">Example of Error Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The_important_of_numerical_evaluation"><span class="nav-number">9.1.2.3.</span> <span class="nav-text">The important of numerical evaluation</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week_6:_Lesson_5"><span class="nav-number">10.</span> <span class="nav-text">Week 6: Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Handling_Skewed_Data"><span class="nav-number">10.1.</span> <span class="nav-text">Handling Skewed Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Error_Metrics_for_Skewed_Classes"><span class="nav-number">10.1.1.</span> <span class="nav-text"> Error Metrics for Skewed Classes </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skew_class"><span class="nav-number">10.1.1.1.</span> <span class="nav-text">Skew class</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Precision/Recall"><span class="nav-number">10.1.1.2.</span> <span class="nav-text">Precision/Recall</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trading_Off_Precision_and_Recall"><span class="nav-number">10.1.2.</span> <span class="nav-text">Trading Off Precision and Recall </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Determine_good_recall/variance:_Average:"><span class="nav-number">10.1.2.1.</span> <span class="nav-text">Determine good recall/variance: Average:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Determine_good_recall/variance:_F_Score"><span class="nav-number">10.1.2.2.</span> <span class="nav-text">Determine good recall/variance: F Score</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#automatcially_determine_the_threshold_for_classifier:_$h(x)$_function"><span class="nav-number">10.1.2.3.</span> <span class="nav-text">automatcially determine the threshold for classifier: $h(x)$ function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data_For_Machine_Learning"><span class="nav-number">10.1.3.</span> <span class="nav-text"> Data For Machine Learning </span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Userful_test"><span class="nav-number">10.1.3.1.</span> <span class="nav-text">Userful test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Improve_performance_with_large_data_set"><span class="nav-number">10.1.3.2.</span> <span class="nav-text">Improve performance with large data set</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logs"><span class="nav-number">11.</span> <span class="nav-text">Logs</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">🍣之神</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'xruan';
      var disqus_identifier = '2016/07/18/ML2/';
      var disqus_title = "[Completed] Coursera: Machine Learning by Stanford - Part2";
      var disqus_url = 'http://www.xuyiruan.com/2016/07/18/ML2/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("oKoPG1XTXkC7T1oasGQwau2g-gzGzoHsz", "JFJCQtIsUKhrX6S7Gvgqdqgk");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

</body>
</html>
